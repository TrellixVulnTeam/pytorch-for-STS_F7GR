The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.
The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.
/lustre/home/jbzhang/anaconda3/envs/pt120/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:115.)
  return torch._C._cuda_getDeviceCount() > 0
/lustre/home/jbzhang/anaconda3/envs/pt120/lib/python3.7/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)

Loading data...

Parameters:
	BATCH_SIZE=64
	CUDA=False
	DEVICE=-1
	DROPOUT=0.5
	EARLY_STOP=1500
	EMBED_DIM=128
	EMBED_NUM=9512
	EPOCHS=256
	KERNEL_NUM=100
	KERNEL_SIZES=[3, 4, 5]
	LOG_INTERVAL=1
	LR=0.001
	MAX_NORM=3.0
	PREDICT=None
	SAVE_BEST=True
	SAVE_DIR=snapshot/2021-07-12_18-49-25
	SAVE_INTERVAL=1000
	SHUFFLE=False
	SNAPSHOT=None
	STATIC=False
	TEST=False
	TEST_INTERVAL=100

Batch[1] - loss: 0.244814 
Batch[2] - loss: 0.159223 
Batch[3] - loss: 0.228038 
Batch[4] - loss: 0.223192 
Batch[5] - loss: 0.180378 
Batch[6] - loss: 0.172810 
Batch[7] - loss: 0.209271 
Batch[8] - loss: 0.117796 
Batch[9] - loss: 0.167782 
Batch[10] - loss: 0.170993 
Batch[11] - loss: 0.155612 
Batch[12] - loss: 0.135381 
Batch[13] - loss: 0.144614 
Batch[14] - loss: 0.147042 
Batch[15] - loss: 0.095156 
Batch[16] - loss: 0.097418 
Batch[17] - loss: 0.112004 
Batch[18] - loss: 0.117964 
Batch[19] - loss: 0.101829 
Batch[20] - loss: 0.129660 
Batch[21] - loss: 0.131879 
Batch[22] - loss: 0.102555 
Batch[23] - loss: 0.108362 
Batch[24] - loss: 0.147438 
Batch[25] - loss: 0.112123 
Batch[26] - loss: 0.102257 
Batch[27] - loss: 0.095797 
Batch[28] - loss: 0.123761 
Batch[29] - loss: 0.122517 
Batch[30] - loss: 0.107038 
Batch[31] - loss: 0.090860 
Batch[32] - loss: 0.099610 
Batch[33] - loss: 0.104026 
Batch[34] - loss: 0.100312 
Batch[35] - loss: 0.100933 
Batch[36] - loss: 0.111648 
Batch[37] - loss: 0.125224 
Batch[38] - loss: 0.152071 
Batch[39] - loss: 0.087374 
Batch[40] - loss: 0.121067 
Batch[41] - loss: 0.074505 
Batch[42] - loss: 0.103916 
Batch[43] - loss: 0.101098 
Batch[44] - loss: 0.100001 
Batch[45] - loss: 0.095228 
Batch[46] - loss: 0.110315 
Batch[47] - loss: 0.095659 
Batch[48] - loss: 0.102551 
Batch[49] - loss: 0.098889 
Batch[50] - loss: 0.105771 
Batch[51] - loss: 0.103004 
Batch[52] - loss: 0.096527 
Batch[53] - loss: 0.108553 
Batch[54] - loss: 0.088690 
Batch[55] - loss: 0.097754 
Batch[56] - loss: 0.115479 
Batch[57] - loss: 0.092899 
Batch[58] - loss: 0.097458 
Batch[59] - loss: 0.114539 
Batch[60] - loss: 0.087622 
Batch[61] - loss: 0.085021 
Batch[62] - loss: 0.108925 
Batch[63] - loss: 0.090182 
Batch[64] - loss: 0.078014 
Batch[65] - loss: 0.100881 
Batch[66] - loss: 0.086063 
Batch[67] - loss: 0.086290 
Batch[68] - loss: 0.094346 
Batch[69] - loss: 0.122351 
Batch[70] - loss: 0.066366 
Batch[71] - loss: 0.071168 
Batch[72] - loss: 0.077553 
Batch[73] - loss: 0.059045 
Batch[74] - loss: 0.041512 
Batch[75] - loss: 0.051984 
Batch[76] - loss: 0.061929 
Batch[77] - loss: 0.046078 
Batch[78] - loss: 0.048353 
Batch[79] - loss: 0.045217 
Batch[80] - loss: 0.052720 
Batch[81] - loss: 0.062216 
Batch[82] - loss: 0.046390 
Batch[83] - loss: 0.080957 
Batch[84] - loss: 0.047447 
Batch[85] - loss: 0.051457 
Batch[86] - loss: 0.048669 
Batch[87] - loss: 0.057723 
Batch[88] - loss: 0.070137 
Batch[89] - loss: 0.057159 
Batch[90] - loss: 0.051771 
Batch[91] - loss: 0.065644 
Batch[92] - loss: 0.065397 
Batch[93] - loss: 0.066222 
Batch[94] - loss: 0.042858 
Batch[95] - loss: 0.082243 
Batch[96] - loss: 0.042378 
Batch[97] - loss: 0.050364 
Batch[98] - loss: 0.057745 
Batch[99] - loss: 0.051160 
Batch[100] - loss: 0.061529 

Evaluation - loss: 0.000099 pearson: 0.3838 

Batch[101] - loss: 0.052486 
Batch[102] - loss: 0.074757 
Batch[103] - loss: 0.053580 
Batch[104] - loss: 0.050974 
Batch[105] - loss: 0.072905 
Batch[106] - loss: 0.061057 
Batch[107] - loss: 0.067375 
Batch[108] - loss: 0.055864 
Batch[109] - loss: 0.049796 
Batch[110] - loss: 0.056407 
Batch[111] - loss: 0.049377 
Batch[112] - loss: 0.068586 
Batch[113] - loss: 0.044535 
Batch[114] - loss: 0.066398 
Batch[115] - loss: 0.049269 
Batch[116] - loss: 0.054603 
Batch[117] - loss: 0.057011 
Batch[118] - loss: 0.037761 
Batch[119] - loss: 0.049824 
Batch[120] - loss: 0.040794 
Batch[121] - loss: 0.059268 
Batch[122] - loss: 0.055832 
Batch[123] - loss: 0.068498 
Batch[124] - loss: 0.055998 
Batch[125] - loss: 0.058397 
Batch[126] - loss: 0.052394 
Batch[127] - loss: 0.041012 
Batch[128] - loss: 0.044550 
Batch[129] - loss: 0.041506 
Batch[130] - loss: 0.036143 
Batch[131] - loss: 0.047787 
Batch[132] - loss: 0.046434 
Batch[133] - loss: 0.045989 
Batch[134] - loss: 0.054243 
Batch[135] - loss: 0.053081 
Batch[136] - loss: 0.060026 
Batch[137] - loss: 0.046346 
Batch[138] - loss: 0.052588 
Batch[139] - loss: 0.027888 
Batch[140] - loss: 0.040723 
Batch[141] - loss: 0.032651 
Batch[142] - loss: 0.023339 
Batch[143] - loss: 0.022491 
Batch[144] - loss: 0.038655 
Batch[145] - loss: 0.019261 
Batch[146] - loss: 0.025978 
Batch[147] - loss: 0.036619 
Batch[148] - loss: 0.024386 
Batch[149] - loss: 0.038038 
Batch[150] - loss: 0.031330 
Batch[151] - loss: 0.034850 
Batch[152] - loss: 0.032175 
Batch[153] - loss: 0.018393 
Batch[154] - loss: 0.035597 
Batch[155] - loss: 0.026230 
Batch[156] - loss: 0.034816 
Batch[157] - loss: 0.029276 
Batch[158] - loss: 0.031095 
Batch[159] - loss: 0.032078 
Batch[160] - loss: 0.040415 
Batch[161] - loss: 0.029419 
Batch[162] - loss: 0.035665 
Batch[163] - loss: 0.032126 
Batch[164] - loss: 0.025776 
Batch[165] - loss: 0.026324 
Batch[166] - loss: 0.025404 
Batch[167] - loss: 0.029984 
Batch[168] - loss: 0.032709 
Batch[169] - loss: 0.035579 
Batch[170] - loss: 0.033361 
Batch[171] - loss: 0.022617 
Batch[172] - loss: 0.030001 
Batch[173] - loss: 0.028034 
Batch[174] - loss: 0.031977 
Batch[175] - loss: 0.029669 
Batch[176] - loss: 0.031004 
Batch[177] - loss: 0.031097 
Batch[178] - loss: 0.039038 
Batch[179] - loss: 0.034639 
Batch[180] - loss: 0.026810 
Batch[181] - loss: 0.037928 
Batch[182] - loss: 0.028012 
Batch[183] - loss: 0.037203 
Batch[184] - loss: 0.029511 
Batch[185] - loss: 0.025142 
Batch[186] - loss: 0.032722 
Batch[187] - loss: 0.028477 
Batch[188] - loss: 0.020941 
Batch[189] - loss: 0.027335 
Batch[190] - loss: 0.028316 
Batch[191] - loss: 0.036657 
Batch[192] - loss: 0.024712 
Batch[193] - loss: 0.031242 
Batch[194] - loss: 0.045295 
Batch[195] - loss: 0.035210 
Batch[196] - loss: 0.021112 
Batch[197] - loss: 0.029479 
Batch[198] - loss: 0.039960 
Batch[199] - loss: 0.025649 
Batch[200] - loss: 0.045391 

Evaluation - loss: 0.000083 pearson: 0.4482 

Batch[201] - loss: 0.036016 
Batch[202] - loss: 0.038764 
Batch[203] - loss: 0.032855 
Batch[204] - loss: 0.027875 
Batch[205] - loss: 0.039623 
Batch[206] - loss: 0.039586 
Batch[207] - loss: 0.027308 
Batch[208] - loss: 0.024433 
Batch[209] - loss: 0.023638 
Batch[210] - loss: 0.015972 
Batch[211] - loss: 0.013712 
Batch[212] - loss: 0.015714 
Batch[213] - loss: 0.016214 
Batch[214] - loss: 0.023310 
Batch[215] - loss: 0.009176 
Batch[216] - loss: 0.023132 
Batch[217] - loss: 0.021826 
Batch[218] - loss: 0.019955 
Batch[219] - loss: 0.016875 
Batch[220] - loss: 0.016360 
Batch[221] - loss: 0.035845 
Batch[222] - loss: 0.019842 
Batch[223] - loss: 0.013813 
Batch[224] - loss: 0.029237 
Batch[225] - loss: 0.014088 
Batch[226] - loss: 0.021190 
Batch[227] - loss: 0.022263 
Batch[228] - loss: 0.012788 
Batch[229] - loss: 0.012538 
Batch[230] - loss: 0.018004 
Batch[231] - loss: 0.021551 
Batch[232] - loss: 0.019292 
Batch[233] - loss: 0.017458 
Batch[234] - loss: 0.019097 
Batch[235] - loss: 0.018755 
Batch[236] - loss: 0.020430 
Batch[237] - loss: 0.020689 
Batch[238] - loss: 0.016426 
Batch[239] - loss: 0.028466 
Batch[240] - loss: 0.018641 
Batch[241] - loss: 0.019977 
Batch[242] - loss: 0.026691 
Batch[243] - loss: 0.022330 
Batch[244] - loss: 0.014485 
Batch[245] - loss: 0.016150 
Batch[246] - loss: 0.011950 
Batch[247] - loss: 0.016061 
Batch[248] - loss: 0.016970 
Batch[249] - loss: 0.019947 
Batch[250] - loss: 0.019145 
Batch[251] - loss: 0.015297 
Batch[252] - loss: 0.015871 
Batch[253] - loss: 0.024661 
Batch[254] - loss: 0.019412 
Batch[255] - loss: 0.019095 
Batch[256] - loss: 0.015970 
Batch[257] - loss: 0.021262 
Batch[258] - loss: 0.012214 
Batch[259] - loss: 0.024043 
Batch[260] - loss: 0.016417 
Batch[261] - loss: 0.022806 
Batch[262] - loss: 0.015709 
Batch[263] - loss: 0.018550 
Batch[264] - loss: 0.018456 
Batch[265] - loss: 0.023902 
Batch[266] - loss: 0.016891 
Batch[267] - loss: 0.018911 
Batch[268] - loss: 0.021078 
Batch[269] - loss: 0.015916 
Batch[270] - loss: 0.012836 
Batch[271] - loss: 0.016293 
Batch[272] - loss: 0.014448 
Batch[273] - loss: 0.014377 
Batch[274] - loss: 0.016981 
Batch[275] - loss: 0.020341 
Batch[276] - loss: 0.014175 
Batch[277] - loss: 0.009223 
Batch[278] - loss: 0.008071 
Batch[279] - loss: 0.011222 
Batch[280] - loss: 0.012543 
Batch[281] - loss: 0.011420 
Batch[282] - loss: 0.011543 
Batch[283] - loss: 0.013958 
Batch[284] - loss: 0.010954 
Batch[285] - loss: 0.013098 
Batch[286] - loss: 0.006254 
Batch[287] - loss: 0.008829 
Batch[288] - loss: 0.009006 
Batch[289] - loss: 0.006111 
Batch[290] - loss: 0.008903 
Batch[291] - loss: 0.011320 
Batch[292] - loss: 0.008492 
Batch[293] - loss: 0.013195 
Batch[294] - loss: 0.011220 
Batch[295] - loss: 0.015972 
Batch[296] - loss: 0.011695 
Batch[297] - loss: 0.007781 
Batch[298] - loss: 0.009615 
Batch[299] - loss: 0.023448 
Batch[300] - loss: 0.011619 

Evaluation - loss: 0.000076 pearson: 0.4883 

Batch[301] - loss: 0.007526 
Batch[302] - loss: 0.011475 
Batch[303] - loss: 0.011360 
Batch[304] - loss: 0.009611 
Batch[305] - loss: 0.009076 
Batch[306] - loss: 0.014340 
Batch[307] - loss: 0.012873 
Batch[308] - loss: 0.015114 
Batch[309] - loss: 0.012631 
Batch[310] - loss: 0.012845 
Batch[311] - loss: 0.015473 
Batch[312] - loss: 0.013588 
Batch[313] - loss: 0.008807 
Batch[314] - loss: 0.012657 
Batch[315] - loss: 0.008294 
Batch[316] - loss: 0.009516 
Batch[317] - loss: 0.006709 
Batch[318] - loss: 0.007833 
Batch[319] - loss: 0.007053 
Batch[320] - loss: 0.011001 
Batch[321] - loss: 0.013471 
Batch[322] - loss: 0.015010 
Batch[323] - loss: 0.019759 
Batch[324] - loss: 0.013091 
Batch[325] - loss: 0.019053 
Batch[326] - loss: 0.011115 
Batch[327] - loss: 0.012887 
Batch[328] - loss: 0.015128 
Batch[329] - loss: 0.010550 
Batch[330] - loss: 0.011270 
Batch[331] - loss: 0.008714 
Batch[332] - loss: 0.017811 
Batch[333] - loss: 0.014247 
Batch[334] - loss: 0.011307 
Batch[335] - loss: 0.010994 
Batch[336] - loss: 0.014162 
Batch[337] - loss: 0.012862 
Batch[338] - loss: 0.022321 
Batch[339] - loss: 0.018248 
Batch[340] - loss: 0.013533 
Batch[341] - loss: 0.012984 
Batch[342] - loss: 0.007553 
Batch[343] - loss: 0.011775 
Batch[344] - loss: 0.011838 
Batch[345] - loss: 0.007762 
Batch[346] - loss: 0.006151 
Batch[347] - loss: 0.005078 
Batch[348] - loss: 0.010610 
Batch[349] - loss: 0.009117 
Batch[350] - loss: 0.006135 
Batch[351] - loss: 0.009959 
Batch[352] - loss: 0.008045 
Batch[353] - loss: 0.005256 
Batch[354] - loss: 0.005648 
Batch[355] - loss: 0.008798 
Batch[356] - loss: 0.007120 
Batch[357] - loss: 0.006317 
Batch[358] - loss: 0.006526 
Batch[359] - loss: 0.012635 
Batch[360] - loss: 0.006148 
Batch[361] - loss: 0.007644 
Batch[362] - loss: 0.007106 
Batch[363] - loss: 0.005751 
Batch[364] - loss: 0.011452 
Batch[365] - loss: 0.009572 
Batch[366] - loss: 0.010590 
Batch[367] - loss: 0.010693 
Batch[368] - loss: 0.010908 
Batch[369] - loss: 0.005994 
Batch[370] - loss: 0.009131 
Batch[371] - loss: 0.004813 
Batch[372] - loss: 0.004030 
Batch[373] - loss: 0.006115 
Batch[374] - loss: 0.008779 
Batch[375] - loss: 0.007638 
Batch[376] - loss: 0.009818 
Batch[377] - loss: 0.010624 
Batch[378] - loss: 0.010247 
Batch[379] - loss: 0.006425 
Batch[380] - loss: 0.007183 
Batch[381] - loss: 0.009325 
Batch[382] - loss: 0.006308 
Batch[383] - loss: 0.007073 
Batch[384] - loss: 0.006509 
Batch[385] - loss: 0.007472 
Batch[386] - loss: 0.009564 
Batch[387] - loss: 0.007730 
Batch[388] - loss: 0.004605 
Batch[389] - loss: 0.004989 
Batch[390] - loss: 0.008749 
Batch[391] - loss: 0.005474 
Batch[392] - loss: 0.009411 
Batch[393] - loss: 0.009584 
Batch[394] - loss: 0.009121 
Batch[395] - loss: 0.012862 
Batch[396] - loss: 0.005673 
Batch[397] - loss: 0.009016 
Batch[398] - loss: 0.007792 
Batch[399] - loss: 0.006519 
Batch[400] - loss: 0.006843 

Evaluation - loss: 0.000074 pearson: 0.5013 

Batch[401] - loss: 0.009875 
Batch[402] - loss: 0.007902 
Batch[403] - loss: 0.007159 
Batch[404] - loss: 0.006024 
Batch[405] - loss: 0.011118 
Batch[406] - loss: 0.007094 
Batch[407] - loss: 0.012336 
Batch[408] - loss: 0.007925 
Batch[409] - loss: 0.015956 
Batch[410] - loss: 0.007763 
Batch[411] - loss: 0.009745 
Batch[412] - loss: 0.010752 
Batch[413] - loss: 0.008251 
Batch[414] - loss: 0.008531 
Batch[415] - loss: 0.003958 
Batch[416] - loss: 0.004308 
Batch[417] - loss: 0.005843 
Batch[418] - loss: 0.007236 
Batch[419] - loss: 0.003710 
Batch[420] - loss: 0.004677 
Batch[421] - loss: 0.007907 
Batch[422] - loss: 0.008721 
Batch[423] - loss: 0.004354 
Batch[424] - loss: 0.004418 
Batch[425] - loss: 0.005014 
Batch[426] - loss: 0.005947 
Batch[427] - loss: 0.005401 
Batch[428] - loss: 0.003558 
Batch[429] - loss: 0.008856 
Batch[430] - loss: 0.005271 
Batch[431] - loss: 0.006319 
Batch[432] - loss: 0.007515 
Batch[433] - loss: 0.004977 
Batch[434] - loss: 0.003348 
Batch[435] - loss: 0.008317 
Batch[436] - loss: 0.004151 
Batch[437] - loss: 0.004903 
Batch[438] - loss: 0.006100 
Batch[439] - loss: 0.006200 
Batch[440] - loss: 0.005249 
Batch[441] - loss: 0.005716 
Batch[442] - loss: 0.005642 
Batch[443] - loss: 0.005909 
Batch[444] - loss: 0.004741 
Batch[445] - loss: 0.005956 
Batch[446] - loss: 0.005312 
Batch[447] - loss: 0.008344 
Batch[448] - loss: 0.005585 
Batch[449] - loss: 0.004191 
Batch[450] - loss: 0.007442 
Batch[451] - loss: 0.008120 
Batch[452] - loss: 0.005620 
Batch[453] - loss: 0.004349 
Batch[454] - loss: 0.004451 
Batch[455] - loss: 0.005944 
Batch[456] - loss: 0.004875 
Batch[457] - loss: 0.005393 
Batch[458] - loss: 0.005302 
Batch[459] - loss: 0.005563 
Batch[460] - loss: 0.005085 
Batch[461] - loss: 0.006600 
Batch[462] - loss: 0.008426 
Batch[463] - loss: 0.006407 
Batch[464] - loss: 0.007845 
Batch[465] - loss: 0.007617 
Batch[466] - loss: 0.006235 
Batch[467] - loss: 0.004260 
Batch[468] - loss: 0.010988 
Batch[469] - loss: 0.004274 
Batch[470] - loss: 0.006102 
Batch[471] - loss: 0.006084 
Batch[472] - loss: 0.005012 
Batch[473] - loss: 0.007619 
Batch[474] - loss: 0.007534 
Batch[475] - loss: 0.007693 
Batch[476] - loss: 0.007244 
Batch[477] - loss: 0.005031 
Batch[478] - loss: 0.005455 
Batch[479] - loss: 0.004330 
Batch[480] - loss: 0.007300 
Batch[481] - loss: 0.008735 
Batch[482] - loss: 0.007493 
Batch[483] - loss: 0.004054 
Batch[484] - loss: 0.002481 
Batch[485] - loss: 0.006769 
Batch[486] - loss: 0.004615 
Batch[487] - loss: 0.002834 
Batch[488] - loss: 0.007146 
Batch[489] - loss: 0.004915 
Batch[490] - loss: 0.003626 
Batch[491] - loss: 0.005367 
Batch[492] - loss: 0.004256 
Batch[493] - loss: 0.005225 
Batch[494] - loss: 0.002955 
Batch[495] - loss: 0.003377 
Batch[496] - loss: 0.003098 
Batch[497] - loss: 0.002848 
Batch[498] - loss: 0.005645 
Batch[499] - loss: 0.005386 
Batch[500] - loss: 0.004081 

Evaluation - loss: 0.000072 pearson: 0.5159 

Batch[501] - loss: 0.004985 
Batch[502] - loss: 0.003760 
Batch[503] - loss: 0.002672 
Batch[504] - loss: 0.004898 
Batch[505] - loss: 0.004514 
Batch[506] - loss: 0.003357 
Batch[507] - loss: 0.009156 
Batch[508] - loss: 0.005012 
Batch[509] - loss: 0.004814 
Batch[510] - loss: 0.003299 
Batch[511] - loss: 0.003950 
Batch[512] - loss: 0.006111 
Batch[513] - loss: 0.003500 
Batch[514] - loss: 0.003721 
Batch[515] - loss: 0.007016 
Batch[516] - loss: 0.004330 
Batch[517] - loss: 0.007294 
Batch[518] - loss: 0.003766 
Batch[519] - loss: 0.007799 
Batch[520] - loss: 0.003981 
Batch[521] - loss: 0.004989 
Batch[522] - loss: 0.003402 
Batch[523] - loss: 0.007741 
Batch[524] - loss: 0.004472 
Batch[525] - loss: 0.004502 
Batch[526] - loss: 0.006985 
Batch[527] - loss: 0.005170 
Batch[528] - loss: 0.004184 
Batch[529] - loss: 0.005267 
Batch[530] - loss: 0.004425 
Batch[531] - loss: 0.005239 
Batch[532] - loss: 0.004940 
Batch[533] - loss: 0.004494 
Batch[534] - loss: 0.006170 
Batch[535] - loss: 0.006284 
Batch[536] - loss: 0.008769 
Batch[537] - loss: 0.004324 
Batch[538] - loss: 0.004332 
Batch[539] - loss: 0.005243 
Batch[540] - loss: 0.004176 
Batch[541] - loss: 0.006130 
Batch[542] - loss: 0.006659 
Batch[543] - loss: 0.005210 
Batch[544] - loss: 0.009751 
Batch[545] - loss: 0.003780 
Batch[546] - loss: 0.006156 
Batch[547] - loss: 0.006052 
Batch[548] - loss: 0.005925 
Batch[549] - loss: 0.005800 
Batch[550] - loss: 0.005355 
Batch[551] - loss: 0.003555 
Batch[552] - loss: 0.009035 
Batch[553] - loss: 0.004333 
Batch[554] - loss: 0.004526 
Batch[555] - loss: 0.004485 
Batch[556] - loss: 0.002087 
Batch[557] - loss: 0.003420 
Batch[558] - loss: 0.003408 
Batch[559] - loss: 0.008519 
Batch[560] - loss: 0.003867 
Batch[561] - loss: 0.003285 
Batch[562] - loss: 0.008122 
Batch[563] - loss: 0.007360 
Batch[564] - loss: 0.007113 
Batch[565] - loss: 0.003287 
Batch[566] - loss: 0.003078 
Batch[567] - loss: 0.003615 
Batch[568] - loss: 0.003175 
Batch[569] - loss: 0.005365 
Batch[570] - loss: 0.003735 
Batch[571] - loss: 0.003039 
Batch[572] - loss: 0.003066 
Batch[573] - loss: 0.003263 
Batch[574] - loss: 0.007163 
Batch[575] - loss: 0.002201 
Batch[576] - loss: 0.005046 
Batch[577] - loss: 0.003921 
Batch[578] - loss: 0.004324 
Batch[579] - loss: 0.003931 
Batch[580] - loss: 0.008909 
Batch[581] - loss: 0.003150 
Batch[582] - loss: 0.003577 
Batch[583] - loss: 0.003680 
Batch[584] - loss: 0.003782 
Batch[585] - loss: 0.004522 
Batch[586] - loss: 0.004013 
Batch[587] - loss: 0.003195 
Batch[588] - loss: 0.004741 
Batch[589] - loss: 0.003371 
Batch[590] - loss: 0.004982 
Batch[591] - loss: 0.003605 
Batch[592] - loss: 0.004015 
Batch[593] - loss: 0.003373 
Batch[594] - loss: 0.005946 
Batch[595] - loss: 0.003219 
Batch[596] - loss: 0.004589 
Batch[597] - loss: 0.004808 
Batch[598] - loss: 0.004564 
Batch[599] - loss: 0.004100 
Batch[600] - loss: 0.002858 

Evaluation - loss: 0.000071 pearson: 0.5235 

Batch[601] - loss: 0.005521 
Batch[602] - loss: 0.003393 
Batch[603] - loss: 0.004065 
Batch[604] - loss: 0.008671 
Batch[605] - loss: 0.003280 
Batch[606] - loss: 0.004848 
Batch[607] - loss: 0.005153 
Batch[608] - loss: 0.004233 
Batch[609] - loss: 0.005345 
Batch[610] - loss: 0.004726 
Batch[611] - loss: 0.004530 
Batch[612] - loss: 0.006543 
Batch[613] - loss: 0.002900 
Batch[614] - loss: 0.004271 
Batch[615] - loss: 0.005528 
Batch[616] - loss: 0.003545 
Batch[617] - loss: 0.004918 
Batch[618] - loss: 0.004572 
Batch[619] - loss: 0.004085 
Batch[620] - loss: 0.003836 
Batch[621] - loss: 0.004716 
Batch[622] - loss: 0.003313 
Batch[623] - loss: 0.002047 
Batch[624] - loss: 0.003037 
Batch[625] - loss: 0.004486 
Batch[626] - loss: 0.002131 
Batch[627] - loss: 0.003713 
Batch[628] - loss: 0.002927 
Batch[629] - loss: 0.002147 
Batch[630] - loss: 0.003980 
Batch[631] - loss: 0.003603 
Batch[632] - loss: 0.002819 
Batch[633] - loss: 0.004046 
Batch[634] - loss: 0.003867 
Batch[635] - loss: 0.005353 
Batch[636] - loss: 0.003040 
Batch[637] - loss: 0.002137 
Batch[638] - loss: 0.004510 
Batch[639] - loss: 0.002703 
Batch[640] - loss: 0.003278 
Batch[641] - loss: 0.004509 
Batch[642] - loss: 0.002880 
Batch[643] - loss: 0.003954 
Batch[644] - loss: 0.001822 
Batch[645] - loss: 0.004212 
Batch[646] - loss: 0.005803 
Batch[647] - loss: 0.006005 
Batch[648] - loss: 0.002861 
Batch[649] - loss: 0.003078 
Batch[650] - loss: 0.002714 
Batch[651] - loss: 0.001929 
Batch[652] - loss: 0.002489 
Batch[653] - loss: 0.007047 
Batch[654] - loss: 0.002582 
Batch[655] - loss: 0.003165 
Batch[656] - loss: 0.011077 
Batch[657] - loss: 0.003288 
Batch[658] - loss: 0.004085 
Batch[659] - loss: 0.003076 
Batch[660] - loss: 0.003890 
Batch[661] - loss: 0.003436 
Batch[662] - loss: 0.003302 
Batch[663] - loss: 0.002884 
Batch[664] - loss: 0.003692 
Batch[665] - loss: 0.001926 
Batch[666] - loss: 0.003855 
Batch[667] - loss: 0.003437 
Batch[668] - loss: 0.003470 
Batch[669] - loss: 0.003410 
Batch[670] - loss: 0.004675 
Batch[671] - loss: 0.004218 
Batch[672] - loss: 0.003663 
Batch[673] - loss: 0.003129 
Batch[674] - loss: 0.004666 
Batch[675] - loss: 0.003118 
Batch[676] - loss: 0.003872 
Batch[677] - loss: 0.004387 
Batch[678] - loss: 0.003186 
Batch[679] - loss: 0.002656 
Batch[680] - loss: 0.002960 
Batch[681] - loss: 0.003095 
Batch[682] - loss: 0.003034 
Batch[683] - loss: 0.003138 
Batch[684] - loss: 0.002402 
Batch[685] - loss: 0.005035 
Batch[686] - loss: 0.002922 
Batch[687] - loss: 0.003965 
Batch[688] - loss: 0.003728 
Batch[689] - loss: 0.004484 
Batch[690] - loss: 0.003762 
Batch[691] - loss: 0.004975 
Batch[692] - loss: 0.002879 
Batch[693] - loss: 0.001778 
Batch[694] - loss: 0.002273 
Batch[695] - loss: 0.002017 
Batch[696] - loss: 0.001425 
Batch[697] - loss: 0.002807 
Batch[698] - loss: 0.002065 
Batch[699] - loss: 0.001947 
Batch[700] - loss: 0.003826 

Evaluation - loss: 0.000070 pearson: 0.5315 

Batch[701] - loss: 0.002678 
Batch[702] - loss: 0.003385 
Batch[703] - loss: 0.003773 
Batch[704] - loss: 0.007985 
Batch[705] - loss: 0.001597 
Batch[706] - loss: 0.004357 
Batch[707] - loss: 0.003627 
Batch[708] - loss: 0.002545 
Batch[709] - loss: 0.002255 
Batch[710] - loss: 0.003354 
Batch[711] - loss: 0.002278 
Batch[712] - loss: 0.002192 
Batch[713] - loss: 0.002427 
Batch[714] - loss: 0.002819 
Batch[715] - loss: 0.002312 
Batch[716] - loss: 0.003234 
Batch[717] - loss: 0.006233 
Batch[718] - loss: 0.001961 
Batch[719] - loss: 0.002266 
Batch[720] - loss: 0.003572 
Batch[721] - loss: 0.004572 
Batch[722] - loss: 0.001937 
Batch[723] - loss: 0.002751 
Batch[724] - loss: 0.001889 
Batch[725] - loss: 0.005240 
Batch[726] - loss: 0.003403 
Batch[727] - loss: 0.002382 
Batch[728] - loss: 0.002524 
Batch[729] - loss: 0.003137 
Batch[730] - loss: 0.002832 
Batch[731] - loss: 0.003891 
Batch[732] - loss: 0.001917 
Batch[733] - loss: 0.003650 
Batch[734] - loss: 0.002378 
Batch[735] - loss: 0.003435 
Batch[736] - loss: 0.002866 
Batch[737] - loss: 0.007954 
Batch[738] - loss: 0.002714 
Batch[739] - loss: 0.002870 
Batch[740] - loss: 0.002408 
Batch[741] - loss: 0.004863 
Batch[742] - loss: 0.003183 
Batch[743] - loss: 0.003884 
Batch[744] - loss: 0.004529 
Batch[745] - loss: 0.003912 
Batch[746] - loss: 0.002469 
Batch[747] - loss: 0.002625 
Batch[748] - loss: 0.001593 
Batch[749] - loss: 0.002416 
Batch[750] - loss: 0.002719 
Batch[751] - loss: 0.001631 
Batch[752] - loss: 0.002891 
Batch[753] - loss: 0.001762 
Batch[754] - loss: 0.003228 
Batch[755] - loss: 0.002750 
Batch[756] - loss: 0.003187 
Batch[757] - loss: 0.004012 
Batch[758] - loss: 0.003554 
Batch[759] - loss: 0.002897 
Batch[760] - loss: 0.003217 
Batch[761] - loss: 0.002155 
Batch[762] - loss: 0.001327 
Batch[763] - loss: 0.002622 
Batch[764] - loss: 0.000879 
Batch[765] - loss: 0.001841 
Batch[766] - loss: 0.002566 
Batch[767] - loss: 0.003446 
Batch[768] - loss: 0.001973 
Batch[769] - loss: 0.002209 
Batch[770] - loss: 0.002580 
Batch[771] - loss: 0.002056 
Batch[772] - loss: 0.002046 
Batch[773] - loss: 0.002811 
Batch[774] - loss: 0.005089 
Batch[775] - loss: 0.004227 
Batch[776] - loss: 0.001715 
Batch[777] - loss: 0.002237 
Batch[778] - loss: 0.002824 
Batch[779] - loss: 0.004850 
Batch[780] - loss: 0.002078 
Batch[781] - loss: 0.002417 
Batch[782] - loss: 0.001111 
Batch[783] - loss: 0.001770 
Batch[784] - loss: 0.003139 
Batch[785] - loss: 0.002774 
Batch[786] - loss: 0.002599 
Batch[787] - loss: 0.001707 
Batch[788] - loss: 0.004424 
Batch[789] - loss: 0.003246 
Batch[790] - loss: 0.002536 
Batch[791] - loss: 0.004874 
Batch[792] - loss: 0.002545 
Batch[793] - loss: 0.002737 
Batch[794] - loss: 0.001797 
Batch[795] - loss: 0.002572 
Batch[796] - loss: 0.004283 
Batch[797] - loss: 0.003039 
Batch[798] - loss: 0.003181 
Batch[799] - loss: 0.001711 
Batch[800] - loss: 0.002790 

Evaluation - loss: 0.000070 pearson: 0.5336 

Batch[801] - loss: 0.002321 
Batch[802] - loss: 0.001665 
Batch[803] - loss: 0.001917 
Batch[804] - loss: 0.001922 
Batch[805] - loss: 0.001822 
Batch[806] - loss: 0.003191 
Batch[807] - loss: 0.002628 
Batch[808] - loss: 0.002681 
Batch[809] - loss: 0.001689 
Batch[810] - loss: 0.003271 
Batch[811] - loss: 0.002152 
Batch[812] - loss: 0.002896 
Batch[813] - loss: 0.002790 
Batch[814] - loss: 0.002888 
Batch[815] - loss: 0.003312 
Batch[816] - loss: 0.001850 
Batch[817] - loss: 0.003636 
Batch[818] - loss: 0.002594 
Batch[819] - loss: 0.002445 
Batch[820] - loss: 0.002963 
Batch[821] - loss: 0.001924 
Batch[822] - loss: 0.002014 
Batch[823] - loss: 0.001797 
Batch[824] - loss: 0.010697 
Batch[825] - loss: 0.003846 
Batch[826] - loss: 0.002271 
Batch[827] - loss: 0.002417 
Batch[828] - loss: 0.003144 
Batch[829] - loss: 0.001407 
Batch[830] - loss: 0.002020 
Batch[831] - loss: 0.002164 
Batch[832] - loss: 0.004305 
Batch[833] - loss: 0.001972 
Batch[834] - loss: 0.002450 
Batch[835] - loss: 0.002093 
Batch[836] - loss: 0.002049 
Batch[837] - loss: 0.002300 
Batch[838] - loss: 0.003026 
Batch[839] - loss: 0.002792 
Batch[840] - loss: 0.001379 
Batch[841] - loss: 0.002531 
Batch[842] - loss: 0.002662 
Batch[843] - loss: 0.001360 
Batch[844] - loss: 0.002695 
Batch[845] - loss: 0.002428 
Batch[846] - loss: 0.002953 
Batch[847] - loss: 0.002392 
Batch[848] - loss: 0.001820 
Batch[849] - loss: 0.002248 
Batch[850] - loss: 0.002406 
Batch[851] - loss: 0.002410 
Batch[852] - loss: 0.001466 
Batch[853] - loss: 0.002025 
Batch[854] - loss: 0.003206 
Batch[855] - loss: 0.001522 
Batch[856] - loss: 0.001721 
Batch[857] - loss: 0.002369 
Batch[858] - loss: 0.001671 
Batch[859] - loss: 0.001870 
Batch[860] - loss: 0.009617 
Batch[861] - loss: 0.001811 
Batch[862] - loss: 0.002383 
Batch[863] - loss: 0.002456 
Batch[864] - loss: 0.001473 
Batch[865] - loss: 0.002414 
Batch[866] - loss: 0.005414 
Batch[867] - loss: 0.002619 
Batch[868] - loss: 0.003286 
Batch[869] - loss: 0.002992 
Batch[870] - loss: 0.001163 
Batch[871] - loss: 0.002527 
Batch[872] - loss: 0.001767 
Batch[873] - loss: 0.003469 
Batch[874] - loss: 0.002181 
Batch[875] - loss: 0.002045 
Batch[876] - loss: 0.001353 
Batch[877] - loss: 0.003086 
Batch[878] - loss: 0.005118 
Batch[879] - loss: 0.003331 
Batch[880] - loss: 0.001998 
Batch[881] - loss: 0.003462 
Batch[882] - loss: 0.001164 
Batch[883] - loss: 0.002321 
Batch[884] - loss: 0.001747 
Batch[885] - loss: 0.002537 
Batch[886] - loss: 0.002736 
Batch[887] - loss: 0.003242 
Batch[888] - loss: 0.002365 
Batch[889] - loss: 0.003181 
Batch[890] - loss: 0.001893 
Batch[891] - loss: 0.002766 
Batch[892] - loss: 0.004526 
Batch[893] - loss: 0.003611 
Batch[894] - loss: 0.001782 
Batch[895] - loss: 0.001799 
Batch[896] - loss: 0.002708 
Batch[897] - loss: 0.002277 
Batch[898] - loss: 0.001910 
Batch[899] - loss: 0.002455 
Batch[900] - loss: 0.004472 

Evaluation - loss: 0.000069 pearson: 0.5376 

Batch[901] - loss: 0.002773 
Batch[902] - loss: 0.002660 
Batch[903] - loss: 0.002367 
Batch[904] - loss: 0.002010 
Batch[905] - loss: 0.002655 
Batch[906] - loss: 0.001682 
Batch[907] - loss: 0.002664 
Batch[908] - loss: 0.002431 
Batch[909] - loss: 0.001355 
Batch[910] - loss: 0.001187 
Batch[911] - loss: 0.001595 
Batch[912] - loss: 0.002015 
Batch[913] - loss: 0.002129 
Batch[914] - loss: 0.002208 
Batch[915] - loss: 0.002237 
Batch[916] - loss: 0.002611 
Batch[917] - loss: 0.002058 
Batch[918] - loss: 0.001931 
Batch[919] - loss: 0.001502 
Batch[920] - loss: 0.002586 
Batch[921] - loss: 0.002000 
Batch[922] - loss: 0.002464 
Batch[923] - loss: 0.002500 
Batch[924] - loss: 0.001694 
Batch[925] - loss: 0.003451 
Batch[926] - loss: 0.002661 
Batch[927] - loss: 0.002845 
Batch[928] - loss: 0.001827 
Batch[929] - loss: 0.002516 
Batch[930] - loss: 0.008907 
Batch[931] - loss: 0.002063 
Batch[932] - loss: 0.002758 
Batch[933] - loss: 0.002830 
Batch[934] - loss: 0.000803 
Batch[935] - loss: 0.003257 
Batch[936] - loss: 0.002201 
Batch[937] - loss: 0.002424 
Batch[938] - loss: 0.002889 
Batch[939] - loss: 0.003505 
Batch[940] - loss: 0.001305 
Batch[941] - loss: 0.001883 
Batch[942] - loss: 0.004014 
Batch[943] - loss: 0.001879 
Batch[944] - loss: 0.002646 
Batch[945] - loss: 0.003350 
Batch[946] - loss: 0.004436 
Batch[947] - loss: 0.004488 
Batch[948] - loss: 0.002206 
Batch[949] - loss: 0.002830 
Batch[950] - loss: 0.001754 
Batch[951] - loss: 0.003245 
Batch[952] - loss: 0.003525 
Batch[953] - loss: 0.002115 
Batch[954] - loss: 0.001973 
Batch[955] - loss: 0.002288 
Batch[956] - loss: 0.002635 
Batch[957] - loss: 0.002333 
Batch[958] - loss: 0.002772 
Batch[959] - loss: 0.001706 
Batch[960] - loss: 0.002065 
Batch[961] - loss: 0.001763 
Batch[962] - loss: 0.003629 
Batch[963] - loss: 0.002804 
Batch[964] - loss: 0.003360 
Batch[965] - loss: 0.002132 
Batch[966] - loss: 0.002245 
Batch[967] - loss: 0.002452 
Batch[968] - loss: 0.002727 
Batch[969] - loss: 0.001462 
Batch[970] - loss: 0.002109 
Batch[971] - loss: 0.004160 
Batch[972] - loss: 0.003015 
Batch[973] - loss: 0.003057 
Batch[974] - loss: 0.003961 
Batch[975] - loss: 0.002335 
Batch[976] - loss: 0.001246 
Batch[977] - loss: 0.001309 
Batch[978] - loss: 0.001734 
Batch[979] - loss: 0.005856 
Batch[980] - loss: 0.002426 
Batch[981] - loss: 0.001000 
Batch[982] - loss: 0.001976 
Batch[983] - loss: 0.005596 
Batch[984] - loss: 0.002107 
Batch[985] - loss: 0.002230 
Batch[986] - loss: 0.002053 
Batch[987] - loss: 0.001875 
Batch[988] - loss: 0.001533 
Batch[989] - loss: 0.001462 
Batch[990] - loss: 0.002179 
Batch[991] - loss: 0.002383 
Batch[992] - loss: 0.002477 
Batch[993] - loss: 0.001950 
Batch[994] - loss: 0.003091 
Batch[995] - loss: 0.003079 
Batch[996] - loss: 0.002166 
Batch[997] - loss: 0.003869 
Batch[998] - loss: 0.001454 
Batch[999] - loss: 0.001800 
Batch[1000] - loss: 0.007687 

Evaluation - loss: 0.000069 pearson: 0.5421 

Batch[1001] - loss: 0.001369 
Batch[1002] - loss: 0.001314 
Batch[1003] - loss: 0.001382 
Batch[1004] - loss: 0.001277 
Batch[1005] - loss: 0.006913 
Batch[1006] - loss: 0.001793 
Batch[1007] - loss: 0.002642 
Batch[1008] - loss: 0.001882 
Batch[1009] - loss: 0.001787 
Batch[1010] - loss: 0.003999 
Batch[1011] - loss: 0.002655 
Batch[1012] - loss: 0.003933 
Batch[1013] - loss: 0.002717 
Batch[1014] - loss: 0.002496 
Batch[1015] - loss: 0.002463 
Batch[1016] - loss: 0.003109 
Batch[1017] - loss: 0.003844 
Batch[1018] - loss: 0.003363 
Batch[1019] - loss: 0.002157 
Batch[1020] - loss: 0.001461 
Batch[1021] - loss: 0.003058 
Batch[1022] - loss: 0.003559 
Batch[1023] - loss: 0.002623 
Batch[1024] - loss: 0.003027 
Batch[1025] - loss: 0.002493 
Batch[1026] - loss: 0.001589 
Batch[1027] - loss: 0.001504 
Batch[1028] - loss: 0.002112 
Batch[1029] - loss: 0.003544 
Batch[1030] - loss: 0.002785 
Batch[1031] - loss: 0.003011 
Batch[1032] - loss: 0.004936 
Batch[1033] - loss: 0.001635 
Batch[1034] - loss: 0.002476 
Batch[1035] - loss: 0.001789 
Batch[1036] - loss: 0.001818 
Batch[1037] - loss: 0.002199 
Batch[1038] - loss: 0.002315 
Batch[1039] - loss: 0.001755 
Batch[1040] - loss: 0.001892 
Batch[1041] - loss: 0.002571 
Batch[1042] - loss: 0.002947 
Batch[1043] - loss: 0.001814 
Batch[1044] - loss: 0.003073 
Batch[1045] - loss: 0.002732 
Batch[1046] - loss: 0.001767 
Batch[1047] - loss: 0.001740 
Batch[1048] - loss: 0.003267 
Batch[1049] - loss: 0.002714 
Batch[1050] - loss: 0.005836 
Batch[1051] - loss: 0.001923 
Batch[1052] - loss: 0.003508 
Batch[1053] - loss: 0.001408 
Batch[1054] - loss: 0.002721 
Batch[1055] - loss: 0.000885 
Batch[1056] - loss: 0.001424 
Batch[1057] - loss: 0.003064 
Batch[1058] - loss: 0.002204 
Batch[1059] - loss: 0.001763 
Batch[1060] - loss: 0.001512 
Batch[1061] - loss: 0.004424 
Batch[1062] - loss: 0.002446 
Batch[1063] - loss: 0.002881 
Batch[1064] - loss: 0.001157 
Batch[1065] - loss: 0.001972 
Batch[1066] - loss: 0.002370 
Batch[1067] - loss: 0.001330 
Batch[1068] - loss: 0.001885 
Batch[1069] - loss: 0.001863 
Batch[1070] - loss: 0.002053 
Batch[1071] - loss: 0.002409 
Batch[1072] - loss: 0.004602 
Batch[1073] - loss: 0.002900 
Batch[1074] - loss: 0.001978 
Batch[1075] - loss: 0.001625 
Batch[1076] - loss: 0.001691 
Batch[1077] - loss: 0.001679 
Batch[1078] - loss: 0.002557 
Batch[1079] - loss: 0.002078 
Batch[1080] - loss: 0.002608 
Batch[1081] - loss: 0.001932 
Batch[1082] - loss: 0.001829 
Batch[1083] - loss: 0.001922 
Batch[1084] - loss: 0.002321 
Batch[1085] - loss: 0.001994 
Batch[1086] - loss: 0.002012 
Batch[1087] - loss: 0.001838 
Batch[1088] - loss: 0.001939 
Batch[1089] - loss: 0.001926 
Batch[1090] - loss: 0.004267 
Batch[1091] - loss: 0.001930 
Batch[1092] - loss: 0.003277 
Batch[1093] - loss: 0.002646 
Batch[1094] - loss: 0.001995 
Batch[1095] - loss: 0.001763 
Batch[1096] - loss: 0.002696 
Batch[1097] - loss: 0.001890 
Batch[1098] - loss: 0.001609 
Batch[1099] - loss: 0.002527 
Batch[1100] - loss: 0.003542 

Evaluation - loss: 0.000069 pearson: 0.5426 

Batch[1101] - loss: 0.001673 
Batch[1102] - loss: 0.001904 
Batch[1103] - loss: 0.002179 
Batch[1104] - loss: 0.001899 
Batch[1105] - loss: 0.002367 
Batch[1106] - loss: 0.002252 
Batch[1107] - loss: 0.001058 
Batch[1108] - loss: 0.000719 
Batch[1109] - loss: 0.001100 
Batch[1110] - loss: 0.002160 
Batch[1111] - loss: 0.001782 
Batch[1112] - loss: 0.001509 
Batch[1113] - loss: 0.001859 
Batch[1114] - loss: 0.002404 
Batch[1115] - loss: 0.000909 
Batch[1116] - loss: 0.002699 
Batch[1117] - loss: 0.002516 
Batch[1118] - loss: 0.001635 
Batch[1119] - loss: 0.001893 
Batch[1120] - loss: 0.001319 
Batch[1121] - loss: 0.001372 
Batch[1122] - loss: 0.002316 
Batch[1123] - loss: 0.001721 
Batch[1124] - loss: 0.002488 
Batch[1125] - loss: 0.003225 
Batch[1126] - loss: 0.002674 
Batch[1127] - loss: 0.002334 
Batch[1128] - loss: 0.001179 
Batch[1129] - loss: 0.001639 
Batch[1130] - loss: 0.002977 
Batch[1131] - loss: 0.001584 
Batch[1132] - loss: 0.001420 
Batch[1133] - loss: 0.002674 
Batch[1134] - loss: 0.002888 
Batch[1135] - loss: 0.001972 
Batch[1136] - loss: 0.001239 
Batch[1137] - loss: 0.001892 
Batch[1138] - loss: 0.001272 
Batch[1139] - loss: 0.003010 
Batch[1140] - loss: 0.001422 
Batch[1141] - loss: 0.002434 
Batch[1142] - loss: 0.001715 
Batch[1143] - loss: 0.002242 
Batch[1144] - loss: 0.002008 
Batch[1145] - loss: 0.002085 
Batch[1146] - loss: 0.001612 
Batch[1147] - loss: 0.001351 
Batch[1148] - loss: 0.001798 
Batch[1149] - loss: 0.002163 
Batch[1150] - loss: 0.002509 
Batch[1151] - loss: 0.001607 
Batch[1152] - loss: 0.002582 
Batch[1153] - loss: 0.001624 
Batch[1154] - loss: 0.002558 
Batch[1155] - loss: 0.002521 
Batch[1156] - loss: 0.001350 
Batch[1157] - loss: 0.002017 
Batch[1158] - loss: 0.001731 
Batch[1159] - loss: 0.003640 
Batch[1160] - loss: 0.001583 
Batch[1161] - loss: 0.002239 
Batch[1162] - loss: 0.001956 
Batch[1163] - loss: 0.002323 
Batch[1164] - loss: 0.001831 
Batch[1165] - loss: 0.002843 
Batch[1166] - loss: 0.001722 
Batch[1167] - loss: 0.002238 
Batch[1168] - loss: 0.001381 
Batch[1169] - loss: 0.001866 
Batch[1170] - loss: 0.001767 
Batch[1171] - loss: 0.002819 
Batch[1172] - loss: 0.002317 
Batch[1173] - loss: 0.004143 
Batch[1174] - loss: 0.001527 
Batch[1175] - loss: 0.001169 
Batch[1176] - loss: 0.004683 
Batch[1177] - loss: 0.001486 
Batch[1178] - loss: 0.001724 
Batch[1179] - loss: 0.002657 
Batch[1180] - loss: 0.001046 
Batch[1181] - loss: 0.002503 
Batch[1182] - loss: 0.008858 
Batch[1183] - loss: 0.002268 
Batch[1184] - loss: 0.002186 
Batch[1185] - loss: 0.001527 
Batch[1186] - loss: 0.010191 
Batch[1187] - loss: 0.001382 
Batch[1188] - loss: 0.001353 
Batch[1189] - loss: 0.002128 
Batch[1190] - loss: 0.001380 
Batch[1191] - loss: 0.001146 
Batch[1192] - loss: 0.001679 
Batch[1193] - loss: 0.001716 
Batch[1194] - loss: 0.001759 
Batch[1195] - loss: 0.001373 
Batch[1196] - loss: 0.003001 
Batch[1197] - loss: 0.002375 
Batch[1198] - loss: 0.002341 
Batch[1199] - loss: 0.002929 
Batch[1200] - loss: 0.002841 

Evaluation - loss: 0.000067 pearson: 0.5528 

Batch[1201] - loss: 0.002981 
Batch[1202] - loss: 0.001798 
Batch[1203] - loss: 0.002806 
Batch[1204] - loss: 0.002596 
Batch[1205] - loss: 0.002113 
Batch[1206] - loss: 0.000989 
Batch[1207] - loss: 0.001574 
Batch[1208] - loss: 0.002722 
Batch[1209] - loss: 0.005199 
Batch[1210] - loss: 0.001856 
Batch[1211] - loss: 0.001204 
Batch[1212] - loss: 0.001169 
Batch[1213] - loss: 0.001985 
Batch[1214] - loss: 0.002802 
Batch[1215] - loss: 0.001518 
Batch[1216] - loss: 0.001593 
Batch[1217] - loss: 0.001684 
Batch[1218] - loss: 0.001323 
Batch[1219] - loss: 0.001995 
Batch[1220] - loss: 0.003372 
Batch[1221] - loss: 0.001595 
Batch[1222] - loss: 0.002036 
Batch[1223] - loss: 0.003066 
Batch[1224] - loss: 0.002588 
Batch[1225] - loss: 0.002852 
Batch[1226] - loss: 0.001378 
Batch[1227] - loss: 0.001173 
Batch[1228] - loss: 0.001468 
Batch[1229] - loss: 0.003028 
Batch[1230] - loss: 0.003144 
Batch[1231] - loss: 0.003419 
Batch[1232] - loss: 0.002007 
Batch[1233] - loss: 0.001802 
Batch[1234] - loss: 0.003695 
Batch[1235] - loss: 0.001928 
Batch[1236] - loss: 0.001248 
Batch[1237] - loss: 0.001552 
Batch[1238] - loss: 0.001801 
Batch[1239] - loss: 0.001897 
Batch[1240] - loss: 0.002583 
Batch[1241] - loss: 0.001506 
Batch[1242] - loss: 0.001906 
Batch[1243] - loss: 0.001425 
Batch[1244] - loss: 0.001635 
Batch[1245] - loss: 0.002323 
Batch[1246] - loss: 0.002092 
Batch[1247] - loss: 0.002332 
Batch[1248] - loss: 0.002034 
Batch[1249] - loss: 0.001217 
Batch[1250] - loss: 0.001472 
Batch[1251] - loss: 0.001541 
Batch[1252] - loss: 0.002313 
Batch[1253] - loss: 0.003995 
Batch[1254] - loss: 0.001949 
Batch[1255] - loss: 0.002683 
Batch[1256] - loss: 0.001414 
Batch[1257] - loss: 0.001723 
Batch[1258] - loss: 0.002024 
Batch[1259] - loss: 0.001101 
Batch[1260] - loss: 0.001352 
Batch[1261] - loss: 0.001571 
Batch[1262] - loss: 0.002898 
Batch[1263] - loss: 0.002073 
Batch[1264] - loss: 0.001934 
Batch[1265] - loss: 0.002367 
Batch[1266] - loss: 0.001911 
Batch[1267] - loss: 0.002860 
Batch[1268] - loss: 0.001320 
Batch[1269] - loss: 0.002690 
Batch[1270] - loss: 0.001619 
Batch[1271] - loss: 0.002891 
Batch[1272] - loss: 0.001392 
Batch[1273] - loss: 0.002656 
Batch[1274] - loss: 0.002111 
Batch[1275] - loss: 0.001948 
Batch[1276] - loss: 0.000935 
Batch[1277] - loss: 0.001576 
Batch[1278] - loss: 0.001630 
Batch[1279] - loss: 0.003401 
Batch[1280] - loss: 0.001802 
Batch[1281] - loss: 0.002398 
Batch[1282] - loss: 0.002556 
Batch[1283] - loss: 0.001836 
Batch[1284] - loss: 0.003206 
Batch[1285] - loss: 0.002090 
Batch[1286] - loss: 0.001966 
Batch[1287] - loss: 0.003063 
Batch[1288] - loss: 0.001621 
Batch[1289] - loss: 0.001738 
Batch[1290] - loss: 0.002183 
Batch[1291] - loss: 0.002095 
Batch[1292] - loss: 0.002696 
Batch[1293] - loss: 0.001630 
Batch[1294] - loss: 0.002360 
Batch[1295] - loss: 0.001849 
Batch[1296] - loss: 0.002377 
Batch[1297] - loss: 0.001956 
Batch[1298] - loss: 0.001799 
Batch[1299] - loss: 0.001070 
Batch[1300] - loss: 0.002226 

Evaluation - loss: 0.000068 pearson: 0.5449 

Batch[1301] - loss: 0.001902 
Batch[1302] - loss: 0.001913 
Batch[1303] - loss: 0.001749 
Batch[1304] - loss: 0.002637 
Batch[1305] - loss: 0.002161 
Batch[1306] - loss: 0.001257 
Batch[1307] - loss: 0.001825 
Batch[1308] - loss: 0.001575 
Batch[1309] - loss: 0.000974 
Batch[1310] - loss: 0.001435 
Batch[1311] - loss: 0.001657 
Batch[1312] - loss: 0.001603 
Batch[1313] - loss: 0.000848 
Batch[1314] - loss: 0.001178 
Batch[1315] - loss: 0.002188 
Batch[1316] - loss: 0.001350 
Batch[1317] - loss: 0.001277 
Batch[1318] - loss: 0.001374 
Batch[1319] - loss: 0.000768 
Batch[1320] - loss: 0.002034 
Batch[1321] - loss: 0.000981 
Batch[1322] - loss: 0.004218 
Batch[1323] - loss: 0.000604 
Batch[1324] - loss: 0.001702 
Batch[1325] - loss: 0.001183 
Batch[1326] - loss: 0.001971 
Batch[1327] - loss: 0.002872 
Batch[1328] - loss: 0.003180 
Batch[1329] - loss: 0.001989 
Batch[1330] - loss: 0.002049 
Batch[1331] - loss: 0.001606 
Batch[1332] - loss: 0.002055 
Batch[1333] - loss: 0.002135 
Batch[1334] - loss: 0.001295 
Batch[1335] - loss: 0.001918 
Batch[1336] - loss: 0.001177 
Batch[1337] - loss: 0.001042 
Batch[1338] - loss: 0.001910 
Batch[1339] - loss: 0.001300 
Batch[1340] - loss: 0.001132 
Batch[1341] - loss: 0.002156 
Batch[1342] - loss: 0.001707 
Batch[1343] - loss: 0.001288 
Batch[1344] - loss: 0.001234 
Batch[1345] - loss: 0.002154 
Batch[1346] - loss: 0.000992 
Batch[1347] - loss: 0.002513 
Batch[1348] - loss: 0.002109 
Batch[1349] - loss: 0.001550 
Batch[1350] - loss: 0.003135 
Batch[1351] - loss: 0.002975 
Batch[1352] - loss: 0.001454 
Batch[1353] - loss: 0.001921 
Batch[1354] - loss: 0.002113 
Batch[1355] - loss: 0.001102 
Batch[1356] - loss: 0.002505 
Batch[1357] - loss: 0.003048 
Batch[1358] - loss: 0.002663 
Batch[1359] - loss: 0.002552 
Batch[1360] - loss: 0.001874 
Batch[1361] - loss: 0.001901 
Batch[1362] - loss: 0.001861 
Batch[1363] - loss: 0.001764 
Batch[1364] - loss: 0.001440 
Batch[1365] - loss: 0.002756 
Batch[1366] - loss: 0.002130 
Batch[1367] - loss: 0.001403 
Batch[1368] - loss: 0.000786 
Batch[1369] - loss: 0.000793 
Batch[1370] - loss: 0.001256 
Batch[1371] - loss: 0.002458 
Batch[1372] - loss: 0.001144 
Batch[1373] - loss: 0.001625 
Batch[1374] - loss: 0.001502 
Batch[1375] - loss: 0.001748 
Batch[1376] - loss: 0.001479 
Batch[1377] - loss: 0.001689 
Batch[1378] - loss: 0.001587 
Batch[1379] - loss: 0.001518 
Batch[1380] - loss: 0.001524 
Batch[1381] - loss: 0.001446 
Batch[1382] - loss: 0.000962 
Batch[1383] - loss: 0.001920 
Batch[1384] - loss: 0.000929 
Batch[1385] - loss: 0.001584 
Batch[1386] - loss: 0.000661 
Batch[1387] - loss: 0.002299 
Batch[1388] - loss: 0.001165 
Batch[1389] - loss: 0.003903 
Batch[1390] - loss: 0.001299 
Batch[1391] - loss: 0.001560 
Batch[1392] - loss: 0.001056 
Batch[1393] - loss: 0.003222 
Batch[1394] - loss: 0.001945 
Batch[1395] - loss: 0.001381 
Batch[1396] - loss: 0.001151 
Batch[1397] - loss: 0.000896 
Batch[1398] - loss: 0.003104 
Batch[1399] - loss: 0.001172 
Batch[1400] - loss: 0.001596 

Evaluation - loss: 0.000067 pearson: 0.5531 

Batch[1401] - loss: 0.001528 
Batch[1402] - loss: 0.002194 
Batch[1403] - loss: 0.001057 
Batch[1404] - loss: 0.001202 
Batch[1405] - loss: 0.002108 
Batch[1406] - loss: 0.002216 
Batch[1407] - loss: 0.000628 
Batch[1408] - loss: 0.001248 
Batch[1409] - loss: 0.002039 
Batch[1410] - loss: 0.001170 
Batch[1411] - loss: 0.001113 
Batch[1412] - loss: 0.001228 
Batch[1413] - loss: 0.002560 
Batch[1414] - loss: 0.003156 
Batch[1415] - loss: 0.001370 
Batch[1416] - loss: 0.000866 
Batch[1417] - loss: 0.001246 
Batch[1418] - loss: 0.001205 
Batch[1419] - loss: 0.001291 
Batch[1420] - loss: 0.002053 
Batch[1421] - loss: 0.001456 
Batch[1422] - loss: 0.002346 
Batch[1423] - loss: 0.001339 
Batch[1424] - loss: 0.001113 
Batch[1425] - loss: 0.001551 
Batch[1426] - loss: 0.001068 
Batch[1427] - loss: 0.001180 
Batch[1428] - loss: 0.001289 
Batch[1429] - loss: 0.001754 
Batch[1430] - loss: 0.002161 
Batch[1431] - loss: 0.000859 
Batch[1432] - loss: 0.001336 
Batch[1433] - loss: 0.000781 
Batch[1434] - loss: 0.001524 
Batch[1435] - loss: 0.002117 
Batch[1436] - loss: 0.001101 
Batch[1437] - loss: 0.001202 
Batch[1438] - loss: 0.000972 
Batch[1439] - loss: 0.001432 
Batch[1440] - loss: 0.000828 
Batch[1441] - loss: 0.000769 
Batch[1442] - loss: 0.001816 
Batch[1443] - loss: 0.001789 
Batch[1444] - loss: 0.001114 
Batch[1445] - loss: 0.001361 
Batch[1446] - loss: 0.001222 
Batch[1447] - loss: 0.001842 
Batch[1448] - loss: 0.001042 
Batch[1449] - loss: 0.002988 
Batch[1450] - loss: 0.001480 
Batch[1451] - loss: 0.002179 
Batch[1452] - loss: 0.001347 
Batch[1453] - loss: 0.002288 
Batch[1454] - loss: 0.001781 
Batch[1455] - loss: 0.001278 
Batch[1456] - loss: 0.001025 
Batch[1457] - loss: 0.001251 
Batch[1458] - loss: 0.001581 
Batch[1459] - loss: 0.003449 
Batch[1460] - loss: 0.002841 
Batch[1461] - loss: 0.002394 
Batch[1462] - loss: 0.001433 
Batch[1463] - loss: 0.000911 
Batch[1464] - loss: 0.001133 
Batch[1465] - loss: 0.002849 
Batch[1466] - loss: 0.002529 
Batch[1467] - loss: 0.001908 
Batch[1468] - loss: 0.001958 
Batch[1469] - loss: 0.001118 
Batch[1470] - loss: 0.002284 
Batch[1471] - loss: 0.001754 
Batch[1472] - loss: 0.002163 
Batch[1473] - loss: 0.002872 
Batch[1474] - loss: 0.001596 
Batch[1475] - loss: 0.001922 
Batch[1476] - loss: 0.001323 
Batch[1477] - loss: 0.002518 
Batch[1478] - loss: 0.001876 
Batch[1479] - loss: 0.001058 
Batch[1480] - loss: 0.002107 
Batch[1481] - loss: 0.001090 
Batch[1482] - loss: 0.001184 
Batch[1483] - loss: 0.001108 
Batch[1484] - loss: 0.001017 
Batch[1485] - loss: 0.001543 
Batch[1486] - loss: 0.001786 
Batch[1487] - loss: 0.001445 
Batch[1488] - loss: 0.001789 
Batch[1489] - loss: 0.002610 
Batch[1490] - loss: 0.001389 
Batch[1491] - loss: 0.000885 
Batch[1492] - loss: 0.001387 
Batch[1493] - loss: 0.001417 
Batch[1494] - loss: 0.001443 
Batch[1495] - loss: 0.002677 
Batch[1496] - loss: 0.001723 
Batch[1497] - loss: 0.002024 
Batch[1498] - loss: 0.001079 
Batch[1499] - loss: 0.001232 
Batch[1500] - loss: 0.003013 

Evaluation - loss: 0.000068 pearson: 0.5505 

Batch[1501] - loss: 0.001672 
Batch[1502] - loss: 0.002687 
Batch[1503] - loss: 0.001989 
Batch[1504] - loss: 0.002088 
Batch[1505] - loss: 0.001780 
Batch[1506] - loss: 0.001627 
Batch[1507] - loss: 0.000920 
Batch[1508] - loss: 0.001170 
Batch[1509] - loss: 0.001160 
Batch[1510] - loss: 0.001047 
Batch[1511] - loss: 0.000819 
Batch[1512] - loss: 0.001335 
Batch[1513] - loss: 0.003409 
Batch[1514] - loss: 0.000692 
Batch[1515] - loss: 0.000874 
Batch[1516] - loss: 0.002001 
Batch[1517] - loss: 0.000922 
Batch[1518] - loss: 0.001083 
Batch[1519] - loss: 0.001185 
Batch[1520] - loss: 0.002097 
Batch[1521] - loss: 0.001344 
Batch[1522] - loss: 0.001424 
Batch[1523] - loss: 0.001185 
Batch[1524] - loss: 0.002859 
Batch[1525] - loss: 0.002925 
Batch[1526] - loss: 0.001593 
Batch[1527] - loss: 0.001789 
Batch[1528] - loss: 0.001048 
Batch[1529] - loss: 0.001655 
Batch[1530] - loss: 0.003335 
Batch[1531] - loss: 0.001217 
Batch[1532] - loss: 0.003295 
Batch[1533] - loss: 0.002373 
Batch[1534] - loss: 0.000903 
Batch[1535] - loss: 0.003106 
Batch[1536] - loss: 0.001288 
Batch[1537] - loss: 0.002281 
Batch[1538] - loss: 0.001744 
Batch[1539] - loss: 0.001161 
Batch[1540] - loss: 0.001754 
Batch[1541] - loss: 0.001343 
Batch[1542] - loss: 0.002299 
Batch[1543] - loss: 0.002121 
Batch[1544] - loss: 0.000955 
Batch[1545] - loss: 0.001602 
Batch[1546] - loss: 0.001375 
Batch[1547] - loss: 0.002016 
Batch[1548] - loss: 0.001415 
Batch[1549] - loss: 0.004459 
Batch[1550] - loss: 0.001388 
Batch[1551] - loss: 0.003119 
Batch[1552] - loss: 0.001145 
Batch[1553] - loss: 0.001677 
Batch[1554] - loss: 0.002822 
Batch[1555] - loss: 0.002100 
Batch[1556] - loss: 0.001463 
Batch[1557] - loss: 0.001909 
Batch[1558] - loss: 0.002973 
Batch[1559] - loss: 0.001739 
Batch[1560] - loss: 0.002003 
Batch[1561] - loss: 0.000984 
Batch[1562] - loss: 0.001380 
Batch[1563] - loss: 0.001213 
Batch[1564] - loss: 0.001730 
Batch[1565] - loss: 0.002537 
Batch[1566] - loss: 0.001901 
Batch[1567] - loss: 0.001777 
Batch[1568] - loss: 0.001633 
Batch[1569] - loss: 0.001238 
Batch[1570] - loss: 0.001701 
Batch[1571] - loss: 0.001187 
Batch[1572] - loss: 0.001429 
Batch[1573] - loss: 0.001527 
Batch[1574] - loss: 0.001845 
Batch[1575] - loss: 0.000855 
Batch[1576] - loss: 0.001023 
Batch[1577] - loss: 0.001094 
Batch[1578] - loss: 0.001356 
Batch[1579] - loss: 0.001824 
Batch[1580] - loss: 0.001430 
Batch[1581] - loss: 0.001172 
Batch[1582] - loss: 0.002611 
Batch[1583] - loss: 0.002685 
Batch[1584] - loss: 0.001864 
Batch[1585] - loss: 0.001012 
Batch[1586] - loss: 0.001242 
Batch[1587] - loss: 0.001435 
Batch[1588] - loss: 0.001323 
Batch[1589] - loss: 0.001136 
Batch[1590] - loss: 0.005724 
Batch[1591] - loss: 0.002476 
Batch[1592] - loss: 0.001969 
Batch[1593] - loss: 0.000639 
Batch[1594] - loss: 0.001676 
Batch[1595] - loss: 0.001006 
Batch[1596] - loss: 0.002409 
Batch[1597] - loss: 0.000966 
Batch[1598] - loss: 0.001675 
Batch[1599] - loss: 0.002182 
Batch[1600] - loss: 0.000832 

Evaluation - loss: 0.000068 pearson: 0.5482 

Batch[1601] - loss: 0.001396 
Batch[1602] - loss: 0.002107 
Batch[1603] - loss: 0.002331 
Batch[1604] - loss: 0.001396 
Batch[1605] - loss: 0.000726 
Batch[1606] - loss: 0.001851 
Batch[1607] - loss: 0.002312 
Batch[1608] - loss: 0.001084 
Batch[1609] - loss: 0.000991 
Batch[1610] - loss: 0.001691 
Batch[1611] - loss: 0.001371 
Batch[1612] - loss: 0.002936 
Batch[1613] - loss: 0.002023 
Batch[1614] - loss: 0.000984 
Batch[1615] - loss: 0.001893 
Batch[1616] - loss: 0.001709 
Batch[1617] - loss: 0.001095 
Batch[1618] - loss: 0.000984 
Batch[1619] - loss: 0.001253 
Batch[1620] - loss: 0.002197 
Batch[1621] - loss: 0.001143 
Batch[1622] - loss: 0.001180 
Batch[1623] - loss: 0.001772 
Batch[1624] - loss: 0.001803 
Batch[1625] - loss: 0.001166 
Batch[1626] - loss: 0.002316 
Batch[1627] - loss: 0.001644 
Batch[1628] - loss: 0.001389 
Batch[1629] - loss: 0.002340 
Batch[1630] - loss: 0.002034 
Batch[1631] - loss: 0.001643 
Batch[1632] - loss: 0.001162 
Batch[1633] - loss: 0.001889 
Batch[1634] - loss: 0.000887 
Batch[1635] - loss: 0.001821 
Batch[1636] - loss: 0.001682 
Batch[1637] - loss: 0.002236 
Batch[1638] - loss: 0.001911 
Batch[1639] - loss: 0.000859 
Batch[1640] - loss: 0.001269 
Batch[1641] - loss: 0.003042 
Batch[1642] - loss: 0.001329 
Batch[1643] - loss: 0.001591 
Batch[1644] - loss: 0.001426 
Batch[1645] - loss: 0.002306 
Batch[1646] - loss: 0.001469 
Batch[1647] - loss: 0.001867 
Batch[1648] - loss: 0.001701 
Batch[1649] - loss: 0.001082 
Batch[1650] - loss: 0.000915 
Batch[1651] - loss: 0.000824 
Batch[1652] - loss: 0.000681 
Batch[1653] - loss: 0.001481 
Batch[1654] - loss: 0.000972 
Batch[1655] - loss: 0.001044 
Batch[1656] - loss: 0.001478 
Batch[1657] - loss: 0.001079 
Batch[1658] - loss: 0.000761 
Batch[1659] - loss: 0.001638 
Batch[1660] - loss: 0.001541 
Batch[1661] - loss: 0.002863 
Batch[1662] - loss: 0.001872 
Batch[1663] - loss: 0.002045 
Batch[1664] - loss: 0.001634 
Batch[1665] - loss: 0.002488 
Batch[1666] - loss: 0.000868 
Batch[1667] - loss: 0.001221 
Batch[1668] - loss: 0.001391 
Batch[1669] - loss: 0.001367 
Batch[1670] - loss: 0.001655 
Batch[1671] - loss: 0.001386 
Batch[1672] - loss: 0.001572 
Batch[1673] - loss: 0.001480 
Batch[1674] - loss: 0.000879 
Batch[1675] - loss: 0.000664 
Batch[1676] - loss: 0.001198 
Batch[1677] - loss: 0.001471 
Batch[1678] - loss: 0.000904 
Batch[1679] - loss: 0.001053 
Batch[1680] - loss: 0.000631 
Batch[1681] - loss: 0.001600 
Batch[1682] - loss: 0.001662 
Batch[1683] - loss: 0.001457 
Batch[1684] - loss: 0.002703 
Batch[1685] - loss: 0.000935 
Batch[1686] - loss: 0.001223 
Batch[1687] - loss: 0.001348 
Batch[1688] - loss: 0.001119 
Batch[1689] - loss: 0.002015 
Batch[1690] - loss: 0.001077 
Batch[1691] - loss: 0.001269 
Batch[1692] - loss: 0.001242 
Batch[1693] - loss: 0.001178 
Batch[1694] - loss: 0.001856 
Batch[1695] - loss: 0.002015 
Batch[1696] - loss: 0.001128 
Batch[1697] - loss: 0.001463 
Batch[1698] - loss: 0.000977 
Batch[1699] - loss: 0.001795 
Batch[1700] - loss: 0.001365 

Evaluation - loss: 0.000067 pearson: 0.5525 

Batch[1701] - loss: 0.001272 
Batch[1702] - loss: 0.001562 
Batch[1703] - loss: 0.001600 
Batch[1704] - loss: 0.002030 
Batch[1705] - loss: 0.001395 
Batch[1706] - loss: 0.000869 
Batch[1707] - loss: 0.001589 
Batch[1708] - loss: 0.001793 
Batch[1709] - loss: 0.001589 
Batch[1710] - loss: 0.001418 
Batch[1711] - loss: 0.000923 
Batch[1712] - loss: 0.002683 
Batch[1713] - loss: 0.002045 
Batch[1714] - loss: 0.001626 
Batch[1715] - loss: 0.000760 
Batch[1716] - loss: 0.001142 
Batch[1717] - loss: 0.001282 
Batch[1718] - loss: 0.000836 
Batch[1719] - loss: 0.001073 
Batch[1720] - loss: 0.001360 
Batch[1721] - loss: 0.001030 
Batch[1722] - loss: 0.001551 
Batch[1723] - loss: 0.001414 
Batch[1724] - loss: 0.001557 
Batch[1725] - loss: 0.001564 
Batch[1726] - loss: 0.000747 
Batch[1727] - loss: 0.001092 
Batch[1728] - loss: 0.000891 
Batch[1729] - loss: 0.001311 
Batch[1730] - loss: 0.000847 
Batch[1731] - loss: 0.000783 
Batch[1732] - loss: 0.000843 
Batch[1733] - loss: 0.001682 
Batch[1734] - loss: 0.002119 
Batch[1735] - loss: 0.000740 
Batch[1736] - loss: 0.002097 
Batch[1737] - loss: 0.000821 
Batch[1738] - loss: 0.001989 
Batch[1739] - loss: 0.001903 
Batch[1740] - loss: 0.000663 
Batch[1741] - loss: 0.001045 
Batch[1742] - loss: 0.001360 
Batch[1743] - loss: 0.002041 
Batch[1744] - loss: 0.001185 
Batch[1745] - loss: 0.000987 
Batch[1746] - loss: 0.001563 
Batch[1747] - loss: 0.002859 
Batch[1748] - loss: 0.001910 
Batch[1749] - loss: 0.001364 
Batch[1750] - loss: 0.001557 
Batch[1751] - loss: 0.002391 
Batch[1752] - loss: 0.001563 
Batch[1753] - loss: 0.002386 
Batch[1754] - loss: 0.001138 
Batch[1755] - loss: 0.002724 
Batch[1756] - loss: 0.000842 
Batch[1757] - loss: 0.002974 
Batch[1758] - loss: 0.001974 
Batch[1759] - loss: 0.001379 
Batch[1760] - loss: 0.001116 
Batch[1761] - loss: 0.001771 
Batch[1762] - loss: 0.001102 
Batch[1763] - loss: 0.002102 
Batch[1764] - loss: 0.001821 
Batch[1765] - loss: 0.000889 
Batch[1766] - loss: 0.001118 
Batch[1767] - loss: 0.001420 
Batch[1768] - loss: 0.000852 
Batch[1769] - loss: 0.001179 
Batch[1770] - loss: 0.001204 
Batch[1771] - loss: 0.000753 
Batch[1772] - loss: 0.001489 
Batch[1773] - loss: 0.000673 
Batch[1774] - loss: 0.001699 
Batch[1775] - loss: 0.001384 
Batch[1776] - loss: 0.002358 
Batch[1777] - loss: 0.001341 
Batch[1778] - loss: 0.001066 
Batch[1779] - loss: 0.000815 
Batch[1780] - loss: 0.000915 
Batch[1781] - loss: 0.001205 
Batch[1782] - loss: 0.001225 
Batch[1783] - loss: 0.001218 
Batch[1784] - loss: 0.001912 
Batch[1785] - loss: 0.001328 
Batch[1786] - loss: 0.001945 
Batch[1787] - loss: 0.001265 
Batch[1788] - loss: 0.001757 
Batch[1789] - loss: 0.000728 
Batch[1790] - loss: 0.000731 
Batch[1791] - loss: 0.000773 
Batch[1792] - loss: 0.000841 
Batch[1793] - loss: 0.001385 
Batch[1794] - loss: 0.001394 
Batch[1795] - loss: 0.001263 
Batch[1796] - loss: 0.000612 
Batch[1797] - loss: 0.001837 
Batch[1798] - loss: 0.000667 
Batch[1799] - loss: 0.001637 
Batch[1800] - loss: 0.000928 

Evaluation - loss: 0.000068 pearson: 0.5501 

Batch[1801] - loss: 0.001075 
Batch[1802] - loss: 0.000593 
Batch[1803] - loss: 0.001355 
Batch[1804] - loss: 0.001016 
Batch[1805] - loss: 0.001849 
Batch[1806] - loss: 0.000753 
Batch[1807] - loss: 0.001260 
Batch[1808] - loss: 0.001314 
Batch[1809] - loss: 0.001685 
Batch[1810] - loss: 0.001109 
Batch[1811] - loss: 0.000718 
Batch[1812] - loss: 0.002661 
Batch[1813] - loss: 0.001966 
Batch[1814] - loss: 0.001453 
Batch[1815] - loss: 0.002032 
Batch[1816] - loss: 0.001763 
Batch[1817] - loss: 0.001445 
Batch[1818] - loss: 0.001055 
Batch[1819] - loss: 0.001451 
Batch[1820] - loss: 0.001707 
Batch[1821] - loss: 0.002169 
Batch[1822] - loss: 0.000904 
Batch[1823] - loss: 0.001273 
Batch[1824] - loss: 0.001204 
Batch[1825] - loss: 0.000499 
Batch[1826] - loss: 0.001611 
Batch[1827] - loss: 0.001307 
Batch[1828] - loss: 0.001422 
Batch[1829] - loss: 0.002223 
Batch[1830] - loss: 0.001482 
Batch[1831] - loss: 0.002509 
Batch[1832] - loss: 0.001431 
Batch[1833] - loss: 0.000730 
Batch[1834] - loss: 0.001908 
Batch[1835] - loss: 0.000633 
Batch[1836] - loss: 0.001256 
Batch[1837] - loss: 0.002208 
Batch[1838] - loss: 0.001579 
Batch[1839] - loss: 0.001757 
Batch[1840] - loss: 0.001164 
Batch[1841] - loss: 0.001648 
Batch[1842] - loss: 0.001577 
Batch[1843] - loss: 0.001643 
Batch[1844] - loss: 0.000974 
Batch[1845] - loss: 0.001148 
Batch[1846] - loss: 0.002000 
Batch[1847] - loss: 0.002229 
Batch[1848] - loss: 0.000918 
Batch[1849] - loss: 0.001407 
Batch[1850] - loss: 0.002076 
Batch[1851] - loss: 0.001023 
Batch[1852] - loss: 0.000766 
Batch[1853] - loss: 0.001999 
Batch[1854] - loss: 0.001509 
Batch[1855] - loss: 0.001329 
Batch[1856] - loss: 0.000805 
Batch[1857] - loss: 0.001387 
Batch[1858] - loss: 0.000828 
Batch[1859] - loss: 0.001833 
Batch[1860] - loss: 0.001024 
Batch[1861] - loss: 0.000935 
Batch[1862] - loss: 0.000777 
Batch[1863] - loss: 0.002056 
Batch[1864] - loss: 0.000684 
Batch[1865] - loss: 0.000879 
Batch[1866] - loss: 0.001742 
Batch[1867] - loss: 0.001712 
Batch[1868] - loss: 0.001060 
Batch[1869] - loss: 0.000984 
Batch[1870] - loss: 0.002357 
Batch[1871] - loss: 0.001160 
Batch[1872] - loss: 0.001841 
Batch[1873] - loss: 0.001357 
Batch[1874] - loss: 0.001213 
Batch[1875] - loss: 0.002062 
Batch[1876] - loss: 0.001072 
Batch[1877] - loss: 0.001415 
Batch[1878] - loss: 0.001481 
Batch[1879] - loss: 0.001145 
Batch[1880] - loss: 0.001070 
Batch[1881] - loss: 0.000961 
Batch[1882] - loss: 0.001132 
Batch[1883] - loss: 0.001887 
Batch[1884] - loss: 0.000659 
Batch[1885] - loss: 0.001792 
Batch[1886] - loss: 0.000891 
Batch[1887] - loss: 0.001731 
Batch[1888] - loss: 0.001501 
Batch[1889] - loss: 0.000782 
Batch[1890] - loss: 0.001097 
Batch[1891] - loss: 0.001307 
Batch[1892] - loss: 0.001645 
Batch[1893] - loss: 0.001445 
Batch[1894] - loss: 0.000858 
Batch[1895] - loss: 0.001252 
Batch[1896] - loss: 0.001307 
Batch[1897] - loss: 0.001107 
Batch[1898] - loss: 0.002369 
Batch[1899] - loss: 0.001127 
Batch[1900] - loss: 0.001345 

Evaluation - loss: 0.000067 pearson: 0.5515 

Batch[1901] - loss: 0.000853 
Batch[1902] - loss: 0.001074 
Batch[1903] - loss: 0.001419 
Batch[1904] - loss: 0.000781 
Batch[1905] - loss: 0.000914 
Batch[1906] - loss: 0.001647 
Batch[1907] - loss: 0.001824 
Batch[1908] - loss: 0.001677 
Batch[1909] - loss: 0.001566 
Batch[1910] - loss: 0.001431 
Batch[1911] - loss: 0.001147 
Batch[1912] - loss: 0.001309 
Batch[1913] - loss: 0.001572 
Batch[1914] - loss: 0.001392 
Batch[1915] - loss: 0.001655 
Batch[1916] - loss: 0.003182 
Batch[1917] - loss: 0.001183 
Batch[1918] - loss: 0.001438 
Batch[1919] - loss: 0.001257 
Batch[1920] - loss: 0.001163 
Batch[1921] - loss: 0.001894 
Batch[1922] - loss: 0.001275 
Batch[1923] - loss: 0.001161 
Batch[1924] - loss: 0.001137 
Batch[1925] - loss: 0.001632 
Batch[1926] - loss: 0.001879 
Batch[1927] - loss: 0.002902 
Batch[1928] - loss: 0.000919 
Batch[1929] - loss: 0.001706 
Batch[1930] - loss: 0.002694 
Batch[1931] - loss: 0.001432 
Batch[1932] - loss: 0.002556 
Batch[1933] - loss: 0.000875 
Batch[1934] - loss: 0.002071 
Batch[1935] - loss: 0.000984 
Batch[1936] - loss: 0.001331 
Batch[1937] - loss: 0.001256 
Batch[1938] - loss: 0.001200 
Batch[1939] - loss: 0.000987 
Batch[1940] - loss: 0.000927 
Batch[1941] - loss: 0.000907 
Batch[1942] - loss: 0.001732 
Batch[1943] - loss: 0.001495 
Batch[1944] - loss: 0.000880 
Batch[1945] - loss: 0.001346 
Batch[1946] - loss: 0.002054 
Batch[1947] - loss: 0.002106 
Batch[1948] - loss: 0.000959 
Batch[1949] - loss: 0.001331 
Batch[1950] - loss: 0.001278 
Batch[1951] - loss: 0.001220 
Batch[1952] - loss: 0.001527 
Batch[1953] - loss: 0.000780 
Batch[1954] - loss: 0.001253 
Batch[1955] - loss: 0.001604 
Batch[1956] - loss: 0.001261 
Batch[1957] - loss: 0.001167 
Batch[1958] - loss: 0.000950 
Batch[1959] - loss: 0.002153 
Batch[1960] - loss: 0.001612 
Batch[1961] - loss: 0.001542 
Batch[1962] - loss: 0.001190 
Batch[1963] - loss: 0.001088 
Batch[1964] - loss: 0.001171 
Batch[1965] - loss: 0.001645 
Batch[1966] - loss: 0.002366 
Batch[1967] - loss: 0.001184 
Batch[1968] - loss: 0.001577 
Batch[1969] - loss: 0.001202 
Batch[1970] - loss: 0.001948 
Batch[1971] - loss: 0.001243 
Batch[1972] - loss: 0.001062 
Batch[1973] - loss: 0.000913 
Batch[1974] - loss: 0.000832 
Batch[1975] - loss: 0.001750 
Batch[1976] - loss: 0.001747 
Batch[1977] - loss: 0.001724 
Batch[1978] - loss: 0.001388 
Batch[1979] - loss: 0.002147 
Batch[1980] - loss: 0.000923 
Batch[1981] - loss: 0.001921 
Batch[1982] - loss: 0.000623 
Batch[1983] - loss: 0.001059 
Batch[1984] - loss: 0.000824 
Batch[1985] - loss: 0.001052 
Batch[1986] - loss: 0.002683 
Batch[1987] - loss: 0.001464 
Batch[1988] - loss: 0.001527 
Batch[1989] - loss: 0.000864 
Batch[1990] - loss: 0.000914 
Batch[1991] - loss: 0.000797 
Batch[1992] - loss: 0.000701 
Batch[1993] - loss: 0.000547 
Batch[1994] - loss: 0.001520 
Batch[1995] - loss: 0.001585 
Batch[1996] - loss: 0.000965 
Batch[1997] - loss: 0.000896 
Batch[1998] - loss: 0.001065 
Batch[1999] - loss: 0.001190 
Batch[2000] - loss: 0.000536 

Evaluation - loss: 0.000067 pearson: 0.5592 

Batch[2001] - loss: 0.002591 
Batch[2002] - loss: 0.000758 
Batch[2003] - loss: 0.000746 
Batch[2004] - loss: 0.000588 
Batch[2005] - loss: 0.001335 
Batch[2006] - loss: 0.000602 
Batch[2007] - loss: 0.000643 
Batch[2008] - loss: 0.000899 
Batch[2009] - loss: 0.002121 
Batch[2010] - loss: 0.001405 
Batch[2011] - loss: 0.000956 
Batch[2012] - loss: 0.001385 
Batch[2013] - loss: 0.001324 
Batch[2014] - loss: 0.000923 
Batch[2015] - loss: 0.001176 
Batch[2016] - loss: 0.000735 
Batch[2017] - loss: 0.001166 
Batch[2018] - loss: 0.001047 
Batch[2019] - loss: 0.001205 
Batch[2020] - loss: 0.001187 
Batch[2021] - loss: 0.001443 
Batch[2022] - loss: 0.001370 
Batch[2023] - loss: 0.000872 
Batch[2024] - loss: 0.000687 
Batch[2025] - loss: 0.001121 
Batch[2026] - loss: 0.001753 
Batch[2027] - loss: 0.000732 
Batch[2028] - loss: 0.001607 
Batch[2029] - loss: 0.001267 
Batch[2030] - loss: 0.001063 
Batch[2031] - loss: 0.002830 
Batch[2032] - loss: 0.001422 
Batch[2033] - loss: 0.001510 
Batch[2034] - loss: 0.001175 
Batch[2035] - loss: 0.000761 
Batch[2036] - loss: 0.000920 
Batch[2037] - loss: 0.001543 
Batch[2038] - loss: 0.002087 
Batch[2039] - loss: 0.000801 
Batch[2040] - loss: 0.000970 
Batch[2041] - loss: 0.001434 
Batch[2042] - loss: 0.000826 
Batch[2043] - loss: 0.001062 
Batch[2044] - loss: 0.000976 
Batch[2045] - loss: 0.001382 
Batch[2046] - loss: 0.001675 
Batch[2047] - loss: 0.001538 
Batch[2048] - loss: 0.000644 
Batch[2049] - loss: 0.001267 
Batch[2050] - loss: 0.001165 
Batch[2051] - loss: 0.002092 
Batch[2052] - loss: 0.001675 
Batch[2053] - loss: 0.000856 
Batch[2054] - loss: 0.000935 
Batch[2055] - loss: 0.001048 
Batch[2056] - loss: 0.001310 
Batch[2057] - loss: 0.001113 
Batch[2058] - loss: 0.001855 
Batch[2059] - loss: 0.001184 
Batch[2060] - loss: 0.002405 
Batch[2061] - loss: 0.000692 
Batch[2062] - loss: 0.001842 
Batch[2063] - loss: 0.002650 
Batch[2064] - loss: 0.001187 
Batch[2065] - loss: 0.001200 
Batch[2066] - loss: 0.001439 
Batch[2067] - loss: 0.000931 
Batch[2068] - loss: 0.001101 
Batch[2069] - loss: 0.001126 
Batch[2070] - loss: 0.000824 
Batch[2071] - loss: 0.000963 
Batch[2072] - loss: 0.001614 
Batch[2073] - loss: 0.001182 
Batch[2074] - loss: 0.000841 
Batch[2075] - loss: 0.000759 
Batch[2076] - loss: 0.001545 
Batch[2077] - loss: 0.001005 
Batch[2078] - loss: 0.000949 
Batch[2079] - loss: 0.001396 
Batch[2080] - loss: 0.001411 
Batch[2081] - loss: 0.001181 
Batch[2082] - loss: 0.001333 
Batch[2083] - loss: 0.000948 
Batch[2084] - loss: 0.001119 
Batch[2085] - loss: 0.000917 
Batch[2086] - loss: 0.001001 
Batch[2087] - loss: 0.001445 
Batch[2088] - loss: 0.001773 
Batch[2089] - loss: 0.000636 
Batch[2090] - loss: 0.001014 
Batch[2091] - loss: 0.001719 
Batch[2092] - loss: 0.001422 
Batch[2093] - loss: 0.001002 
Batch[2094] - loss: 0.001040 
Batch[2095] - loss: 0.001475 
Batch[2096] - loss: 0.001358 
Batch[2097] - loss: 0.000755 
Batch[2098] - loss: 0.000850 
Batch[2099] - loss: 0.001178 
Batch[2100] - loss: 0.001195 

Evaluation - loss: 0.000067 pearson: 0.5540 

Batch[2101] - loss: 0.002142 
Batch[2102] - loss: 0.001856 
Batch[2103] - loss: 0.001515 
Batch[2104] - loss: 0.000897 
Batch[2105] - loss: 0.001081 
Batch[2106] - loss: 0.001013 
Batch[2107] - loss: 0.001406 
Batch[2108] - loss: 0.001100 
Batch[2109] - loss: 0.000977 
Batch[2110] - loss: 0.001124 
Batch[2111] - loss: 0.000943 
Batch[2112] - loss: 0.002030 
Batch[2113] - loss: 0.001383 
Batch[2114] - loss: 0.000802 
Batch[2115] - loss: 0.001238 
Batch[2116] - loss: 0.000721 
Batch[2117] - loss: 0.002589 
Batch[2118] - loss: 0.001530 
Batch[2119] - loss: 0.001120 
Batch[2120] - loss: 0.001249 
Batch[2121] - loss: 0.001308 
Batch[2122] - loss: 0.000822 
Batch[2123] - loss: 0.001099 
Batch[2124] - loss: 0.001417 
Batch[2125] - loss: 0.001457 
Batch[2126] - loss: 0.000822 
Batch[2127] - loss: 0.001849 
Batch[2128] - loss: 0.001558 
Batch[2129] - loss: 0.000751 
Batch[2130] - loss: 0.000628 
Batch[2131] - loss: 0.001532 
Batch[2132] - loss: 0.001225 
Batch[2133] - loss: 0.002370 
Batch[2134] - loss: 0.000863 
Batch[2135] - loss: 0.001315 
Batch[2136] - loss: 0.001208 
Batch[2137] - loss: 0.000841 
Batch[2138] - loss: 0.001057 
Batch[2139] - loss: 0.001150 
Batch[2140] - loss: 0.000445 
Batch[2141] - loss: 0.000807 
Batch[2142] - loss: 0.001785 
Batch[2143] - loss: 0.001219 
Batch[2144] - loss: 0.001368 
Batch[2145] - loss: 0.000406 
Batch[2146] - loss: 0.001791 
Batch[2147] - loss: 0.001209 
Batch[2148] - loss: 0.000868 
Batch[2149] - loss: 0.000783 
Batch[2150] - loss: 0.001577 
Batch[2151] - loss: 0.001580 
Batch[2152] - loss: 0.001152 
Batch[2153] - loss: 0.000838 
Batch[2154] - loss: 0.000648 
Batch[2155] - loss: 0.001064 
Batch[2156] - loss: 0.001747 
Batch[2157] - loss: 0.001536 
Batch[2158] - loss: 0.000606 
Batch[2159] - loss: 0.001385 
Batch[2160] - loss: 0.001034 
Batch[2161] - loss: 0.001475 
Batch[2162] - loss: 0.001131 
Batch[2163] - loss: 0.000828 
Batch[2164] - loss: 0.001173 
Batch[2165] - loss: 0.001946 
Batch[2166] - loss: 0.002703 
Batch[2167] - loss: 0.000822 
Batch[2168] - loss: 0.000969 
Batch[2169] - loss: 0.001097 
Batch[2170] - loss: 0.000814 
Batch[2171] - loss: 0.001650 
Batch[2172] - loss: 0.001516 
Batch[2173] - loss: 0.001923 
Batch[2174] - loss: 0.000859 
Batch[2175] - loss: 0.001085 
Batch[2176] - loss: 0.001267 
Batch[2177] - loss: 0.000886 
Batch[2178] - loss: 0.001132 
Batch[2179] - loss: 0.000863 
Batch[2180] - loss: 0.001853 
Batch[2181] - loss: 0.000901 
Batch[2182] - loss: 0.001531 
Batch[2183] - loss: 0.000644 
Batch[2184] - loss: 0.001123 
Batch[2185] - loss: 0.002111 
Batch[2186] - loss: 0.000957 
Batch[2187] - loss: 0.000952 
Batch[2188] - loss: 0.001612 
Batch[2189] - loss: 0.001073 
Batch[2190] - loss: 0.000891 
Batch[2191] - loss: 0.001198 
Batch[2192] - loss: 0.001452 
Batch[2193] - loss: 0.000861 
Batch[2194] - loss: 0.001551 
Batch[2195] - loss: 0.002217 
Batch[2196] - loss: 0.000821 
Batch[2197] - loss: 0.000690 
Batch[2198] - loss: 0.001391 
Batch[2199] - loss: 0.001177 
Batch[2200] - loss: 0.001267 

Evaluation - loss: 0.000067 pearson: 0.5522 

Batch[2201] - loss: 0.000595 
Batch[2202] - loss: 0.000737 
Batch[2203] - loss: 0.001538 
Batch[2204] - loss: 0.000863 
Batch[2205] - loss: 0.000755 
Batch[2206] - loss: 0.000777 
Batch[2207] - loss: 0.001723 
Batch[2208] - loss: 0.000926 
Batch[2209] - loss: 0.000545 
Batch[2210] - loss: 0.000839 
Batch[2211] - loss: 0.000847 
Batch[2212] - loss: 0.000775 
Batch[2213] - loss: 0.001903 
Batch[2214] - loss: 0.001342 
Batch[2215] - loss: 0.001327 
Batch[2216] - loss: 0.002243 
Batch[2217] - loss: 0.000663 
Batch[2218] - loss: 0.000816 
Batch[2219] - loss: 0.001067 
Batch[2220] - loss: 0.001134 
Batch[2221] - loss: 0.001349 
Batch[2222] - loss: 0.000800 
Batch[2223] - loss: 0.000712 
Batch[2224] - loss: 0.001039 
Batch[2225] - loss: 0.000873 
Batch[2226] - loss: 0.000999 
Batch[2227] - loss: 0.000989 
Batch[2228] - loss: 0.000751 
Batch[2229] - loss: 0.001044 
Batch[2230] - loss: 0.001362 
Batch[2231] - loss: 0.001356 
Batch[2232] - loss: 0.001072 
Batch[2233] - loss: 0.001573 
Batch[2234] - loss: 0.000927 
Batch[2235] - loss: 0.000818 
Batch[2236] - loss: 0.000850 
Batch[2237] - loss: 0.002438 
Batch[2238] - loss: 0.001375 
Batch[2239] - loss: 0.000802 
Batch[2240] - loss: 0.000658 
Batch[2241] - loss: 0.000904 
Batch[2242] - loss: 0.000859 
Batch[2243] - loss: 0.002416 
Batch[2244] - loss: 0.001224 
Batch[2245] - loss: 0.001643 
Batch[2246] - loss: 0.000812 
Batch[2247] - loss: 0.001041 
Batch[2248] - loss: 0.001096 
Batch[2249] - loss: 0.001103 
Batch[2250] - loss: 0.000774 
Batch[2251] - loss: 0.000965 
Batch[2252] - loss: 0.001203 
Batch[2253] - loss: 0.000873 
Batch[2254] - loss: 0.001267 
Batch[2255] - loss: 0.001073 
Batch[2256] - loss: 0.001847 
Batch[2257] - loss: 0.001522 
Batch[2258] - loss: 0.001103 
Batch[2259] - loss: 0.001156 
Batch[2260] - loss: 0.000799 
Batch[2261] - loss: 0.001141 
Batch[2262] - loss: 0.001061 
Batch[2263] - loss: 0.000797 
Batch[2264] - loss: 0.001482 
Batch[2265] - loss: 0.001030 
Batch[2266] - loss: 0.000934 
Batch[2267] - loss: 0.001168 
Batch[2268] - loss: 0.001640 
Batch[2269] - loss: 0.000786 
Batch[2270] - loss: 0.000910 
Batch[2271] - loss: 0.000841 
Batch[2272] - loss: 0.000666 
Batch[2273] - loss: 0.000541 
Batch[2274] - loss: 0.000882 
Batch[2275] - loss: 0.001006 
Batch[2276] - loss: 0.001486 
Batch[2277] - loss: 0.000725 
Batch[2278] - loss: 0.001083 
Batch[2279] - loss: 0.000857 
Batch[2280] - loss: 0.000892 
Batch[2281] - loss: 0.000947 
Batch[2282] - loss: 0.000930 
Batch[2283] - loss: 0.001058 
Batch[2284] - loss: 0.001266 
Batch[2285] - loss: 0.001136 
Batch[2286] - loss: 0.000790 
Batch[2287] - loss: 0.000600 
Batch[2288] - loss: 0.000883 
Batch[2289] - loss: 0.001176 
Batch[2290] - loss: 0.001136 
Batch[2291] - loss: 0.000993 
Batch[2292] - loss: 0.000777 
Batch[2293] - loss: 0.001199 
Batch[2294] - loss: 0.000934 
Batch[2295] - loss: 0.001292 
Batch[2296] - loss: 0.001841 
Batch[2297] - loss: 0.000979 
Batch[2298] - loss: 0.001445 
Batch[2299] - loss: 0.000980 
Batch[2300] - loss: 0.001053 

Evaluation - loss: 0.000067 pearson: 0.5525 

Batch[2301] - loss: 0.000844 
Batch[2302] - loss: 0.001185 
Batch[2303] - loss: 0.001175 
Batch[2304] - loss: 0.000623 
Batch[2305] - loss: 0.001713 
Batch[2306] - loss: 0.000512 
Batch[2307] - loss: 0.001195 
Batch[2308] - loss: 0.001216 
Batch[2309] - loss: 0.001217 
Batch[2310] - loss: 0.003187 
Batch[2311] - loss: 0.001413 
Batch[2312] - loss: 0.001557 
Batch[2313] - loss: 0.001970 
Batch[2314] - loss: 0.000923 
Batch[2315] - loss: 0.001081 
Batch[2316] - loss: 0.000897 
Batch[2317] - loss: 0.001123 
Batch[2318] - loss: 0.001391 
Batch[2319] - loss: 0.000910 
Batch[2320] - loss: 0.001538 
Batch[2321] - loss: 0.001055 
Batch[2322] - loss: 0.000592 
Batch[2323] - loss: 0.001080 
Batch[2324] - loss: 0.001805 
Batch[2325] - loss: 0.001199 
Batch[2326] - loss: 0.000542 
Batch[2327] - loss: 0.001027 
Batch[2328] - loss: 0.001031 
Batch[2329] - loss: 0.000697 
Batch[2330] - loss: 0.001501 
Batch[2331] - loss: 0.001420 
Batch[2332] - loss: 0.001294 
Batch[2333] - loss: 0.000935 
Batch[2334] - loss: 0.001023 
Batch[2335] - loss: 0.000624 
Batch[2336] - loss: 0.000568 
Batch[2337] - loss: 0.000648 
Batch[2338] - loss: 0.000802 
Batch[2339] - loss: 0.001085 
Batch[2340] - loss: 0.001117 
Batch[2341] - loss: 0.001561 
Batch[2342] - loss: 0.002190 
Batch[2343] - loss: 0.000928 
Batch[2344] - loss: 0.000767 
Batch[2345] - loss: 0.001270 
Batch[2346] - loss: 0.001504 
Batch[2347] - loss: 0.001166 
Batch[2348] - loss: 0.001309 
Batch[2349] - loss: 0.001751 
Batch[2350] - loss: 0.000676 
Batch[2351] - loss: 0.002208 
Batch[2352] - loss: 0.001072 
Batch[2353] - loss: 0.000501 
Batch[2354] - loss: 0.001336 
Batch[2355] - loss: 0.001385 
Batch[2356] - loss: 0.001914 
Batch[2357] - loss: 0.000769 
Batch[2358] - loss: 0.001351 
Batch[2359] - loss: 0.000908 
Batch[2360] - loss: 0.001169 
Batch[2361] - loss: 0.001443 
Batch[2362] - loss: 0.000615 
Batch[2363] - loss: 0.000787 
Batch[2364] - loss: 0.001589 
Batch[2365] - loss: 0.001512 
Batch[2366] - loss: 0.000379 
Batch[2367] - loss: 0.001400 
Batch[2368] - loss: 0.001492 
Batch[2369] - loss: 0.001634 
Batch[2370] - loss: 0.001584 
Batch[2371] - loss: 0.001085 
Batch[2372] - loss: 0.000887 
Batch[2373] - loss: 0.001341 
Batch[2374] - loss: 0.001129 
Batch[2375] - loss: 0.001217 
Batch[2376] - loss: 0.001152 
Batch[2377] - loss: 0.001357 
Batch[2378] - loss: 0.001897 
Batch[2379] - loss: 0.000651 
Batch[2380] - loss: 0.000808 
Batch[2381] - loss: 0.000915 
Batch[2382] - loss: 0.001395 
Batch[2383] - loss: 0.001840 
Batch[2384] - loss: 0.001126 
Batch[2385] - loss: 0.000609 
Batch[2386] - loss: 0.001493 
Batch[2387] - loss: 0.001434 
Batch[2388] - loss: 0.001537 
Batch[2389] - loss: 0.001393 
Batch[2390] - loss: 0.001253 
Batch[2391] - loss: 0.000905 
Batch[2392] - loss: 0.000817 
Batch[2393] - loss: 0.001862 
Batch[2394] - loss: 0.000724 
Batch[2395] - loss: 0.000833 
Batch[2396] - loss: 0.000996 
Batch[2397] - loss: 0.001171 
Batch[2398] - loss: 0.000624 
Batch[2399] - loss: 0.000861 
Batch[2400] - loss: 0.001558 

Evaluation - loss: 0.000068 pearson: 0.5490 

Batch[2401] - loss: 0.000843 
Batch[2402] - loss: 0.000990 
Batch[2403] - loss: 0.000937 
Batch[2404] - loss: 0.000681 
Batch[2405] - loss: 0.001750 
Batch[2406] - loss: 0.000946 
Batch[2407] - loss: 0.001240 
Batch[2408] - loss: 0.000694 
Batch[2409] - loss: 0.001059 
Batch[2410] - loss: 0.001182 
Batch[2411] - loss: 0.000723 
Batch[2412] - loss: 0.000680 
Batch[2413] - loss: 0.000957 
Batch[2414] - loss: 0.001049 
Batch[2415] - loss: 0.000540 
Batch[2416] - loss: 0.000884 
Batch[2417] - loss: 0.001742 
Batch[2418] - loss: 0.000815 
Batch[2419] - loss: 0.000764 
Batch[2420] - loss: 0.001220 
Batch[2421] - loss: 0.001334 
Batch[2422] - loss: 0.001721 
Batch[2423] - loss: 0.000941 
Batch[2424] - loss: 0.001107 
Batch[2425] - loss: 0.000651 
Batch[2426] - loss: 0.000864 
Batch[2427] - loss: 0.000812 
Batch[2428] - loss: 0.001340 
Batch[2429] - loss: 0.001174 
Batch[2430] - loss: 0.000914 
Batch[2431] - loss: 0.000959 
Batch[2432] - loss: 0.000620 
Batch[2433] - loss: 0.000766 
Batch[2434] - loss: 0.000834 
Batch[2435] - loss: 0.001355 
Batch[2436] - loss: 0.000718 
Batch[2437] - loss: 0.001039 
Batch[2438] - loss: 0.000865 
Batch[2439] - loss: 0.001830 
Batch[2440] - loss: 0.000634 
Batch[2441] - loss: 0.000990 
Batch[2442] - loss: 0.000631 
Batch[2443] - loss: 0.001256 
Batch[2444] - loss: 0.001454 
Batch[2445] - loss: 0.000660 
Batch[2446] - loss: 0.000619 
Batch[2447] - loss: 0.000924 
Batch[2448] - loss: 0.000849 
Batch[2449] - loss: 0.000971 
Batch[2450] - loss: 0.001039 
Batch[2451] - loss: 0.001036 
Batch[2452] - loss: 0.001644 
Batch[2453] - loss: 0.002023 
Batch[2454] - loss: 0.000740 
Batch[2455] - loss: 0.000671 
Batch[2456] - loss: 0.000682 
Batch[2457] - loss: 0.001186 
Batch[2458] - loss: 0.001189 
Batch[2459] - loss: 0.000925 
Batch[2460] - loss: 0.001295 
Batch[2461] - loss: 0.000896 
Batch[2462] - loss: 0.001051 
Batch[2463] - loss: 0.001475 
Batch[2464] - loss: 0.002516 
Batch[2465] - loss: 0.001053 
Batch[2466] - loss: 0.000531 
Batch[2467] - loss: 0.001111 
Batch[2468] - loss: 0.001336 
Batch[2469] - loss: 0.000697 
Batch[2470] - loss: 0.000740 
Batch[2471] - loss: 0.000807 
Batch[2472] - loss: 0.001029 
Batch[2473] - loss: 0.000827 
Batch[2474] - loss: 0.000432 
Batch[2475] - loss: 0.001112 
Batch[2476] - loss: 0.000713 
Batch[2477] - loss: 0.001187 
Batch[2478] - loss: 0.000380 
Batch[2479] - loss: 0.000919 
Batch[2480] - loss: 0.000594 
Batch[2481] - loss: 0.001005 
Batch[2482] - loss: 0.001314 
Batch[2483] - loss: 0.000453 
Batch[2484] - loss: 0.001107 
Batch[2485] - loss: 0.000822 
Batch[2486] - loss: 0.001058 
Batch[2487] - loss: 0.001282 
Batch[2488] - loss: 0.001509 
Batch[2489] - loss: 0.001017 
Batch[2490] - loss: 0.000731 
Batch[2491] - loss: 0.001578 
Batch[2492] - loss: 0.001043 
Batch[2493] - loss: 0.001500 
Batch[2494] - loss: 0.000859 
Batch[2495] - loss: 0.000923 
Batch[2496] - loss: 0.001027 
Batch[2497] - loss: 0.000597 
Batch[2498] - loss: 0.001401 
Batch[2499] - loss: 0.000751 
Batch[2500] - loss: 0.000731 

Evaluation - loss: 0.000067 pearson: 0.5527 

Batch[2501] - loss: 0.000826 
Batch[2502] - loss: 0.000926 
Batch[2503] - loss: 0.001674 
Batch[2504] - loss: 0.001057 
Batch[2505] - loss: 0.000901 
Batch[2506] - loss: 0.001070 
Batch[2507] - loss: 0.001092 
Batch[2508] - loss: 0.001295 
Batch[2509] - loss: 0.000747 
Batch[2510] - loss: 0.000960 
Batch[2511] - loss: 0.000803 
Batch[2512] - loss: 0.001263 
Batch[2513] - loss: 0.000527 
Batch[2514] - loss: 0.000745 
Batch[2515] - loss: 0.000889 
Batch[2516] - loss: 0.000590 
Batch[2517] - loss: 0.001621 
Batch[2518] - loss: 0.001424 
Batch[2519] - loss: 0.000826 
Batch[2520] - loss: 0.000801 
Batch[2521] - loss: 0.000855 
Batch[2522] - loss: 0.001067 
Batch[2523] - loss: 0.000570 
Batch[2524] - loss: 0.000685 
Batch[2525] - loss: 0.000628 
Batch[2526] - loss: 0.000445 
Batch[2527] - loss: 0.000668 
Batch[2528] - loss: 0.000601 
Batch[2529] - loss: 0.000753 
Batch[2530] - loss: 0.001302 
Batch[2531] - loss: 0.000955 
Batch[2532] - loss: 0.000548 
Batch[2533] - loss: 0.000607 
Batch[2534] - loss: 0.001366 
Batch[2535] - loss: 0.001849 
Batch[2536] - loss: 0.001068 
Batch[2537] - loss: 0.000782 
Batch[2538] - loss: 0.000868 
Batch[2539] - loss: 0.001589 
Batch[2540] - loss: 0.001340 
Batch[2541] - loss: 0.001486 
Batch[2542] - loss: 0.000662 
Batch[2543] - loss: 0.000750 
Batch[2544] - loss: 0.001143 
Batch[2545] - loss: 0.000684 
Batch[2546] - loss: 0.001590 
Batch[2547] - loss: 0.001432 
Batch[2548] - loss: 0.000774 
Batch[2549] - loss: 0.000816 
Batch[2550] - loss: 0.000741 
Batch[2551] - loss: 0.001213 
Batch[2552] - loss: 0.001204 
Batch[2553] - loss: 0.001071 
Batch[2554] - loss: 0.000702 
Batch[2555] - loss: 0.001073 
Batch[2556] - loss: 0.001917 
Batch[2557] - loss: 0.001025 
Batch[2558] - loss: 0.001020 
Batch[2559] - loss: 0.001111 
Batch[2560] - loss: 0.001534 
Batch[2561] - loss: 0.001019 
Batch[2562] - loss: 0.000804 
Batch[2563] - loss: 0.001778 
Batch[2564] - loss: 0.000854 
Batch[2565] - loss: 0.000718 
Batch[2566] - loss: 0.000685 
Batch[2567] - loss: 0.000805 
Batch[2568] - loss: 0.001176 
Batch[2569] - loss: 0.000912 
Batch[2570] - loss: 0.000913 
Batch[2571] - loss: 0.001080 
Batch[2572] - loss: 0.001117 
Batch[2573] - loss: 0.001516 
Batch[2574] - loss: 0.000779 
Batch[2575] - loss: 0.001160 
Batch[2576] - loss: 0.000824 
Batch[2577] - loss: 0.001575 
Batch[2578] - loss: 0.000734 
Batch[2579] - loss: 0.000646 
Batch[2580] - loss: 0.000715 
Batch[2581] - loss: 0.001210 
Batch[2582] - loss: 0.000577 
Batch[2583] - loss: 0.000812 
Batch[2584] - loss: 0.001359 
Batch[2585] - loss: 0.001014 
Batch[2586] - loss: 0.001091 
Batch[2587] - loss: 0.001209 
Batch[2588] - loss: 0.000719 
Batch[2589] - loss: 0.001341 
Batch[2590] - loss: 0.000587 
Batch[2591] - loss: 0.001525 
Batch[2592] - loss: 0.001628 
Batch[2593] - loss: 0.000961 
Batch[2594] - loss: 0.001057 
Batch[2595] - loss: 0.000441 
Batch[2596] - loss: 0.001905 
Batch[2597] - loss: 0.001088 
Batch[2598] - loss: 0.001182 
Batch[2599] - loss: 0.000942 
Batch[2600] - loss: 0.000582 

Evaluation - loss: 0.000067 pearson: 0.5544 

Batch[2601] - loss: 0.001025 
Batch[2602] - loss: 0.001518 
Batch[2603] - loss: 0.001147 
Batch[2604] - loss: 0.000805 
Batch[2605] - loss: 0.001094 
Batch[2606] - loss: 0.001046 
Batch[2607] - loss: 0.000627 
Batch[2608] - loss: 0.000846 
Batch[2609] - loss: 0.000707 
Batch[2610] - loss: 0.001049 
Batch[2611] - loss: 0.000483 
Batch[2612] - loss: 0.001465 
Batch[2613] - loss: 0.000685 
Batch[2614] - loss: 0.000813 
Batch[2615] - loss: 0.002168 
Batch[2616] - loss: 0.001625 
Batch[2617] - loss: 0.001385 
Batch[2618] - loss: 0.000449 
Batch[2619] - loss: 0.001052 
Batch[2620] - loss: 0.001654 
Batch[2621] - loss: 0.000946 
Batch[2622] - loss: 0.001505 
Batch[2623] - loss: 0.000819 
Batch[2624] - loss: 0.000845 
Batch[2625] - loss: 0.000666 
Batch[2626] - loss: 0.000737 
Batch[2627] - loss: 0.000632 
Batch[2628] - loss: 0.000573 
Batch[2629] - loss: 0.000762 
Batch[2630] - loss: 0.000883 
Batch[2631] - loss: 0.000610 
Batch[2632] - loss: 0.001022 
Batch[2633] - loss: 0.000637 
Batch[2634] - loss: 0.001224 
Batch[2635] - loss: 0.001835 
Batch[2636] - loss: 0.000869 
Batch[2637] - loss: 0.001508 
Batch[2638] - loss: 0.000671 
Batch[2639] - loss: 0.001578 
Batch[2640] - loss: 0.001011 
Batch[2641] - loss: 0.001647 
Batch[2642] - loss: 0.000921 
Batch[2643] - loss: 0.001019 
Batch[2644] - loss: 0.001174 
Batch[2645] - loss: 0.001725 
Batch[2646] - loss: 0.000662 
Batch[2647] - loss: 0.001633 
Batch[2648] - loss: 0.000937 
Batch[2649] - loss: 0.001418 
Batch[2650] - loss: 0.000785 
Batch[2651] - loss: 0.001450 
Batch[2652] - loss: 0.000808 
Batch[2653] - loss: 0.001545 
Batch[2654] - loss: 0.002140 
Batch[2655] - loss: 0.000931 
Batch[2656] - loss: 0.000778 
Batch[2657] - loss: 0.001677 
Batch[2658] - loss: 0.001362 
Batch[2659] - loss: 0.001448 
Batch[2660] - loss: 0.001013 
Batch[2661] - loss: 0.001032 
Batch[2662] - loss: 0.000755 
Batch[2663] - loss: 0.000905 
Batch[2664] - loss: 0.000946 
Batch[2665] - loss: 0.001118 
Batch[2666] - loss: 0.000690 
Batch[2667] - loss: 0.000685 
Batch[2668] - loss: 0.001834 
Batch[2669] - loss: 0.001332 
Batch[2670] - loss: 0.001064 
Batch[2671] - loss: 0.000573 
Batch[2672] - loss: 0.000862 
Batch[2673] - loss: 0.000649 
Batch[2674] - loss: 0.001103 
Batch[2675] - loss: 0.001149 
Batch[2676] - loss: 0.001520 
Batch[2677] - loss: 0.000835 
Batch[2678] - loss: 0.000874 
Batch[2679] - loss: 0.000811 
Batch[2680] - loss: 0.001138 
Batch[2681] - loss: 0.001189 
Batch[2682] - loss: 0.001133 
Batch[2683] - loss: 0.001112 
Batch[2684] - loss: 0.001046 
Batch[2685] - loss: 0.001301 
Batch[2686] - loss: 0.000664 
Batch[2687] - loss: 0.001025 
Batch[2688] - loss: 0.001109 
Batch[2689] - loss: 0.000705 
Batch[2690] - loss: 0.001035 
Batch[2691] - loss: 0.001157 
Batch[2692] - loss: 0.000557 
Batch[2693] - loss: 0.001182 
Batch[2694] - loss: 0.000729 
Batch[2695] - loss: 0.000835 
Batch[2696] - loss: 0.001065 
Batch[2697] - loss: 0.000717 
Batch[2698] - loss: 0.000754 
Batch[2699] - loss: 0.000966 
Batch[2700] - loss: 0.000640 

Evaluation - loss: 0.000068 pearson: 0.5516 

Batch[2701] - loss: 0.000795 
Batch[2702] - loss: 0.000828 
Batch[2703] - loss: 0.000823 
Batch[2704] - loss: 0.001206 
Batch[2705] - loss: 0.000484 
Batch[2706] - loss: 0.000631 
Batch[2707] - loss: 0.001353 
Batch[2708] - loss: 0.001783 
Batch[2709] - loss: 0.001283 
Batch[2710] - loss: 0.000556 
Batch[2711] - loss: 0.001429 
Batch[2712] - loss: 0.000620 
Batch[2713] - loss: 0.001421 
Batch[2714] - loss: 0.000893 
Batch[2715] - loss: 0.001181 
Batch[2716] - loss: 0.001837 
Batch[2717] - loss: 0.000664 
Batch[2718] - loss: 0.000860 
Batch[2719] - loss: 0.001931 
Batch[2720] - loss: 0.001064 
Batch[2721] - loss: 0.001621 
Batch[2722] - loss: 0.001037 
Batch[2723] - loss: 0.000822 
Batch[2724] - loss: 0.000565 
Batch[2725] - loss: 0.001093 
Batch[2726] - loss: 0.001538 
Batch[2727] - loss: 0.000694 
Batch[2728] - loss: 0.000998 
Batch[2729] - loss: 0.001103 
Batch[2730] - loss: 0.000552 
Batch[2731] - loss: 0.001240 
Batch[2732] - loss: 0.001549 
Batch[2733] - loss: 0.001023 
Batch[2734] - loss: 0.000881 
Batch[2735] - loss: 0.001206 
Batch[2736] - loss: 0.002828 
Batch[2737] - loss: 0.000576 
Batch[2738] - loss: 0.000750 
Batch[2739] - loss: 0.001047 
Batch[2740] - loss: 0.002051 
Batch[2741] - loss: 0.000928 
Batch[2742] - loss: 0.001150 
Batch[2743] - loss: 0.001160 
Batch[2744] - loss: 0.000937 
Batch[2745] - loss: 0.000484 
Batch[2746] - loss: 0.000755 
Batch[2747] - loss: 0.000955 
Batch[2748] - loss: 0.001229 
Batch[2749] - loss: 0.000449 
Batch[2750] - loss: 0.001099 
Batch[2751] - loss: 0.000721 
Batch[2752] - loss: 0.000831 
Batch[2753] - loss: 0.000594 
Batch[2754] - loss: 0.000967 
Batch[2755] - loss: 0.002418 
Batch[2756] - loss: 0.001356 
Batch[2757] - loss: 0.000879 
Batch[2758] - loss: 0.001482 
Batch[2759] - loss: 0.001117 
Batch[2760] - loss: 0.000882 
Batch[2761] - loss: 0.001064 
Batch[2762] - loss: 0.001027 
Batch[2763] - loss: 0.000640 
Batch[2764] - loss: 0.000758 
Batch[2765] - loss: 0.001704 
Batch[2766] - loss: 0.000800 
Batch[2767] - loss: 0.000911 
Batch[2768] - loss: 0.001571 
Batch[2769] - loss: 0.000637 
Batch[2770] - loss: 0.001044 
Batch[2771] - loss: 0.001072 
Batch[2772] - loss: 0.002686 
Batch[2773] - loss: 0.001061 
Batch[2774] - loss: 0.001144 
Batch[2775] - loss: 0.001349 
Batch[2776] - loss: 0.001519 
Batch[2777] - loss: 0.001625 
Batch[2778] - loss: 0.001939 
Batch[2779] - loss: 0.000826 
Batch[2780] - loss: 0.000687 
Batch[2781] - loss: 0.001881 
Batch[2782] - loss: 0.000507 
Batch[2783] - loss: 0.000953 
Batch[2784] - loss: 0.001304 
Batch[2785] - loss: 0.001904 
Batch[2786] - loss: 0.001001 
Batch[2787] - loss: 0.000947 
Batch[2788] - loss: 0.001359 
Batch[2789] - loss: 0.001525 
Batch[2790] - loss: 0.001077 
Batch[2791] - loss: 0.000620 
Batch[2792] - loss: 0.001062 
Batch[2793] - loss: 0.001343 
Batch[2794] - loss: 0.001235 
Batch[2795] - loss: 0.001230 
Batch[2796] - loss: 0.001007 
Batch[2797] - loss: 0.001195 
Batch[2798] - loss: 0.000901 
Batch[2799] - loss: 0.001356 
Batch[2800] - loss: 0.001247 

Evaluation - loss: 0.000067 pearson: 0.5538 

Batch[2801] - loss: 0.001753 
Batch[2802] - loss: 0.000846 
Batch[2803] - loss: 0.000910 
Batch[2804] - loss: 0.000896 
Batch[2805] - loss: 0.000821 
Batch[2806] - loss: 0.000716 
Batch[2807] - loss: 0.000755 
Batch[2808] - loss: 0.000796 
Batch[2809] - loss: 0.001529 
Batch[2810] - loss: 0.001569 
Batch[2811] - loss: 0.001891 
Batch[2812] - loss: 0.000867 
Batch[2813] - loss: 0.001658 
Batch[2814] - loss: 0.000763 
Batch[2815] - loss: 0.000696 
Batch[2816] - loss: 0.001024 
Batch[2817] - loss: 0.000832 
Batch[2818] - loss: 0.000743 
Batch[2819] - loss: 0.000829 
Batch[2820] - loss: 0.001369 
Batch[2821] - loss: 0.000748 
Batch[2822] - loss: 0.001305 
Batch[2823] - loss: 0.000902 
Batch[2824] - loss: 0.000577 
Batch[2825] - loss: 0.000923 
Batch[2826] - loss: 0.001023 
Batch[2827] - loss: 0.000638 
Batch[2828] - loss: 0.001122 
Batch[2829] - loss: 0.000972 
Batch[2830] - loss: 0.000917 
Batch[2831] - loss: 0.000772 
Batch[2832] - loss: 0.000949 
Batch[2833] - loss: 0.001154 
Batch[2834] - loss: 0.000751 
Batch[2835] - loss: 0.000807 
Batch[2836] - loss: 0.001331 
Batch[2837] - loss: 0.001287 
Batch[2838] - loss: 0.001230 
Batch[2839] - loss: 0.000870 
Batch[2840] - loss: 0.001992 
Batch[2841] - loss: 0.001220 
Batch[2842] - loss: 0.001249 
Batch[2843] - loss: 0.001476 
Batch[2844] - loss: 0.001136 
Batch[2845] - loss: 0.000688 
Batch[2846] - loss: 0.001669 
Batch[2847] - loss: 0.003043 
Batch[2848] - loss: 0.001077 
Batch[2849] - loss: 0.001648 
Batch[2850] - loss: 0.000625 
Batch[2851] - loss: 0.002107 
Batch[2852] - loss: 0.001524 
Batch[2853] - loss: 0.001156 
Batch[2854] - loss: 0.001031 
Batch[2855] - loss: 0.000661 
Batch[2856] - loss: 0.000868 
Batch[2857] - loss: 0.000885 
Batch[2858] - loss: 0.001338 
Batch[2859] - loss: 0.000831 
Batch[2860] - loss: 0.002342 
Batch[2861] - loss: 0.000504 
Batch[2862] - loss: 0.000960 
Batch[2863] - loss: 0.001156 
Batch[2864] - loss: 0.001169 
Batch[2865] - loss: 0.000791 
Batch[2866] - loss: 0.001331 
Batch[2867] - loss: 0.001936 
Batch[2868] - loss: 0.001506 
Batch[2869] - loss: 0.001073 
Batch[2870] - loss: 0.001362 
Batch[2871] - loss: 0.001291 
Batch[2872] - loss: 0.001403 
Batch[2873] - loss: 0.001192 
Batch[2874] - loss: 0.000913 
Batch[2875] - loss: 0.001177 
Batch[2876] - loss: 0.000716 
Batch[2877] - loss: 0.000907 
Batch[2878] - loss: 0.001097 
Batch[2879] - loss: 0.001245 
Batch[2880] - loss: 0.000745 
Batch[2881] - loss: 0.002255 
Batch[2882] - loss: 0.001049 
Batch[2883] - loss: 0.001063 
Batch[2884] - loss: 0.001100 
Batch[2885] - loss: 0.001223 
Batch[2886] - loss: 0.001704 
Batch[2887] - loss: 0.001732 
Batch[2888] - loss: 0.001314 
Batch[2889] - loss: 0.000761 
Batch[2890] - loss: 0.000995 
Batch[2891] - loss: 0.001009 
Batch[2892] - loss: 0.000765 
Batch[2893] - loss: 0.001242 
Batch[2894] - loss: 0.000953 
Batch[2895] - loss: 0.000914 
Batch[2896] - loss: 0.000852 
Batch[2897] - loss: 0.001651 
Batch[2898] - loss: 0.000862 
Batch[2899] - loss: 0.000624 
Batch[2900] - loss: 0.000882 

Evaluation - loss: 0.000068 pearson: 0.5517 

Batch[2901] - loss: 0.000558 
Batch[2902] - loss: 0.000786 
Batch[2903] - loss: 0.001514 
Batch[2904] - loss: 0.001515 
Batch[2905] - loss: 0.001802 
Batch[2906] - loss: 0.001321 
Batch[2907] - loss: 0.000767 
Batch[2908] - loss: 0.001905 
Batch[2909] - loss: 0.000541 
Batch[2910] - loss: 0.001069 
Batch[2911] - loss: 0.001244 
Batch[2912] - loss: 0.000722 
Batch[2913] - loss: 0.000914 
Batch[2914] - loss: 0.000715 
Batch[2915] - loss: 0.001489 
Batch[2916] - loss: 0.001293 
Batch[2917] - loss: 0.001139 
Batch[2918] - loss: 0.000676 
Batch[2919] - loss: 0.001998 
Batch[2920] - loss: 0.001185 
Batch[2921] - loss: 0.000807 
Batch[2922] - loss: 0.001191 
Batch[2923] - loss: 0.000653 
Batch[2924] - loss: 0.001083 
Batch[2925] - loss: 0.001475 
Batch[2926] - loss: 0.000927 
Batch[2927] - loss: 0.001276 
Batch[2928] - loss: 0.001271 
Batch[2929] - loss: 0.002591 
Batch[2930] - loss: 0.001959 
Batch[2931] - loss: 0.000729 
Batch[2932] - loss: 0.000755 
Batch[2933] - loss: 0.001602 
Batch[2934] - loss: 0.001129 
Batch[2935] - loss: 0.000783 
Batch[2936] - loss: 0.001177 
Batch[2937] - loss: 0.001587 
Batch[2938] - loss: 0.001348 
Batch[2939] - loss: 0.000938 
Batch[2940] - loss: 0.000615 
Batch[2941] - loss: 0.001166 
Batch[2942] - loss: 0.000939 
Batch[2943] - loss: 0.001115 
Batch[2944] - loss: 0.001397 
Batch[2945] - loss: 0.001120 
Batch[2946] - loss: 0.001675 
Batch[2947] - loss: 0.000831 
Batch[2948] - loss: 0.001387 
Batch[2949] - loss: 0.001936 
Batch[2950] - loss: 0.000595 
Batch[2951] - loss: 0.001026 
Batch[2952] - loss: 0.001915 
Batch[2953] - loss: 0.001967 
Batch[2954] - loss: 0.000918 
Batch[2955] - loss: 0.000958 
Batch[2956] - loss: 0.000477 
Batch[2957] - loss: 0.001402 
Batch[2958] - loss: 0.001284 
Batch[2959] - loss: 0.001231 
Batch[2960] - loss: 0.001067 
Batch[2961] - loss: 0.001234 
Batch[2962] - loss: 0.001372 
Batch[2963] - loss: 0.000798 
Batch[2964] - loss: 0.001348 
Batch[2965] - loss: 0.000861 
Batch[2966] - loss: 0.001344 
Batch[2967] - loss: 0.001157 
Batch[2968] - loss: 0.000685 
Batch[2969] - loss: 0.000979 
Batch[2970] - loss: 0.001154 
Batch[2971] - loss: 0.000784 
Batch[2972] - loss: 0.001116 
Batch[2973] - loss: 0.000909 
Batch[2974] - loss: 0.001795 
Batch[2975] - loss: 0.000616 
Batch[2976] - loss: 0.001299 
Batch[2977] - loss: 0.000746 
Batch[2978] - loss: 0.000945 
Batch[2979] - loss: 0.001289 
Batch[2980] - loss: 0.001724 
Batch[2981] - loss: 0.000952 
Batch[2982] - loss: 0.001467 
Batch[2983] - loss: 0.001230 
Batch[2984] - loss: 0.001494 
Batch[2985] - loss: 0.001059 
Batch[2986] - loss: 0.001689 
Batch[2987] - loss: 0.000719 
Batch[2988] - loss: 0.002704 
Batch[2989] - loss: 0.001095 
Batch[2990] - loss: 0.000973 
Batch[2991] - loss: 0.001216 
Batch[2992] - loss: 0.000782 
Batch[2993] - loss: 0.000471 
Batch[2994] - loss: 0.001037 
Batch[2995] - loss: 0.000864 
Batch[2996] - loss: 0.001189 
Batch[2997] - loss: 0.000916 
Batch[2998] - loss: 0.001039 
Batch[2999] - loss: 0.002114 
Batch[3000] - loss: 0.001048 

Evaluation - loss: 0.000067 pearson: 0.5548 

Batch[3001] - loss: 0.001373 
Batch[3002] - loss: 0.000912 
Batch[3003] - loss: 0.001174 
Batch[3004] - loss: 0.000809 
Batch[3005] - loss: 0.001672 
Batch[3006] - loss: 0.001001 
Batch[3007] - loss: 0.001006 
Batch[3008] - loss: 0.001536 
Batch[3009] - loss: 0.001845 
Batch[3010] - loss: 0.001343 
Batch[3011] - loss: 0.001653 
Batch[3012] - loss: 0.000949 
Batch[3013] - loss: 0.000869 
Batch[3014] - loss: 0.001034 
Batch[3015] - loss: 0.001039 
Batch[3016] - loss: 0.001492 
Batch[3017] - loss: 0.000865 
Batch[3018] - loss: 0.001368 
Batch[3019] - loss: 0.001604 
Batch[3020] - loss: 0.000720 
Batch[3021] - loss: 0.000871 
Batch[3022] - loss: 0.000777 
Batch[3023] - loss: 0.001058 
Batch[3024] - loss: 0.001819 
Batch[3025] - loss: 0.001249 
Batch[3026] - loss: 0.000792 
Batch[3027] - loss: 0.001332 
Batch[3028] - loss: 0.000721 
Batch[3029] - loss: 0.001682 
Batch[3030] - loss: 0.001468 
Batch[3031] - loss: 0.000965 
Batch[3032] - loss: 0.000751 
Batch[3033] - loss: 0.001154 
Batch[3034] - loss: 0.000824 
Batch[3035] - loss: 0.000637 
Batch[3036] - loss: 0.000578 
Batch[3037] - loss: 0.001180 
Batch[3038] - loss: 0.000615 
Batch[3039] - loss: 0.000702 
Batch[3040] - loss: 0.001166 
Batch[3041] - loss: 0.000721 
Batch[3042] - loss: 0.000944 
Batch[3043] - loss: 0.001188 
Batch[3044] - loss: 0.001376 
Batch[3045] - loss: 0.000748 
Batch[3046] - loss: 0.001198 
Batch[3047] - loss: 0.000811 
Batch[3048] - loss: 0.000902 
Batch[3049] - loss: 0.000689 
Batch[3050] - loss: 0.002186 
Batch[3051] - loss: 0.001348 
Batch[3052] - loss: 0.001423 
Batch[3053] - loss: 0.000861 
Batch[3054] - loss: 0.001100 
Batch[3055] - loss: 0.001282 
Batch[3056] - loss: 0.001234 
Batch[3057] - loss: 0.000623 
Batch[3058] - loss: 0.002044 
Batch[3059] - loss: 0.000490 
Batch[3060] - loss: 0.001036 
Batch[3061] - loss: 0.000644 
Batch[3062] - loss: 0.000792 
Batch[3063] - loss: 0.000713 
Batch[3064] - loss: 0.001168 
Batch[3065] - loss: 0.000699 
Batch[3066] - loss: 0.001353 
Batch[3067] - loss: 0.000759 
Batch[3068] - loss: 0.001369 
Batch[3069] - loss: 0.001125 
Batch[3070] - loss: 0.000948 
Batch[3071] - loss: 0.001438 
Batch[3072] - loss: 0.000681 
Batch[3073] - loss: 0.001413 
Batch[3074] - loss: 0.000783 
Batch[3075] - loss: 0.002070 
Batch[3076] - loss: 0.000942 
Batch[3077] - loss: 0.000782 
Batch[3078] - loss: 0.000818 
Batch[3079] - loss: 0.001231 
Batch[3080] - loss: 0.001002 
Batch[3081] - loss: 0.000996 
Batch[3082] - loss: 0.000887 
Batch[3083] - loss: 0.001124 
Batch[3084] - loss: 0.000604 
Batch[3085] - loss: 0.000945 
Batch[3086] - loss: 0.000857 
Batch[3087] - loss: 0.001607 
Batch[3088] - loss: 0.000847 
Batch[3089] - loss: 0.001027 
Batch[3090] - loss: 0.001977 
Batch[3091] - loss: 0.001569 
Batch[3092] - loss: 0.001397 
Batch[3093] - loss: 0.000753 
Batch[3094] - loss: 0.000908 
Batch[3095] - loss: 0.001308 
Batch[3096] - loss: 0.001120 
Batch[3097] - loss: 0.000926 
Batch[3098] - loss: 0.000672 
Batch[3099] - loss: 0.000606 
Batch[3100] - loss: 0.001710 

Evaluation - loss: 0.000067 pearson: 0.5555 

Batch[3101] - loss: 0.001246 
Batch[3102] - loss: 0.001149 
Batch[3103] - loss: 0.001623 
Batch[3104] - loss: 0.000757 
Batch[3105] - loss: 0.000803 
Batch[3106] - loss: 0.000946 
Batch[3107] - loss: 0.000535 
Batch[3108] - loss: 0.000802 
Batch[3109] - loss: 0.000563 
Batch[3110] - loss: 0.000613 
Batch[3111] - loss: 0.000483 
Batch[3112] - loss: 0.001086 
Batch[3113] - loss: 0.001384 
Batch[3114] - loss: 0.001113 
Batch[3115] - loss: 0.001064 
Batch[3116] - loss: 0.000981 
Batch[3117] - loss: 0.001548 
Batch[3118] - loss: 0.001030 
Batch[3119] - loss: 0.001332 
Batch[3120] - loss: 0.000442 
Batch[3121] - loss: 0.000917 
Batch[3122] - loss: 0.000735 
Batch[3123] - loss: 0.000918 
Batch[3124] - loss: 0.001089 
Batch[3125] - loss: 0.000998 
Batch[3126] - loss: 0.000909 
Batch[3127] - loss: 0.001976 
Batch[3128] - loss: 0.000722 
Batch[3129] - loss: 0.001539 
Batch[3130] - loss: 0.001057 
Batch[3131] - loss: 0.001179 
Batch[3132] - loss: 0.001432 
Batch[3133] - loss: 0.000914 
Batch[3134] - loss: 0.000543 
Batch[3135] - loss: 0.000958 
Batch[3136] - loss: 0.001129 
Batch[3137] - loss: 0.000974 
Batch[3138] - loss: 0.001031 
Batch[3139] - loss: 0.000867 
Batch[3140] - loss: 0.000866 
Batch[3141] - loss: 0.001168 
Batch[3142] - loss: 0.000842 
Batch[3143] - loss: 0.001122 
Batch[3144] - loss: 0.001548 
Batch[3145] - loss: 0.000631 
Batch[3146] - loss: 0.000582 
Batch[3147] - loss: 0.001170 
Batch[3148] - loss: 0.001140 
Batch[3149] - loss: 0.000637 
Batch[3150] - loss: 0.001408 
Batch[3151] - loss: 0.000669 
Batch[3152] - loss: 0.001005 
Batch[3153] - loss: 0.001142 
Batch[3154] - loss: 0.000615 
Batch[3155] - loss: 0.001576 
Batch[3156] - loss: 0.001172 
Batch[3157] - loss: 0.000846 
Batch[3158] - loss: 0.000809 
Batch[3159] - loss: 0.001032 
Batch[3160] - loss: 0.001150 
Batch[3161] - loss: 0.000714 
Batch[3162] - loss: 0.000805 
Batch[3163] - loss: 0.000688 
Batch[3164] - loss: 0.001175 
Batch[3165] - loss: 0.001002 
Batch[3166] - loss: 0.000687 
Batch[3167] - loss: 0.001336 
Batch[3168] - loss: 0.001411 
Batch[3169] - loss: 0.000627 
Batch[3170] - loss: 0.000602 
Batch[3171] - loss: 0.001137 
Batch[3172] - loss: 0.001206 
Batch[3173] - loss: 0.000856 
Batch[3174] - loss: 0.002290 
Batch[3175] - loss: 0.000849 
Batch[3176] - loss: 0.000871 
Batch[3177] - loss: 0.001092 
Batch[3178] - loss: 0.000467 
Batch[3179] - loss: 0.000508 
Batch[3180] - loss: 0.001049 
Batch[3181] - loss: 0.000671 
Batch[3182] - loss: 0.001303 
Batch[3183] - loss: 0.000768 
Batch[3184] - loss: 0.001481 
Batch[3185] - loss: 0.001147 
Batch[3186] - loss: 0.000742 
Batch[3187] - loss: 0.000895 
Batch[3188] - loss: 0.001350 
Batch[3189] - loss: 0.000949 
Batch[3190] - loss: 0.000729 
Batch[3191] - loss: 0.000901 
Batch[3192] - loss: 0.001074 
Batch[3193] - loss: 0.000719 
Batch[3194] - loss: 0.001176 
Batch[3195] - loss: 0.000614 
Batch[3196] - loss: 0.000627 
Batch[3197] - loss: 0.000963 
Batch[3198] - loss: 0.001030 
Batch[3199] - loss: 0.001034 
Batch[3200] - loss: 0.001235 

Evaluation - loss: 0.000067 pearson: 0.5554 

Batch[3201] - loss: 0.001481 
Batch[3202] - loss: 0.001101 
Batch[3203] - loss: 0.001276 
Batch[3204] - loss: 0.000776 
Batch[3205] - loss: 0.000429 
Batch[3206] - loss: 0.001025 
Batch[3207] - loss: 0.001065 
Batch[3208] - loss: 0.000888 
Batch[3209] - loss: 0.001441 
Batch[3210] - loss: 0.000870 
Batch[3211] - loss: 0.000888 
Batch[3212] - loss: 0.000821 
Batch[3213] - loss: 0.000495 
Batch[3214] - loss: 0.000732 
Batch[3215] - loss: 0.000900 
Batch[3216] - loss: 0.001170 
Batch[3217] - loss: 0.002027 
Batch[3218] - loss: 0.000623 
Batch[3219] - loss: 0.001711 
Batch[3220] - loss: 0.000728 
Batch[3221] - loss: 0.000986 
Batch[3222] - loss: 0.000825 
Batch[3223] - loss: 0.001195 
Batch[3224] - loss: 0.001048 
Batch[3225] - loss: 0.000989 
Batch[3226] - loss: 0.000935 
Batch[3227] - loss: 0.000688 
Batch[3228] - loss: 0.000832 
Batch[3229] - loss: 0.001003 
Batch[3230] - loss: 0.000802 
Batch[3231] - loss: 0.000750 
Batch[3232] - loss: 0.000997 
Batch[3233] - loss: 0.000442 
Batch[3234] - loss: 0.001126 
Batch[3235] - loss: 0.001028 
Batch[3236] - loss: 0.001211 
Batch[3237] - loss: 0.002207 
Batch[3238] - loss: 0.001032 
Batch[3239] - loss: 0.000509 
Batch[3240] - loss: 0.001031 
Batch[3241] - loss: 0.001186 
Batch[3242] - loss: 0.000660 
Batch[3243] - loss: 0.000618 
Batch[3244] - loss: 0.000792 
Batch[3245] - loss: 0.001077 
Batch[3246] - loss: 0.000632 
Batch[3247] - loss: 0.000669 
Batch[3248] - loss: 0.001070 
Batch[3249] - loss: 0.001091 
Batch[3250] - loss: 0.000947 
Batch[3251] - loss: 0.000697 
Batch[3252] - loss: 0.000556 
Batch[3253] - loss: 0.000897 
Batch[3254] - loss: 0.000907 
Batch[3255] - loss: 0.001376 
Batch[3256] - loss: 0.001039 
Batch[3257] - loss: 0.000432 
Batch[3258] - loss: 0.001624 
Batch[3259] - loss: 0.000778 
Batch[3260] - loss: 0.000603 
Batch[3261] - loss: 0.001241 
Batch[3262] - loss: 0.001175 
Batch[3263] - loss: 0.000381 
Batch[3264] - loss: 0.001100 
Batch[3265] - loss: 0.001005 
Batch[3266] - loss: 0.000939 
Batch[3267] - loss: 0.001337 
Batch[3268] - loss: 0.000524 
Batch[3269] - loss: 0.001548 
Batch[3270] - loss: 0.001073 
Batch[3271] - loss: 0.000654 
Batch[3272] - loss: 0.000881 
Batch[3273] - loss: 0.000672 
Batch[3274] - loss: 0.000683 
Batch[3275] - loss: 0.001018 
Batch[3276] - loss: 0.000931 
Batch[3277] - loss: 0.000799 
Batch[3278] - loss: 0.000725 
Batch[3279] - loss: 0.001142 
Batch[3280] - loss: 0.000929 
Batch[3281] - loss: 0.001137 
Batch[3282] - loss: 0.000889 
Batch[3283] - loss: 0.000985 
Batch[3284] - loss: 0.000452 
Batch[3285] - loss: 0.000885 
Batch[3286] - loss: 0.001255 
Batch[3287] - loss: 0.000794 
Batch[3288] - loss: 0.001544 
Batch[3289] - loss: 0.001621 
Batch[3290] - loss: 0.000774 
Batch[3291] - loss: 0.000609 
Batch[3292] - loss: 0.001293 
Batch[3293] - loss: 0.000621 
Batch[3294] - loss: 0.000969 
Batch[3295] - loss: 0.000788 
Batch[3296] - loss: 0.001002 
Batch[3297] - loss: 0.001126 
Batch[3298] - loss: 0.000973 
Batch[3299] - loss: 0.000660 
Batch[3300] - loss: 0.000802 

Evaluation - loss: 0.000067 pearson: 0.5600 

Batch[3301] - loss: 0.000572 
Batch[3302] - loss: 0.001177 
Batch[3303] - loss: 0.001376 
Batch[3304] - loss: 0.001001 
Batch[3305] - loss: 0.000928 
Batch[3306] - loss: 0.001184 
Batch[3307] - loss: 0.000787 
Batch[3308] - loss: 0.000811 
Batch[3309] - loss: 0.001183 
Batch[3310] - loss: 0.001057 
Batch[3311] - loss: 0.000721 
Batch[3312] - loss: 0.000523 
Batch[3313] - loss: 0.000731 
Batch[3314] - loss: 0.000860 
Batch[3315] - loss: 0.000783 
Batch[3316] - loss: 0.001298 
Batch[3317] - loss: 0.000894 
Batch[3318] - loss: 0.001525 
Batch[3319] - loss: 0.001056 
Batch[3320] - loss: 0.000691 
Batch[3321] - loss: 0.001104 
Batch[3322] - loss: 0.001086 
Batch[3323] - loss: 0.001255 
Batch[3324] - loss: 0.000807 
Batch[3325] - loss: 0.001075 
Batch[3326] - loss: 0.000951 
Batch[3327] - loss: 0.000717 
Batch[3328] - loss: 0.001791 
Batch[3329] - loss: 0.000649 
Batch[3330] - loss: 0.000957 
Batch[3331] - loss: 0.001287 
Batch[3332] - loss: 0.001377 
Batch[3333] - loss: 0.001023 
Batch[3334] - loss: 0.000999 
Batch[3335] - loss: 0.000922 
Batch[3336] - loss: 0.001009 
Batch[3337] - loss: 0.000926 
Batch[3338] - loss: 0.001007 
Batch[3339] - loss: 0.000971 
Batch[3340] - loss: 0.000615 
Batch[3341] - loss: 0.000941 
Batch[3342] - loss: 0.000649 
Batch[3343] - loss: 0.000867 
Batch[3344] - loss: 0.000651 
Batch[3345] - loss: 0.001533 
Batch[3346] - loss: 0.000871 
Batch[3347] - loss: 0.001299 
Batch[3348] - loss: 0.000667 
Batch[3349] - loss: 0.000640 
Batch[3350] - loss: 0.000838 
Batch[3351] - loss: 0.001067 
Batch[3352] - loss: 0.000886 
Batch[3353] - loss: 0.001457 
Batch[3354] - loss: 0.000757 
Batch[3355] - loss: 0.000992 
Batch[3356] - loss: 0.000790 
Batch[3357] - loss: 0.000505 
Batch[3358] - loss: 0.000887 
Batch[3359] - loss: 0.000866 
Batch[3360] - loss: 0.000873 
Batch[3361] - loss: 0.001439 
Batch[3362] - loss: 0.001380 
Batch[3363] - loss: 0.000466 
Batch[3364] - loss: 0.000902 
Batch[3365] - loss: 0.000768 
Batch[3366] - loss: 0.001556 
Batch[3367] - loss: 0.000988 
Batch[3368] - loss: 0.000906 
Batch[3369] - loss: 0.001289 
Batch[3370] - loss: 0.000943 
Batch[3371] - loss: 0.000682 
Batch[3372] - loss: 0.000509 
Batch[3373] - loss: 0.000380 
Batch[3374] - loss: 0.000541 
Batch[3375] - loss: 0.000474 
Batch[3376] - loss: 0.001298 
Batch[3377] - loss: 0.000968 
Batch[3378] - loss: 0.000718 
Batch[3379] - loss: 0.001531 
Batch[3380] - loss: 0.000582 
Batch[3381] - loss: 0.000952 
Batch[3382] - loss: 0.000888 
Batch[3383] - loss: 0.000627 
Batch[3384] - loss: 0.000935 
Batch[3385] - loss: 0.000834 
Batch[3386] - loss: 0.000720 
Batch[3387] - loss: 0.001282 
Batch[3388] - loss: 0.000867 
Batch[3389] - loss: 0.001036 
Batch[3390] - loss: 0.000877 
Batch[3391] - loss: 0.000745 
Batch[3392] - loss: 0.001344 
Batch[3393] - loss: 0.001031 
Batch[3394] - loss: 0.000837 
Batch[3395] - loss: 0.001510 
Batch[3396] - loss: 0.001406 
Batch[3397] - loss: 0.001472 
Batch[3398] - loss: 0.001156 
Batch[3399] - loss: 0.000816 
Batch[3400] - loss: 0.000410 

Evaluation - loss: 0.000068 pearson: 0.5529 

Batch[3401] - loss: 0.000561 
Batch[3402] - loss: 0.000934 
Batch[3403] - loss: 0.001738 
Batch[3404] - loss: 0.000585 
Batch[3405] - loss: 0.001084 
Batch[3406] - loss: 0.001016 
Batch[3407] - loss: 0.001169 
Batch[3408] - loss: 0.000912 
Batch[3409] - loss: 0.000795 
Batch[3410] - loss: 0.001705 
Batch[3411] - loss: 0.001223 
Batch[3412] - loss: 0.001007 
Batch[3413] - loss: 0.001127 
Batch[3414] - loss: 0.001604 
Batch[3415] - loss: 0.000613 
Batch[3416] - loss: 0.000791 
Batch[3417] - loss: 0.001089 
Batch[3418] - loss: 0.000949 
Batch[3419] - loss: 0.000508 
Batch[3420] - loss: 0.001041 
Batch[3421] - loss: 0.000875 
Batch[3422] - loss: 0.001379 
Batch[3423] - loss: 0.000984 
Batch[3424] - loss: 0.000918 
Batch[3425] - loss: 0.000795 
Batch[3426] - loss: 0.000723 
Batch[3427] - loss: 0.000922 
Batch[3428] - loss: 0.000694 
Batch[3429] - loss: 0.000964 
Batch[3430] - loss: 0.000704 
Batch[3431] - loss: 0.001391 
Batch[3432] - loss: 0.001645 
Batch[3433] - loss: 0.000494 
Batch[3434] - loss: 0.000852 
Batch[3435] - loss: 0.001064 
Batch[3436] - loss: 0.000641 
Batch[3437] - loss: 0.000829 
Batch[3438] - loss: 0.000459 
Batch[3439] - loss: 0.000537 
Batch[3440] - loss: 0.000874 
Batch[3441] - loss: 0.000946 
Batch[3442] - loss: 0.000997 
Batch[3443] - loss: 0.000930 
Batch[3444] - loss: 0.000807 
Batch[3445] - loss: 0.000696 
Batch[3446] - loss: 0.001152 
Batch[3447] - loss: 0.001077 
Batch[3448] - loss: 0.001010 
Batch[3449] - loss: 0.001101 
Batch[3450] - loss: 0.000948 
Batch[3451] - loss: 0.000412 
Batch[3452] - loss: 0.001273 
Batch[3453] - loss: 0.000676 
Batch[3454] - loss: 0.000788 
Batch[3455] - loss: 0.000508 
Batch[3456] - loss: 0.000745 
Batch[3457] - loss: 0.000840 
Batch[3458] - loss: 0.001048 
Batch[3459] - loss: 0.000985 
Batch[3460] - loss: 0.000582 
Batch[3461] - loss: 0.001531 
Batch[3462] - loss: 0.000982 
Batch[3463] - loss: 0.001065 
Batch[3464] - loss: 0.001281 
Batch[3465] - loss: 0.001179 
Batch[3466] - loss: 0.001281 
Batch[3467] - loss: 0.000782 
Batch[3468] - loss: 0.001126 
Batch[3469] - loss: 0.000932 
Batch[3470] - loss: 0.000938 
Batch[3471] - loss: 0.000635 
Batch[3472] - loss: 0.001192 
Batch[3473] - loss: 0.000667 
Batch[3474] - loss: 0.000896 
Batch[3475] - loss: 0.000660 
Batch[3476] - loss: 0.001566 
Batch[3477] - loss: 0.000786 
Batch[3478] - loss: 0.000814 
Batch[3479] - loss: 0.000960 
Batch[3480] - loss: 0.000840 
Batch[3481] - loss: 0.000583 
Batch[3482] - loss: 0.000736 
Batch[3483] - loss: 0.000703 
Batch[3484] - loss: 0.001493 
Batch[3485] - loss: 0.001304 
Batch[3486] - loss: 0.000651 
Batch[3487] - loss: 0.001262 
Batch[3488] - loss: 0.001253 
Batch[3489] - loss: 0.001164 
Batch[3490] - loss: 0.000872 
Batch[3491] - loss: 0.001542 
Batch[3492] - loss: 0.001270 
Batch[3493] - loss: 0.000853 
Batch[3494] - loss: 0.001581 
Batch[3495] - loss: 0.000897 
Batch[3496] - loss: 0.000816 
Batch[3497] - loss: 0.001187 
Batch[3498] - loss: 0.000849 
Batch[3499] - loss: 0.000620 
Batch[3500] - loss: 0.001088 

Evaluation - loss: 0.000067 pearson: 0.5540 

Batch[3501] - loss: 0.000875 
Batch[3502] - loss: 0.000936 
Batch[3503] - loss: 0.000951 
Batch[3504] - loss: 0.001097 
Batch[3505] - loss: 0.000619 
Batch[3506] - loss: 0.001200 
Batch[3507] - loss: 0.001270 
Batch[3508] - loss: 0.000824 
Batch[3509] - loss: 0.000640 
Batch[3510] - loss: 0.000911 
Batch[3511] - loss: 0.000505 
Batch[3512] - loss: 0.000823 
Batch[3513] - loss: 0.000549 
Batch[3514] - loss: 0.000886 
Batch[3515] - loss: 0.000543 
Batch[3516] - loss: 0.000812 
Batch[3517] - loss: 0.001622 
Batch[3518] - loss: 0.001172 
Batch[3519] - loss: 0.000887 
Batch[3520] - loss: 0.000797 
Batch[3521] - loss: 0.000336 
Batch[3522] - loss: 0.000451 
Batch[3523] - loss: 0.000636 
Batch[3524] - loss: 0.000509 
Batch[3525] - loss: 0.002083 
Batch[3526] - loss: 0.000670 
Batch[3527] - loss: 0.000623 
Batch[3528] - loss: 0.000763 
Batch[3529] - loss: 0.000766 
Batch[3530] - loss: 0.000444 
Batch[3531] - loss: 0.000627 
Batch[3532] - loss: 0.001218 
Batch[3533] - loss: 0.001348 
Batch[3534] - loss: 0.001074 
Batch[3535] - loss: 0.001392 
Batch[3536] - loss: 0.000871 
Batch[3537] - loss: 0.000859 
Batch[3538] - loss: 0.000959 
Batch[3539] - loss: 0.001253 
Batch[3540] - loss: 0.001505 
Batch[3541] - loss: 0.000644 
Batch[3542] - loss: 0.000882 
Batch[3543] - loss: 0.001629 
Batch[3544] - loss: 0.000894 
Batch[3545] - loss: 0.001060 
Batch[3546] - loss: 0.001149 
Batch[3547] - loss: 0.001239 
Batch[3548] - loss: 0.000694 
Batch[3549] - loss: 0.000945 
Batch[3550] - loss: 0.001169 
Batch[3551] - loss: 0.001123 
Batch[3552] - loss: 0.000854 
Batch[3553] - loss: 0.001684 
Batch[3554] - loss: 0.000704 
Batch[3555] - loss: 0.001166 
Batch[3556] - loss: 0.001153 
Batch[3557] - loss: 0.001170 
Batch[3558] - loss: 0.001116 
Batch[3559] - loss: 0.001462 
Batch[3560] - loss: 0.000821 
Batch[3561] - loss: 0.000973 
Batch[3562] - loss: 0.000721 
Batch[3563] - loss: 0.000849 
Batch[3564] - loss: 0.001040 
Batch[3565] - loss: 0.000693 
Batch[3566] - loss: 0.000960 
Batch[3567] - loss: 0.000584 
Batch[3568] - loss: 0.000738 
Batch[3569] - loss: 0.001063 
Batch[3570] - loss: 0.000670 
Batch[3571] - loss: 0.000797 
Batch[3572] - loss: 0.000617 
Batch[3573] - loss: 0.001119 
Batch[3574] - loss: 0.000763 
Batch[3575] - loss: 0.000623 
Batch[3576] - loss: 0.002362 
Batch[3577] - loss: 0.000583 
Batch[3578] - loss: 0.001160 
Batch[3579] - loss: 0.000884 
Batch[3580] - loss: 0.001370 
Batch[3581] - loss: 0.001254 
Batch[3582] - loss: 0.000893 
Batch[3583] - loss: 0.000787 
Batch[3584] - loss: 0.000511 
Batch[3585] - loss: 0.000560 
Batch[3586] - loss: 0.001048 
Batch[3587] - loss: 0.000700 
Batch[3588] - loss: 0.001948 
Batch[3589] - loss: 0.001327 
Batch[3590] - loss: 0.000850 
Batch[3591] - loss: 0.001157 
Batch[3592] - loss: 0.001557 
Batch[3593] - loss: 0.000970 
Batch[3594] - loss: 0.000991 
Batch[3595] - loss: 0.001054 
Batch[3596] - loss: 0.001270 
Batch[3597] - loss: 0.000837 
Batch[3598] - loss: 0.001662 
Batch[3599] - loss: 0.000906 
Batch[3600] - loss: 0.001238 

Evaluation - loss: 0.000066 pearson: 0.5632 

Batch[3601] - loss: 0.000974 
Batch[3602] - loss: 0.000864 
Batch[3603] - loss: 0.001081 
Batch[3604] - loss: 0.001329 
Batch[3605] - loss: 0.000707 
Batch[3606] - loss: 0.001128 
Batch[3607] - loss: 0.000760 
Batch[3608] - loss: 0.000607 
Batch[3609] - loss: 0.000919 
Batch[3610] - loss: 0.001024 
Batch[3611] - loss: 0.000831 
Batch[3612] - loss: 0.000664 
Batch[3613] - loss: 0.000646 
Batch[3614] - loss: 0.001392 
Batch[3615] - loss: 0.001248 
Batch[3616] - loss: 0.001100 
Batch[3617] - loss: 0.000930 
Batch[3618] - loss: 0.001623 
Batch[3619] - loss: 0.000776 
Batch[3620] - loss: 0.000636 
Batch[3621] - loss: 0.000863 
Batch[3622] - loss: 0.001383 
Batch[3623] - loss: 0.000851 
Batch[3624] - loss: 0.000665 
Batch[3625] - loss: 0.001107 
Batch[3626] - loss: 0.000617 
Batch[3627] - loss: 0.000806 
Batch[3628] - loss: 0.000781 
Batch[3629] - loss: 0.000644 
Batch[3630] - loss: 0.000797 
Batch[3631] - loss: 0.000774 
Batch[3632] - loss: 0.001032 
Batch[3633] - loss: 0.000825 
Batch[3634] - loss: 0.001028 
Batch[3635] - loss: 0.000584 
Batch[3636] - loss: 0.000863 
Batch[3637] - loss: 0.001003 
Batch[3638] - loss: 0.000967 
Batch[3639] - loss: 0.000902 
Batch[3640] - loss: 0.001762 
Batch[3641] - loss: 0.001021 
Batch[3642] - loss: 0.000825 
Batch[3643] - loss: 0.001162 
Batch[3644] - loss: 0.000532 
Batch[3645] - loss: 0.000964 
Batch[3646] - loss: 0.000648 
Batch[3647] - loss: 0.000675 
Batch[3648] - loss: 0.000933 
Batch[3649] - loss: 0.001225 
Batch[3650] - loss: 0.001183 
Batch[3651] - loss: 0.001296 
Batch[3652] - loss: 0.001108 
Batch[3653] - loss: 0.000828 
Batch[3654] - loss: 0.001122 
Batch[3655] - loss: 0.000606 
Batch[3656] - loss: 0.000728 
Batch[3657] - loss: 0.000621 
Batch[3658] - loss: 0.000569 
Batch[3659] - loss: 0.000601 
Batch[3660] - loss: 0.000301 
Batch[3661] - loss: 0.000468 
Batch[3662] - loss: 0.000657 
Batch[3663] - loss: 0.000719 
Batch[3664] - loss: 0.001373 
Batch[3665] - loss: 0.000784 
Batch[3666] - loss: 0.001244 
Batch[3667] - loss: 0.000442 
Batch[3668] - loss: 0.001316 
Batch[3669] - loss: 0.001780 
Batch[3670] - loss: 0.000593 
Batch[3671] - loss: 0.000913 
Batch[3672] - loss: 0.001882 
Batch[3673] - loss: 0.001082 
Batch[3674] - loss: 0.000582 
Batch[3675] - loss: 0.001668 
Batch[3676] - loss: 0.000843 
Batch[3677] - loss: 0.000644 
Batch[3678] - loss: 0.000822 
Batch[3679] - loss: 0.000989 
Batch[3680] - loss: 0.000734 
Batch[3681] - loss: 0.000477 
Batch[3682] - loss: 0.001088 
Batch[3683] - loss: 0.000611 
Batch[3684] - loss: 0.001129 
Batch[3685] - loss: 0.000668 
Batch[3686] - loss: 0.000609 
Batch[3687] - loss: 0.000997 
Batch[3688] - loss: 0.001122 
Batch[3689] - loss: 0.001074 
Batch[3690] - loss: 0.001727 
Batch[3691] - loss: 0.000937 
Batch[3692] - loss: 0.000771 
Batch[3693] - loss: 0.001114 
Batch[3694] - loss: 0.000951 
Batch[3695] - loss: 0.000616 
Batch[3696] - loss: 0.001551 
Batch[3697] - loss: 0.000616 
Batch[3698] - loss: 0.000641 
Batch[3699] - loss: 0.000762 
Batch[3700] - loss: 0.000795 

Evaluation - loss: 0.000067 pearson: 0.5614 

Batch[3701] - loss: 0.001176 
Batch[3702] - loss: 0.000913 
Batch[3703] - loss: 0.000780 
Batch[3704] - loss: 0.000799 
Batch[3705] - loss: 0.001328 
Batch[3706] - loss: 0.002166 
Batch[3707] - loss: 0.000593 
Batch[3708] - loss: 0.001279 
Batch[3709] - loss: 0.000965 
Batch[3710] - loss: 0.000799 
Batch[3711] - loss: 0.000876 
Batch[3712] - loss: 0.000734 
Batch[3713] - loss: 0.000547 
Batch[3714] - loss: 0.002384 
Batch[3715] - loss: 0.000635 
Batch[3716] - loss: 0.000754 
Batch[3717] - loss: 0.000667 
Batch[3718] - loss: 0.000726 
Batch[3719] - loss: 0.001096 
Batch[3720] - loss: 0.000613 
Batch[3721] - loss: 0.001221 
Batch[3722] - loss: 0.000584 
Batch[3723] - loss: 0.000608 
Batch[3724] - loss: 0.001445 
Batch[3725] - loss: 0.000831 
Batch[3726] - loss: 0.000913 
Batch[3727] - loss: 0.001298 
Batch[3728] - loss: 0.000767 
Batch[3729] - loss: 0.001402 
Batch[3730] - loss: 0.000846 
Batch[3731] - loss: 0.001032 
Batch[3732] - loss: 0.000628 
Batch[3733] - loss: 0.001090 
Batch[3734] - loss: 0.000880 
Batch[3735] - loss: 0.000988 
Batch[3736] - loss: 0.000559 
Batch[3737] - loss: 0.000646 
Batch[3738] - loss: 0.000885 
Batch[3739] - loss: 0.001020 
Batch[3740] - loss: 0.000964 
Batch[3741] - loss: 0.000687 
Batch[3742] - loss: 0.000930 
Batch[3743] - loss: 0.000482 
Batch[3744] - loss: 0.000706 
Batch[3745] - loss: 0.000981 
Batch[3746] - loss: 0.001325 
Batch[3747] - loss: 0.000984 
Batch[3748] - loss: 0.000941 
Batch[3749] - loss: 0.000666 
Batch[3750] - loss: 0.000892 
Batch[3751] - loss: 0.000610 
Batch[3752] - loss: 0.001038 
Batch[3753] - loss: 0.000643 
Batch[3754] - loss: 0.001368 
Batch[3755] - loss: 0.001019 
Batch[3756] - loss: 0.000797 
Batch[3757] - loss: 0.000570 
Batch[3758] - loss: 0.000923 
Batch[3759] - loss: 0.000992 
Batch[3760] - loss: 0.000422 
Batch[3761] - loss: 0.000655 
Batch[3762] - loss: 0.001449 
Batch[3763] - loss: 0.000666 
Batch[3764] - loss: 0.001483 
Batch[3765] - loss: 0.000780 
Batch[3766] - loss: 0.000984 
Batch[3767] - loss: 0.001042 
Batch[3768] - loss: 0.000626 
Batch[3769] - loss: 0.001293 
Batch[3770] - loss: 0.000998 
Batch[3771] - loss: 0.000895 
Batch[3772] - loss: 0.000483 
Batch[3773] - loss: 0.000607 
Batch[3774] - loss: 0.000787 
Batch[3775] - loss: 0.000680 
Batch[3776] - loss: 0.000606 
Batch[3777] - loss: 0.001050 
Batch[3778] - loss: 0.001103 
Batch[3779] - loss: 0.000517 
Batch[3780] - loss: 0.000685 
Batch[3781] - loss: 0.000761 
Batch[3782] - loss: 0.001196 
Batch[3783] - loss: 0.000882 
Batch[3784] - loss: 0.000767 
Batch[3785] - loss: 0.001241 
Batch[3786] - loss: 0.000787 
Batch[3787] - loss: 0.000723 
Batch[3788] - loss: 0.000718 
Batch[3789] - loss: 0.001304 
Batch[3790] - loss: 0.000908 
Batch[3791] - loss: 0.001035 
Batch[3792] - loss: 0.000460 
Batch[3793] - loss: 0.001657 
Batch[3794] - loss: 0.000807 
Batch[3795] - loss: 0.000855 
Batch[3796] - loss: 0.000508 
Batch[3797] - loss: 0.000531 
Batch[3798] - loss: 0.001519 
Batch[3799] - loss: 0.000945 
Batch[3800] - loss: 0.000578 

Evaluation - loss: 0.000067 pearson: 0.5560 

Batch[3801] - loss: 0.000804 
Batch[3802] - loss: 0.000931 
Batch[3803] - loss: 0.000707 
Batch[3804] - loss: 0.000704 
Batch[3805] - loss: 0.001061 
Batch[3806] - loss: 0.000712 
Batch[3807] - loss: 0.000839 
Batch[3808] - loss: 0.002535 
Batch[3809] - loss: 0.001581 
Batch[3810] - loss: 0.000677 
Batch[3811] - loss: 0.001266 
Batch[3812] - loss: 0.000636 
Batch[3813] - loss: 0.000840 
Batch[3814] - loss: 0.000846 
Batch[3815] - loss: 0.000878 
Batch[3816] - loss: 0.000821 
Batch[3817] - loss: 0.000928 
Batch[3818] - loss: 0.000998 
Batch[3819] - loss: 0.001645 
Batch[3820] - loss: 0.000577 
Batch[3821] - loss: 0.000786 
Batch[3822] - loss: 0.001158 
Batch[3823] - loss: 0.000697 
Batch[3824] - loss: 0.000953 
Batch[3825] - loss: 0.000945 
Batch[3826] - loss: 0.000618 
Batch[3827] - loss: 0.001129 
Batch[3828] - loss: 0.000371 
Batch[3829] - loss: 0.000685 
Batch[3830] - loss: 0.000497 
Batch[3831] - loss: 0.001631 
Batch[3832] - loss: 0.000932 
Batch[3833] - loss: 0.001168 
Batch[3834] - loss: 0.000722 
Batch[3835] - loss: 0.000830 
Batch[3836] - loss: 0.000964 
Batch[3837] - loss: 0.000814 
Batch[3838] - loss: 0.000966 
Batch[3839] - loss: 0.000670 
Batch[3840] - loss: 0.000854 
Batch[3841] - loss: 0.001082 
Batch[3842] - loss: 0.001259 
Batch[3843] - loss: 0.000818 
Batch[3844] - loss: 0.000630 
Batch[3845] - loss: 0.001158 
Batch[3846] - loss: 0.000804 
Batch[3847] - loss: 0.001042 
Batch[3848] - loss: 0.001042 
Batch[3849] - loss: 0.000767 
Batch[3850] - loss: 0.001258 
Batch[3851] - loss: 0.000538 
Batch[3852] - loss: 0.000696 
Batch[3853] - loss: 0.000757 
Batch[3854] - loss: 0.000841 
Batch[3855] - loss: 0.000593 
Batch[3856] - loss: 0.000477 
Batch[3857] - loss: 0.000483 
Batch[3858] - loss: 0.001744 
Batch[3859] - loss: 0.000746 
Batch[3860] - loss: 0.000942 
Batch[3861] - loss: 0.000617 
Batch[3862] - loss: 0.001494 
Batch[3863] - loss: 0.000672 
Batch[3864] - loss: 0.001051 
Batch[3865] - loss: 0.000558 
Batch[3866] - loss: 0.000690 
Batch[3867] - loss: 0.000586 
Batch[3868] - loss: 0.000815 
Batch[3869] - loss: 0.000897 
Batch[3870] - loss: 0.000423 
Batch[3871] - loss: 0.000372 
Batch[3872] - loss: 0.000683 
Batch[3873] - loss: 0.001703 
Batch[3874] - loss: 0.002207 
Batch[3875] - loss: 0.000610 
Batch[3876] - loss: 0.000703 
Batch[3877] - loss: 0.001302 
Batch[3878] - loss: 0.001091 
Batch[3879] - loss: 0.000981 
Batch[3880] - loss: 0.000844 
Batch[3881] - loss: 0.000780 
Batch[3882] - loss: 0.000745 
Batch[3883] - loss: 0.000601 
Batch[3884] - loss: 0.001325 
Batch[3885] - loss: 0.000698 
Batch[3886] - loss: 0.001581 
Batch[3887] - loss: 0.000907 
Batch[3888] - loss: 0.000612 
Batch[3889] - loss: 0.001330 
Batch[3890] - loss: 0.000755 
Batch[3891] - loss: 0.001181 
Batch[3892] - loss: 0.000725 
Batch[3893] - loss: 0.000411 
Batch[3894] - loss: 0.000872 
Batch[3895] - loss: 0.000621 
Batch[3896] - loss: 0.000687 
Batch[3897] - loss: 0.000660 
Batch[3898] - loss: 0.000712 
Batch[3899] - loss: 0.001364 
Batch[3900] - loss: 0.000596 

Evaluation - loss: 0.000067 pearson: 0.5601 

Batch[3901] - loss: 0.000574 
Batch[3902] - loss: 0.000505 
Batch[3903] - loss: 0.000878 
Batch[3904] - loss: 0.000951 
Batch[3905] - loss: 0.001230 
Batch[3906] - loss: 0.001885 
Batch[3907] - loss: 0.000703 
Batch[3908] - loss: 0.000847 
Batch[3909] - loss: 0.000432 
Batch[3910] - loss: 0.000681 
Batch[3911] - loss: 0.001173 
Batch[3912] - loss: 0.000523 
Batch[3913] - loss: 0.000917 
Batch[3914] - loss: 0.000631 
Batch[3915] - loss: 0.000852 
Batch[3916] - loss: 0.000751 
Batch[3917] - loss: 0.000618 
Batch[3918] - loss: 0.000938 
Batch[3919] - loss: 0.001059 
Batch[3920] - loss: 0.001047 
Batch[3921] - loss: 0.000939 
Batch[3922] - loss: 0.000669 
Batch[3923] - loss: 0.001105 
Batch[3924] - loss: 0.000770 
Batch[3925] - loss: 0.000679 
Batch[3926] - loss: 0.000676 
Batch[3927] - loss: 0.001019 
Batch[3928] - loss: 0.000748 
Batch[3929] - loss: 0.000880 
Batch[3930] - loss: 0.000625 
Batch[3931] - loss: 0.000761 
Batch[3932] - loss: 0.000942 
Batch[3933] - loss: 0.000545 
Batch[3934] - loss: 0.000969 
Batch[3935] - loss: 0.000677 
Batch[3936] - loss: 0.000826 
Batch[3937] - loss: 0.000627 
Batch[3938] - loss: 0.001080 
Batch[3939] - loss: 0.001473 
Batch[3940] - loss: 0.000478 
Batch[3941] - loss: 0.000681 
Batch[3942] - loss: 0.000671 
Batch[3943] - loss: 0.000552 
Batch[3944] - loss: 0.000769 
Batch[3945] - loss: 0.000898 
Batch[3946] - loss: 0.000788 
Batch[3947] - loss: 0.000580 
Batch[3948] - loss: 0.000976 
Batch[3949] - loss: 0.000452 
Batch[3950] - loss: 0.000497 
Batch[3951] - loss: 0.000694 
Batch[3952] - loss: 0.000612 
Batch[3953] - loss: 0.001130 
Batch[3954] - loss: 0.001574 
Batch[3955] - loss: 0.000428 
Batch[3956] - loss: 0.001022 
Batch[3957] - loss: 0.001366 
Batch[3958] - loss: 0.000915 
Batch[3959] - loss: 0.000376 
Batch[3960] - loss: 0.000676 
Batch[3961] - loss: 0.000880 
Batch[3962] - loss: 0.000928 
Batch[3963] - loss: 0.001103 
Batch[3964] - loss: 0.000943 
Batch[3965] - loss: 0.000487 
Batch[3966] - loss: 0.000479 
Batch[3967] - loss: 0.000988 
Batch[3968] - loss: 0.000877 
Batch[3969] - loss: 0.000653 
Batch[3970] - loss: 0.002335 
Batch[3971] - loss: 0.000695 
Batch[3972] - loss: 0.001161 
Batch[3973] - loss: 0.000987 
Batch[3974] - loss: 0.000577 
Batch[3975] - loss: 0.001067 
Batch[3976] - loss: 0.000907 
Batch[3977] - loss: 0.000609 
Batch[3978] - loss: 0.000538 
Batch[3979] - loss: 0.001499 
Batch[3980] - loss: 0.001004 
Batch[3981] - loss: 0.000775 
Batch[3982] - loss: 0.000695 
Batch[3983] - loss: 0.000872 
Batch[3984] - loss: 0.000638 
Batch[3985] - loss: 0.000786 
Batch[3986] - loss: 0.001194 
Batch[3987] - loss: 0.000703 
Batch[3988] - loss: 0.001009 
Batch[3989] - loss: 0.001254 
Batch[3990] - loss: 0.000749 
Batch[3991] - loss: 0.000910 
Batch[3992] - loss: 0.001186 
Batch[3993] - loss: 0.000690 
Batch[3994] - loss: 0.001779 
Batch[3995] - loss: 0.000960 
Batch[3996] - loss: 0.000668 
Batch[3997] - loss: 0.000607 
Batch[3998] - loss: 0.000804 
Batch[3999] - loss: 0.000616 
Batch[4000] - loss: 0.001035 

Evaluation - loss: 0.000067 pearson: 0.5564 

Batch[4001] - loss: 0.000659 
Batch[4002] - loss: 0.001002 
Batch[4003] - loss: 0.000396 
Batch[4004] - loss: 0.000703 
Batch[4005] - loss: 0.001134 
Batch[4006] - loss: 0.000693 
Batch[4007] - loss: 0.000514 
Batch[4008] - loss: 0.000969 
Batch[4009] - loss: 0.001410 
Batch[4010] - loss: 0.000613 
Batch[4011] - loss: 0.001084 
Batch[4012] - loss: 0.000819 
Batch[4013] - loss: 0.001268 
Batch[4014] - loss: 0.001003 
Batch[4015] - loss: 0.001055 
Batch[4016] - loss: 0.000999 
Batch[4017] - loss: 0.001571 
Batch[4018] - loss: 0.000613 
Batch[4019] - loss: 0.000890 
Batch[4020] - loss: 0.001097 
Batch[4021] - loss: 0.001115 
Batch[4022] - loss: 0.000882 
Batch[4023] - loss: 0.000661 
Batch[4024] - loss: 0.000715 
Batch[4025] - loss: 0.001229 
Batch[4026] - loss: 0.001277 
Batch[4027] - loss: 0.000716 
Batch[4028] - loss: 0.000788 
Batch[4029] - loss: 0.000928 
Batch[4030] - loss: 0.001213 
Batch[4031] - loss: 0.000819 
Batch[4032] - loss: 0.000955 
Batch[4033] - loss: 0.000510 
Batch[4034] - loss: 0.001137 
Batch[4035] - loss: 0.000990 
Batch[4036] - loss: 0.000625 
Batch[4037] - loss: 0.000793 
Batch[4038] - loss: 0.000691 
Batch[4039] - loss: 0.000671 
Batch[4040] - loss: 0.000563 
Batch[4041] - loss: 0.000635 
Batch[4042] - loss: 0.000531 
Batch[4043] - loss: 0.000739 
Batch[4044] - loss: 0.002169 
Batch[4045] - loss: 0.000403 
Batch[4046] - loss: 0.000807 
Batch[4047] - loss: 0.001719 
Batch[4048] - loss: 0.001094 
Batch[4049] - loss: 0.000473 
Batch[4050] - loss: 0.001060 
Batch[4051] - loss: 0.000740 
Batch[4052] - loss: 0.000648 
Batch[4053] - loss: 0.000901 
Batch[4054] - loss: 0.001110 
Batch[4055] - loss: 0.000906 
Batch[4056] - loss: 0.000908 
Batch[4057] - loss: 0.000969 
Batch[4058] - loss: 0.000398 
Batch[4059] - loss: 0.000601 
Batch[4060] - loss: 0.001005 
Batch[4061] - loss: 0.001016 
Batch[4062] - loss: 0.000587 
Batch[4063] - loss: 0.000645 
Batch[4064] - loss: 0.000985 
Batch[4065] - loss: 0.000903 
Batch[4066] - loss: 0.001517 
Batch[4067] - loss: 0.001012 
Batch[4068] - loss: 0.000341 
Batch[4069] - loss: 0.000679 
Batch[4070] - loss: 0.000661 
Batch[4071] - loss: 0.000621 
Batch[4072] - loss: 0.001024 
Batch[4073] - loss: 0.001134 
Batch[4074] - loss: 0.000544 
Batch[4075] - loss: 0.000782 
Batch[4076] - loss: 0.000664 
Batch[4077] - loss: 0.000910 
Batch[4078] - loss: 0.000939 
Batch[4079] - loss: 0.000709 
Batch[4080] - loss: 0.001043 
Batch[4081] - loss: 0.001233 
Batch[4082] - loss: 0.000566 
Batch[4083] - loss: 0.000488 
Batch[4084] - loss: 0.000717 
Batch[4085] - loss: 0.000565 
Batch[4086] - loss: 0.001127 
Batch[4087] - loss: 0.001729 
Batch[4088] - loss: 0.000609 
Batch[4089] - loss: 0.001589 
Batch[4090] - loss: 0.000874 
Batch[4091] - loss: 0.000767 
Batch[4092] - loss: 0.000397 
Batch[4093] - loss: 0.001187 
Batch[4094] - loss: 0.000801 
Batch[4095] - loss: 0.000962 
Batch[4096] - loss: 0.000552 
Batch[4097] - loss: 0.000899 
Batch[4098] - loss: 0.000640 
Batch[4099] - loss: 0.000589 
Batch[4100] - loss: 0.001148 

Evaluation - loss: 0.000067 pearson: 0.5581 

Batch[4101] - loss: 0.002134 
Batch[4102] - loss: 0.001211 
Batch[4103] - loss: 0.000402 
Batch[4104] - loss: 0.001006 
Batch[4105] - loss: 0.000497 
Batch[4106] - loss: 0.000661 
Batch[4107] - loss: 0.000897 
Batch[4108] - loss: 0.000993 
Batch[4109] - loss: 0.001190 
Batch[4110] - loss: 0.001429 
Batch[4111] - loss: 0.001172 
Batch[4112] - loss: 0.000749 
Batch[4113] - loss: 0.001591 
Batch[4114] - loss: 0.000585 
Batch[4115] - loss: 0.000816 
Batch[4116] - loss: 0.001695 
Batch[4117] - loss: 0.001053 
Batch[4118] - loss: 0.000673 
Batch[4119] - loss: 0.001190 
Batch[4120] - loss: 0.000809 
Batch[4121] - loss: 0.000997 
Batch[4122] - loss: 0.001168 
Batch[4123] - loss: 0.000800 
Batch[4124] - loss: 0.003222 
Batch[4125] - loss: 0.000899 
Batch[4126] - loss: 0.001094 
Batch[4127] - loss: 0.000762 
Batch[4128] - loss: 0.000824 
Batch[4129] - loss: 0.001640 
Batch[4130] - loss: 0.000823 
Batch[4131] - loss: 0.001032 
Batch[4132] - loss: 0.000618 
Batch[4133] - loss: 0.000508 
Batch[4134] - loss: 0.000986 
Batch[4135] - loss: 0.000932 
Batch[4136] - loss: 0.000494 
Batch[4137] - loss: 0.000867 
Batch[4138] - loss: 0.001693 
Batch[4139] - loss: 0.000996 
Batch[4140] - loss: 0.000887 
Batch[4141] - loss: 0.001014 
Batch[4142] - loss: 0.000570 
Batch[4143] - loss: 0.000902 
Batch[4144] - loss: 0.001125 
Batch[4145] - loss: 0.000687 
Batch[4146] - loss: 0.000500 
Batch[4147] - loss: 0.000969 
Batch[4148] - loss: 0.000845 
Batch[4149] - loss: 0.000868 
Batch[4150] - loss: 0.000517 
Batch[4151] - loss: 0.000648 
Batch[4152] - loss: 0.000810 
Batch[4153] - loss: 0.000768 
Batch[4154] - loss: 0.001003 
Batch[4155] - loss: 0.000821 
Batch[4156] - loss: 0.001078 
Batch[4157] - loss: 0.000735 
Batch[4158] - loss: 0.000629 
Batch[4159] - loss: 0.001274 
Batch[4160] - loss: 0.001174 
Batch[4161] - loss: 0.000654 
Batch[4162] - loss: 0.001051 
Batch[4163] - loss: 0.000828 
Batch[4164] - loss: 0.001317 
Batch[4165] - loss: 0.000561 
Batch[4166] - loss: 0.000859 
Batch[4167] - loss: 0.000569 
Batch[4168] - loss: 0.000479 
Batch[4169] - loss: 0.000850 
Batch[4170] - loss: 0.001589 
Batch[4171] - loss: 0.000431 
Batch[4172] - loss: 0.001228 
Batch[4173] - loss: 0.001513 
Batch[4174] - loss: 0.000776 
Batch[4175] - loss: 0.001004 
Batch[4176] - loss: 0.000855 
Batch[4177] - loss: 0.000781 
Batch[4178] - loss: 0.000670 
Batch[4179] - loss: 0.000915 
Batch[4180] - loss: 0.000925 
Batch[4181] - loss: 0.001354 
Batch[4182] - loss: 0.000844 
Batch[4183] - loss: 0.000600 
Batch[4184] - loss: 0.001067 
Batch[4185] - loss: 0.000609 
Batch[4186] - loss: 0.000900 
Batch[4187] - loss: 0.000813 
Batch[4188] - loss: 0.001346 
Batch[4189] - loss: 0.000559 
Batch[4190] - loss: 0.001013 
Batch[4191] - loss: 0.000859 
Batch[4192] - loss: 0.000728 
Batch[4193] - loss: 0.001569 
Batch[4194] - loss: 0.000964 
Batch[4195] - loss: 0.000598 
Batch[4196] - loss: 0.000962 
Batch[4197] - loss: 0.001459 
Batch[4198] - loss: 0.000839 
Batch[4199] - loss: 0.001348 
Batch[4200] - loss: 0.000723 

Evaluation - loss: 0.000067 pearson: 0.5594 

Batch[4201] - loss: 0.000534 
Batch[4202] - loss: 0.001113 
Batch[4203] - loss: 0.000733 
Batch[4204] - loss: 0.001009 
Batch[4205] - loss: 0.001518 
Batch[4206] - loss: 0.001668 
Batch[4207] - loss: 0.001317 
Batch[4208] - loss: 0.000759 
Batch[4209] - loss: 0.000588 
Batch[4210] - loss: 0.001102 
Batch[4211] - loss: 0.000520 
Batch[4212] - loss: 0.000913 
Batch[4213] - loss: 0.000464 
Batch[4214] - loss: 0.001385 
Batch[4215] - loss: 0.000995 
Batch[4216] - loss: 0.000807 
Batch[4217] - loss: 0.000550 
Batch[4218] - loss: 0.000822 
Batch[4219] - loss: 0.001313 
Batch[4220] - loss: 0.000542 
Batch[4221] - loss: 0.000811 
Batch[4222] - loss: 0.001151 
Batch[4223] - loss: 0.001084 
Batch[4224] - loss: 0.000685 
Batch[4225] - loss: 0.000737 
Batch[4226] - loss: 0.001153 
Batch[4227] - loss: 0.000496 
Batch[4228] - loss: 0.000428 
Batch[4229] - loss: 0.001088 
Batch[4230] - loss: 0.000563 
Batch[4231] - loss: 0.000846 
Batch[4232] - loss: 0.000514 
Batch[4233] - loss: 0.000478 
Batch[4234] - loss: 0.000577 
Batch[4235] - loss: 0.000979 
Batch[4236] - loss: 0.001307 
Batch[4237] - loss: 0.000697 
Batch[4238] - loss: 0.001076 
Batch[4239] - loss: 0.000748 
Batch[4240] - loss: 0.001026 
Batch[4241] - loss: 0.000543 
Batch[4242] - loss: 0.001842 
Batch[4243] - loss: 0.000699 
Batch[4244] - loss: 0.000906 
Batch[4245] - loss: 0.000875 
Batch[4246] - loss: 0.001281 
Batch[4247] - loss: 0.000486 
Batch[4248] - loss: 0.000802 
Batch[4249] - loss: 0.000723 
Batch[4250] - loss: 0.000628 
Batch[4251] - loss: 0.000666 
Batch[4252] - loss: 0.001029 
Batch[4253] - loss: 0.000722 
Batch[4254] - loss: 0.001510 
Batch[4255] - loss: 0.000981 
Batch[4256] - loss: 0.001369 
Batch[4257] - loss: 0.001083 
Batch[4258] - loss: 0.000784 
Batch[4259] - loss: 0.001085 
Batch[4260] - loss: 0.000619 
Batch[4261] - loss: 0.001238 
Batch[4262] - loss: 0.001124 
Batch[4263] - loss: 0.000843 
Batch[4264] - loss: 0.000897 
Batch[4265] - loss: 0.002202 
Batch[4266] - loss: 0.000767 
Batch[4267] - loss: 0.000893 
Batch[4268] - loss: 0.000527 
Batch[4269] - loss: 0.000710 
Batch[4270] - loss: 0.001083 
Batch[4271] - loss: 0.000730 
Batch[4272] - loss: 0.001374 
Batch[4273] - loss: 0.001077 
Batch[4274] - loss: 0.000831 
Batch[4275] - loss: 0.000397 
Batch[4276] - loss: 0.000897 
Batch[4277] - loss: 0.000629 
Batch[4278] - loss: 0.001136 
Batch[4279] - loss: 0.000887 
Batch[4280] - loss: 0.001244 
Batch[4281] - loss: 0.000794 
Batch[4282] - loss: 0.000673 
Batch[4283] - loss: 0.000745 
Batch[4284] - loss: 0.000726 
Batch[4285] - loss: 0.001108 
Batch[4286] - loss: 0.001126 
Batch[4287] - loss: 0.000459 
Batch[4288] - loss: 0.000818 
Batch[4289] - loss: 0.001560 
Batch[4290] - loss: 0.000928 
Batch[4291] - loss: 0.000797 
Batch[4292] - loss: 0.001096 
Batch[4293] - loss: 0.000635 
Batch[4294] - loss: 0.000782 
Batch[4295] - loss: 0.000585 
Batch[4296] - loss: 0.000621 
Batch[4297] - loss: 0.001187 
Batch[4298] - loss: 0.001244 
Batch[4299] - loss: 0.000767 
Batch[4300] - loss: 0.001577 

Evaluation - loss: 0.000067 pearson: 0.5585 

Batch[4301] - loss: 0.000797 
Batch[4302] - loss: 0.000836 
Batch[4303] - loss: 0.001149 
Batch[4304] - loss: 0.000887 
Batch[4305] - loss: 0.001005 
Batch[4306] - loss: 0.000598 
Batch[4307] - loss: 0.000447 
Batch[4308] - loss: 0.000694 
Batch[4309] - loss: 0.001200 
Batch[4310] - loss: 0.001239 
Batch[4311] - loss: 0.000889 
Batch[4312] - loss: 0.000942 
Batch[4313] - loss: 0.000740 
Batch[4314] - loss: 0.000836 
Batch[4315] - loss: 0.001301 
Batch[4316] - loss: 0.000988 
Batch[4317] - loss: 0.001180 
Batch[4318] - loss: 0.001110 
Batch[4319] - loss: 0.000525 
Batch[4320] - loss: 0.000890 
Batch[4321] - loss: 0.001164 
Batch[4322] - loss: 0.000562 
Batch[4323] - loss: 0.000673 
Batch[4324] - loss: 0.001020 
Batch[4325] - loss: 0.000485 
Batch[4326] - loss: 0.000998 
Batch[4327] - loss: 0.000873 
Batch[4328] - loss: 0.000967 
Batch[4329] - loss: 0.000814 
Batch[4330] - loss: 0.000850 
Batch[4331] - loss: 0.000655 
Batch[4332] - loss: 0.001216 
Batch[4333] - loss: 0.001322 
Batch[4334] - loss: 0.000857 
Batch[4335] - loss: 0.000773 
Batch[4336] - loss: 0.000562 
Batch[4337] - loss: 0.001006 
Batch[4338] - loss: 0.001525 
Batch[4339] - loss: 0.000516 
Batch[4340] - loss: 0.000975 
Batch[4341] - loss: 0.000863 
Batch[4342] - loss: 0.000785 
Batch[4343] - loss: 0.000925 
Batch[4344] - loss: 0.001019 
Batch[4345] - loss: 0.000566 
Batch[4346] - loss: 0.002071 
Batch[4347] - loss: 0.000757 
Batch[4348] - loss: 0.000546 
Batch[4349] - loss: 0.000942 
Batch[4350] - loss: 0.000763 
Batch[4351] - loss: 0.000891 
Batch[4352] - loss: 0.000790 
Batch[4353] - loss: 0.001864 
Batch[4354] - loss: 0.001414 
Batch[4355] - loss: 0.000727 
Batch[4356] - loss: 0.000516 
Batch[4357] - loss: 0.001074 
Batch[4358] - loss: 0.000682 
Batch[4359] - loss: 0.001160 
Batch[4360] - loss: 0.000684 
Batch[4361] - loss: 0.000686 
Batch[4362] - loss: 0.001868 
Batch[4363] - loss: 0.001115 
Batch[4364] - loss: 0.001172 
Batch[4365] - loss: 0.001045 
Batch[4366] - loss: 0.000981 
Batch[4367] - loss: 0.001217 
Batch[4368] - loss: 0.000957 
Batch[4369] - loss: 0.000731 
Batch[4370] - loss: 0.000980 
Batch[4371] - loss: 0.001614 
Batch[4372] - loss: 0.000652 
Batch[4373] - loss: 0.001384 
Batch[4374] - loss: 0.001010 
Batch[4375] - loss: 0.000610 
Batch[4376] - loss: 0.000586 
Batch[4377] - loss: 0.000864 
Batch[4378] - loss: 0.000810 
Batch[4379] - loss: 0.001195 
Batch[4380] - loss: 0.000640 
Batch[4381] - loss: 0.001355 
Batch[4382] - loss: 0.001054 
Batch[4383] - loss: 0.001022 
Batch[4384] - loss: 0.000841 
Batch[4385] - loss: 0.000674 
Batch[4386] - loss: 0.001109 
Batch[4387] - loss: 0.001156 
Batch[4388] - loss: 0.000956 
Batch[4389] - loss: 0.000547 
Batch[4390] - loss: 0.000920 
Batch[4391] - loss: 0.001429 
Batch[4392] - loss: 0.000716 
Batch[4393] - loss: 0.001182 
Batch[4394] - loss: 0.000863 
Batch[4395] - loss: 0.000909 
Batch[4396] - loss: 0.000913 
Batch[4397] - loss: 0.000576 
Batch[4398] - loss: 0.000689 
Batch[4399] - loss: 0.001016 
Batch[4400] - loss: 0.000701 

Evaluation - loss: 0.000068 pearson: 0.5525 

Batch[4401] - loss: 0.000969 
Batch[4402] - loss: 0.001026 
Batch[4403] - loss: 0.000466 
Batch[4404] - loss: 0.000731 
Batch[4405] - loss: 0.000761 
Batch[4406] - loss: 0.000972 
Batch[4407] - loss: 0.000675 
Batch[4408] - loss: 0.000756 
Batch[4409] - loss: 0.000542 
Batch[4410] - loss: 0.000974 
Batch[4411] - loss: 0.000564 
Batch[4412] - loss: 0.001059 
Batch[4413] - loss: 0.001229 
Batch[4414] - loss: 0.000495 
Batch[4415] - loss: 0.001275 
Batch[4416] - loss: 0.000834 
Batch[4417] - loss: 0.000508 
Batch[4418] - loss: 0.000497 
Batch[4419] - loss: 0.001007 
Batch[4420] - loss: 0.000943 
Batch[4421] - loss: 0.000880 
Batch[4422] - loss: 0.000347 
Batch[4423] - loss: 0.000902 
Batch[4424] - loss: 0.000771 
Batch[4425] - loss: 0.000608 
Batch[4426] - loss: 0.000720 
Batch[4427] - loss: 0.000822 
Batch[4428] - loss: 0.000473 
Batch[4429] - loss: 0.000951 
Batch[4430] - loss: 0.000453 
Batch[4431] - loss: 0.000576 
Batch[4432] - loss: 0.001038 
Batch[4433] - loss: 0.000865 
Batch[4434] - loss: 0.001214 
Batch[4435] - loss: 0.000518 
Batch[4436] - loss: 0.000853 
Batch[4437] - loss: 0.000911 
Batch[4438] - loss: 0.001245 
Batch[4439] - loss: 0.000905 
Batch[4440] - loss: 0.000516 
Batch[4441] - loss: 0.001335 
Batch[4442] - loss: 0.001256 
Batch[4443] - loss: 0.002494 
Batch[4444] - loss: 0.000664 
Batch[4445] - loss: 0.000826 
Batch[4446] - loss: 0.000961 
Batch[4447] - loss: 0.001256 
Batch[4448] - loss: 0.001100 
Batch[4449] - loss: 0.000728 
Batch[4450] - loss: 0.000549 
Batch[4451] - loss: 0.000829 
Batch[4452] - loss: 0.000775 
Batch[4453] - loss: 0.001290 
Batch[4454] - loss: 0.000607 
Batch[4455] - loss: 0.000549 
Batch[4456] - loss: 0.000789 
Batch[4457] - loss: 0.000619 
Batch[4458] - loss: 0.001029 
Batch[4459] - loss: 0.000956 
Batch[4460] - loss: 0.000800 
Batch[4461] - loss: 0.000544 
Batch[4462] - loss: 0.001293 
Batch[4463] - loss: 0.001734 
Batch[4464] - loss: 0.000513 
Batch[4465] - loss: 0.001105 
Batch[4466] - loss: 0.000975 
Batch[4467] - loss: 0.000752 
Batch[4468] - loss: 0.001166 
Batch[4469] - loss: 0.000738 
Batch[4470] - loss: 0.000888 
Batch[4471] - loss: 0.000657 
Batch[4472] - loss: 0.000859 
Batch[4473] - loss: 0.000624 
Batch[4474] - loss: 0.000482 
Batch[4475] - loss: 0.000621 
Batch[4476] - loss: 0.001016 
Batch[4477] - loss: 0.000462 
Batch[4478] - loss: 0.000611 
Batch[4479] - loss: 0.000556 
Batch[4480] - loss: 0.000571 
Batch[4481] - loss: 0.000823 
Batch[4482] - loss: 0.000916 
Batch[4483] - loss: 0.000554 
Batch[4484] - loss: 0.000918 
Batch[4485] - loss: 0.001292 
Batch[4486] - loss: 0.000867 
Batch[4487] - loss: 0.000404 
Batch[4488] - loss: 0.000525 
Batch[4489] - loss: 0.000406 
Batch[4490] - loss: 0.000665 
Batch[4491] - loss: 0.000725 
Batch[4492] - loss: 0.000822 
Batch[4493] - loss: 0.000646 
Batch[4494] - loss: 0.001045 
Batch[4495] - loss: 0.000836 
Batch[4496] - loss: 0.000763 
Batch[4497] - loss: 0.000775 
Batch[4498] - loss: 0.000919 
Batch[4499] - loss: 0.001416 
Batch[4500] - loss: 0.000962 

Evaluation - loss: 0.000068 pearson: 0.5564 

Batch[4501] - loss: 0.000763 
Batch[4502] - loss: 0.000558 
Batch[4503] - loss: 0.000540 
Batch[4504] - loss: 0.000789 
Batch[4505] - loss: 0.000970 
Batch[4506] - loss: 0.001707 
Batch[4507] - loss: 0.000592 
Batch[4508] - loss: 0.000701 
Batch[4509] - loss: 0.001029 
Batch[4510] - loss: 0.000449 
Batch[4511] - loss: 0.000989 
Batch[4512] - loss: 0.000582 
Batch[4513] - loss: 0.000500 
Batch[4514] - loss: 0.000977 
Batch[4515] - loss: 0.000721 
Batch[4516] - loss: 0.000962 
Batch[4517] - loss: 0.001091 
Batch[4518] - loss: 0.001006 
Batch[4519] - loss: 0.000702 
Batch[4520] - loss: 0.000632 
Batch[4521] - loss: 0.001318 
Batch[4522] - loss: 0.001080 
Batch[4523] - loss: 0.000997 
Batch[4524] - loss: 0.000823 
Batch[4525] - loss: 0.000762 
Batch[4526] - loss: 0.000632 
Batch[4527] - loss: 0.000815 
Batch[4528] - loss: 0.001037 
Batch[4529] - loss: 0.000846 
Batch[4530] - loss: 0.000488 
Batch[4531] - loss: 0.000516 
Batch[4532] - loss: 0.000778 
Batch[4533] - loss: 0.000787 
Batch[4534] - loss: 0.000749 
Batch[4535] - loss: 0.000746 
Batch[4536] - loss: 0.001352 
Batch[4537] - loss: 0.000823 
Batch[4538] - loss: 0.000774 
Batch[4539] - loss: 0.000788 
Batch[4540] - loss: 0.000574 
Batch[4541] - loss: 0.000754 
Batch[4542] - loss: 0.001014 
Batch[4543] - loss: 0.000899 
Batch[4544] - loss: 0.001013 
Batch[4545] - loss: 0.000787 
Batch[4546] - loss: 0.000876 
Batch[4547] - loss: 0.001229 
Batch[4548] - loss: 0.000734 
Batch[4549] - loss: 0.001068 
Batch[4550] - loss: 0.002085 
Batch[4551] - loss: 0.000723 
Batch[4552] - loss: 0.000415 
Batch[4553] - loss: 0.000740 
Batch[4554] - loss: 0.000761 
Batch[4555] - loss: 0.001204 
Batch[4556] - loss: 0.000653 
Batch[4557] - loss: 0.000705 
Batch[4558] - loss: 0.000634 
Batch[4559] - loss: 0.001260 
Batch[4560] - loss: 0.000456 
Batch[4561] - loss: 0.000751 
Batch[4562] - loss: 0.001485 
Batch[4563] - loss: 0.000982 
Batch[4564] - loss: 0.001061 
Batch[4565] - loss: 0.000537 
Batch[4566] - loss: 0.000479 
Batch[4567] - loss: 0.001555 
Batch[4568] - loss: 0.000806 
Batch[4569] - loss: 0.000564 
Batch[4570] - loss: 0.000772 
Batch[4571] - loss: 0.001311 
Batch[4572] - loss: 0.001180 
Batch[4573] - loss: 0.001579 
Batch[4574] - loss: 0.000703 
Batch[4575] - loss: 0.000750 
Batch[4576] - loss: 0.000571 
Batch[4577] - loss: 0.000943 
Batch[4578] - loss: 0.000848 
Batch[4579] - loss: 0.000812 
Batch[4580] - loss: 0.000856 
Batch[4581] - loss: 0.000521 
Batch[4582] - loss: 0.000953 
Batch[4583] - loss: 0.000612 
Batch[4584] - loss: 0.000720 
Batch[4585] - loss: 0.001483 
Batch[4586] - loss: 0.000880 
Batch[4587] - loss: 0.000696 
Batch[4588] - loss: 0.000764 
Batch[4589] - loss: 0.000456 
Batch[4590] - loss: 0.000603 
Batch[4591] - loss: 0.000820 
Batch[4592] - loss: 0.000631 
Batch[4593] - loss: 0.000798 
Batch[4594] - loss: 0.000828 
Batch[4595] - loss: 0.000711 
Batch[4596] - loss: 0.000946 
Batch[4597] - loss: 0.001098 
Batch[4598] - loss: 0.000767 
Batch[4599] - loss: 0.000625 
Batch[4600] - loss: 0.000397 

Evaluation - loss: 0.000067 pearson: 0.5604 

Batch[4601] - loss: 0.000535 
Batch[4602] - loss: 0.000770 
Batch[4603] - loss: 0.000480 
Batch[4604] - loss: 0.000530 
Batch[4605] - loss: 0.000633 
Batch[4606] - loss: 0.001378 
Batch[4607] - loss: 0.000890 
Batch[4608] - loss: 0.000888 
Batch[4609] - loss: 0.000976 
Batch[4610] - loss: 0.000529 
Batch[4611] - loss: 0.000887 
Batch[4612] - loss: 0.000942 
Batch[4613] - loss: 0.000663 
Batch[4614] - loss: 0.000624 
Batch[4615] - loss: 0.000731 
Batch[4616] - loss: 0.000988 
Batch[4617] - loss: 0.001008 
Batch[4618] - loss: 0.000785 
Batch[4619] - loss: 0.000387 
Batch[4620] - loss: 0.000560 
Batch[4621] - loss: 0.000648 
Batch[4622] - loss: 0.001389 
Batch[4623] - loss: 0.000988 
Batch[4624] - loss: 0.000942 
Batch[4625] - loss: 0.000711 
Batch[4626] - loss: 0.000632 
Batch[4627] - loss: 0.000906 
Batch[4628] - loss: 0.001138 
Batch[4629] - loss: 0.000769 
Batch[4630] - loss: 0.000773 
Batch[4631] - loss: 0.001005 
Batch[4632] - loss: 0.000746 
Batch[4633] - loss: 0.000635 
Batch[4634] - loss: 0.000474 
Batch[4635] - loss: 0.000500 
Batch[4636] - loss: 0.000575 
Batch[4637] - loss: 0.000613 
Batch[4638] - loss: 0.000544 
Batch[4639] - loss: 0.000593 
Batch[4640] - loss: 0.000782 
Batch[4641] - loss: 0.000923 
Batch[4642] - loss: 0.000535 
Batch[4643] - loss: 0.000616 
Batch[4644] - loss: 0.000732 
Batch[4645] - loss: 0.001037 
Batch[4646] - loss: 0.001114 
Batch[4647] - loss: 0.000704 
Batch[4648] - loss: 0.001134 
Batch[4649] - loss: 0.001654 
Batch[4650] - loss: 0.000294 
Batch[4651] - loss: 0.000571 
Batch[4652] - loss: 0.000711 
Batch[4653] - loss: 0.001128 
Batch[4654] - loss: 0.001450 
Batch[4655] - loss: 0.001132 
Batch[4656] - loss: 0.000786 
Batch[4657] - loss: 0.000666 
Batch[4658] - loss: 0.000591 
Batch[4659] - loss: 0.000916 
Batch[4660] - loss: 0.000870 
Batch[4661] - loss: 0.001090 
Batch[4662] - loss: 0.001503 
Batch[4663] - loss: 0.000725 
Batch[4664] - loss: 0.000810 
Batch[4665] - loss: 0.000581 
Batch[4666] - loss: 0.000642 
Batch[4667] - loss: 0.000761 
Batch[4668] - loss: 0.000792 
Batch[4669] - loss: 0.000484 
Batch[4670] - loss: 0.000438 
Batch[4671] - loss: 0.000977 
Batch[4672] - loss: 0.000960 
Batch[4673] - loss: 0.000831 
Batch[4674] - loss: 0.001272 
Batch[4675] - loss: 0.000617 
Batch[4676] - loss: 0.001085 
Batch[4677] - loss: 0.000656 
Batch[4678] - loss: 0.001088 
Batch[4679] - loss: 0.000731 
Batch[4680] - loss: 0.000986 
Batch[4681] - loss: 0.000795 
Batch[4682] - loss: 0.000646 
Batch[4683] - loss: 0.001402 
Batch[4684] - loss: 0.000644 
Batch[4685] - loss: 0.001083 
Batch[4686] - loss: 0.001262 
Batch[4687] - loss: 0.000647 
Batch[4688] - loss: 0.000684 
Batch[4689] - loss: 0.000497 
Batch[4690] - loss: 0.000703 
Batch[4691] - loss: 0.001359 
Batch[4692] - loss: 0.000670 
Batch[4693] - loss: 0.000503 
Batch[4694] - loss: 0.000451 
Batch[4695] - loss: 0.000716 
Batch[4696] - loss: 0.000674 
Batch[4697] - loss: 0.000655 
Batch[4698] - loss: 0.000504 
Batch[4699] - loss: 0.000687 
Batch[4700] - loss: 0.000900 

Evaluation - loss: 0.000068 pearson: 0.5554 

Batch[4701] - loss: 0.000630 
Batch[4702] - loss: 0.000784 
Batch[4703] - loss: 0.000706 
Batch[4704] - loss: 0.000908 
Batch[4705] - loss: 0.000694 
Batch[4706] - loss: 0.002145 
Batch[4707] - loss: 0.001132 
Batch[4708] - loss: 0.000688 
Batch[4709] - loss: 0.000974 
Batch[4710] - loss: 0.001022 
Batch[4711] - loss: 0.000390 
Batch[4712] - loss: 0.001236 
Batch[4713] - loss: 0.000454 
Batch[4714] - loss: 0.000655 
Batch[4715] - loss: 0.001029 
Batch[4716] - loss: 0.000661 
Batch[4717] - loss: 0.000938 
Batch[4718] - loss: 0.000897 
Batch[4719] - loss: 0.000624 
Batch[4720] - loss: 0.000872 
Batch[4721] - loss: 0.000500 
Batch[4722] - loss: 0.001236 
Batch[4723] - loss: 0.000542 
Batch[4724] - loss: 0.001266 
Batch[4725] - loss: 0.000830 
Batch[4726] - loss: 0.000909 
Batch[4727] - loss: 0.001485 
Batch[4728] - loss: 0.000649 
Batch[4729] - loss: 0.000655 
Batch[4730] - loss: 0.000874 
Batch[4731] - loss: 0.000865 
Batch[4732] - loss: 0.000749 
Batch[4733] - loss: 0.000959 
Batch[4734] - loss: 0.000921 
Batch[4735] - loss: 0.000710 
Batch[4736] - loss: 0.000709 
Batch[4737] - loss: 0.000798 
Batch[4738] - loss: 0.000721 
Batch[4739] - loss: 0.000853 
Batch[4740] - loss: 0.000548 
Batch[4741] - loss: 0.000928 
Batch[4742] - loss: 0.000555 
Batch[4743] - loss: 0.000855 
Batch[4744] - loss: 0.000834 
Batch[4745] - loss: 0.000447 
Batch[4746] - loss: 0.000486 
Batch[4747] - loss: 0.001066 
Batch[4748] - loss: 0.000717 
Batch[4749] - loss: 0.000662 
Batch[4750] - loss: 0.000666 
Batch[4751] - loss: 0.000491 
Batch[4752] - loss: 0.001259 
Batch[4753] - loss: 0.000581 
Batch[4754] - loss: 0.000735 
Batch[4755] - loss: 0.000566 
Batch[4756] - loss: 0.000639 
Batch[4757] - loss: 0.001369 
Batch[4758] - loss: 0.000467 
Batch[4759] - loss: 0.001294 
Batch[4760] - loss: 0.000671 
Batch[4761] - loss: 0.000501 
Batch[4762] - loss: 0.000447 
Batch[4763] - loss: 0.000402 
Batch[4764] - loss: 0.001053 
Batch[4765] - loss: 0.001278 
Batch[4766] - loss: 0.000802 
Batch[4767] - loss: 0.000684 
Batch[4768] - loss: 0.000333 
Batch[4769] - loss: 0.000537 
Batch[4770] - loss: 0.000577 
Batch[4771] - loss: 0.000912 
Batch[4772] - loss: 0.000400 
Batch[4773] - loss: 0.000797 
Batch[4774] - loss: 0.000487 
Batch[4775] - loss: 0.001176 
Batch[4776] - loss: 0.000777 
Batch[4777] - loss: 0.000857 
Batch[4778] - loss: 0.000653 
Batch[4779] - loss: 0.001042 
Batch[4780] - loss: 0.000526 
Batch[4781] - loss: 0.000659 
Batch[4782] - loss: 0.000526 
Batch[4783] - loss: 0.001151 
Batch[4784] - loss: 0.000690 
Batch[4785] - loss: 0.000831 
Batch[4786] - loss: 0.000578 
Batch[4787] - loss: 0.001030 
Batch[4788] - loss: 0.000585 
Batch[4789] - loss: 0.000647 
Batch[4790] - loss: 0.000786 
Batch[4791] - loss: 0.000858 
Batch[4792] - loss: 0.000671 
Batch[4793] - loss: 0.001079 
Batch[4794] - loss: 0.001446 
Batch[4795] - loss: 0.000977 
Batch[4796] - loss: 0.000987 
Batch[4797] - loss: 0.001245 
Batch[4798] - loss: 0.000840 
Batch[4799] - loss: 0.000528 
Batch[4800] - loss: 0.000904 

Evaluation - loss: 0.000068 pearson: 0.5567 

Batch[4801] - loss: 0.001007 
Batch[4802] - loss: 0.000770 
Batch[4803] - loss: 0.000334 
Batch[4804] - loss: 0.000526 
Batch[4805] - loss: 0.000657 
Batch[4806] - loss: 0.000808 
Batch[4807] - loss: 0.000963 
Batch[4808] - loss: 0.000532 
Batch[4809] - loss: 0.000546 
Batch[4810] - loss: 0.000651 
Batch[4811] - loss: 0.000839 
Batch[4812] - loss: 0.001037 
Batch[4813] - loss: 0.000393 
Batch[4814] - loss: 0.000450 
Batch[4815] - loss: 0.000682 
Batch[4816] - loss: 0.000696 
Batch[4817] - loss: 0.000662 
Batch[4818] - loss: 0.000551 
Batch[4819] - loss: 0.000659 
Batch[4820] - loss: 0.000553 
Batch[4821] - loss: 0.000697 
Batch[4822] - loss: 0.000689 
Batch[4823] - loss: 0.000430 
Batch[4824] - loss: 0.001079 
Batch[4825] - loss: 0.000478 
Batch[4826] - loss: 0.001825 
Batch[4827] - loss: 0.000713 
Batch[4828] - loss: 0.001007 
Batch[4829] - loss: 0.000530 
Batch[4830] - loss: 0.001269 
Batch[4831] - loss: 0.000565 
Batch[4832] - loss: 0.000732 
Batch[4833] - loss: 0.000754 
Batch[4834] - loss: 0.000981 
Batch[4835] - loss: 0.000858 
Batch[4836] - loss: 0.000541 
Batch[4837] - loss: 0.000916 
Batch[4838] - loss: 0.000812 
Batch[4839] - loss: 0.000630 
Batch[4840] - loss: 0.000586 
Batch[4841] - loss: 0.000776 
Batch[4842] - loss: 0.001189 
Batch[4843] - loss: 0.000935 
Batch[4844] - loss: 0.001332 
Batch[4845] - loss: 0.000986 
Batch[4846] - loss: 0.001136 
Batch[4847] - loss: 0.000550 
Batch[4848] - loss: 0.000804 
Batch[4849] - loss: 0.000794 
Batch[4850] - loss: 0.000600 
Batch[4851] - loss: 0.000537 
Batch[4852] - loss: 0.000611 
Batch[4853] - loss: 0.000536 
Batch[4854] - loss: 0.000821 
Batch[4855] - loss: 0.001011 
Batch[4856] - loss: 0.000923 
Batch[4857] - loss: 0.000712 
Batch[4858] - loss: 0.000556 
Batch[4859] - loss: 0.000502 
Batch[4860] - loss: 0.000605 
Batch[4861] - loss: 0.000760 
Batch[4862] - loss: 0.001080 
Batch[4863] - loss: 0.000740 
Batch[4864] - loss: 0.000813 
Batch[4865] - loss: 0.000889 
Batch[4866] - loss: 0.000670 
Batch[4867] - loss: 0.001120 
Batch[4868] - loss: 0.000588 
Batch[4869] - loss: 0.000572 
Batch[4870] - loss: 0.000749 
Batch[4871] - loss: 0.001312 
Batch[4872] - loss: 0.000406 
Batch[4873] - loss: 0.000858 
Batch[4874] - loss: 0.001105 
Batch[4875] - loss: 0.000557 
Batch[4876] - loss: 0.000550 
Batch[4877] - loss: 0.000479 
Batch[4878] - loss: 0.000780 
Batch[4879] - loss: 0.000387 
Batch[4880] - loss: 0.000506 
Batch[4881] - loss: 0.000837 
Batch[4882] - loss: 0.001038 
Batch[4883] - loss: 0.000728 
Batch[4884] - loss: 0.000878 
Batch[4885] - loss: 0.000482 
Batch[4886] - loss: 0.000862 
Batch[4887] - loss: 0.000491 
Batch[4888] - loss: 0.000599 
Batch[4889] - loss: 0.000762 
Batch[4890] - loss: 0.000571 
Batch[4891] - loss: 0.000484 
Batch[4892] - loss: 0.000658 
Batch[4893] - loss: 0.000904 
Batch[4894] - loss: 0.000429 
Batch[4895] - loss: 0.001045 
Batch[4896] - loss: 0.000615 
Batch[4897] - loss: 0.000763 
Batch[4898] - loss: 0.000589 
Batch[4899] - loss: 0.000643 
Batch[4900] - loss: 0.000538 

Evaluation - loss: 0.000068 pearson: 0.5575 

Batch[4901] - loss: 0.001201 
Batch[4902] - loss: 0.000429 
Batch[4903] - loss: 0.000865 
Batch[4904] - loss: 0.001234 
Batch[4905] - loss: 0.000498 
Batch[4906] - loss: 0.001160 
Batch[4907] - loss: 0.001082 
Batch[4908] - loss: 0.001079 
Batch[4909] - loss: 0.000537 
Batch[4910] - loss: 0.000697 
Batch[4911] - loss: 0.000550 
Batch[4912] - loss: 0.000465 
Batch[4913] - loss: 0.001271 
Batch[4914] - loss: 0.000352 
Batch[4915] - loss: 0.000602 
Batch[4916] - loss: 0.000666 
Batch[4917] - loss: 0.000590 
Batch[4918] - loss: 0.001059 
Batch[4919] - loss: 0.000560 
Batch[4920] - loss: 0.000551 
Batch[4921] - loss: 0.000437 
Batch[4922] - loss: 0.000909 
Batch[4923] - loss: 0.000778 
Batch[4924] - loss: 0.001469 
Batch[4925] - loss: 0.000707 
Batch[4926] - loss: 0.000824 
Batch[4927] - loss: 0.000658 
Batch[4928] - loss: 0.001459 
Batch[4929] - loss: 0.000536 
Batch[4930] - loss: 0.001281 
Batch[4931] - loss: 0.000560 
Batch[4932] - loss: 0.000906 
Batch[4933] - loss: 0.000816 
Batch[4934] - loss: 0.000743 
Batch[4935] - loss: 0.000560 
Batch[4936] - loss: 0.000913 
Batch[4937] - loss: 0.000475 
Batch[4938] - loss: 0.001013 
Batch[4939] - loss: 0.001243 
Batch[4940] - loss: 0.000749 
Batch[4941] - loss: 0.000926 
Batch[4942] - loss: 0.000412 
Batch[4943] - loss: 0.000796 
Batch[4944] - loss: 0.001176 
Batch[4945] - loss: 0.000844 
Batch[4946] - loss: 0.000713 
Batch[4947] - loss: 0.000726 
Batch[4948] - loss: 0.001383 
Batch[4949] - loss: 0.000340 
Batch[4950] - loss: 0.000810 
Batch[4951] - loss: 0.001216 
Batch[4952] - loss: 0.000964 
Batch[4953] - loss: 0.000814 
Batch[4954] - loss: 0.000697 
Batch[4955] - loss: 0.000876 
Batch[4956] - loss: 0.000431 
Batch[4957] - loss: 0.000730 
Batch[4958] - loss: 0.000606 
Batch[4959] - loss: 0.000666 
Batch[4960] - loss: 0.000473 
Batch[4961] - loss: 0.000751 
Batch[4962] - loss: 0.000546 
Batch[4963] - loss: 0.000730 
Batch[4964] - loss: 0.000466 
Batch[4965] - loss: 0.000819 
Batch[4966] - loss: 0.000550 
Batch[4967] - loss: 0.000665 
Batch[4968] - loss: 0.000584 
Batch[4969] - loss: 0.000709 
Batch[4970] - loss: 0.000637 
Batch[4971] - loss: 0.001129 
Batch[4972] - loss: 0.000332 
Batch[4973] - loss: 0.000506 
Batch[4974] - loss: 0.000606 
Batch[4975] - loss: 0.000894 
Batch[4976] - loss: 0.001393 
Batch[4977] - loss: 0.000722 
Batch[4978] - loss: 0.000614 
Batch[4979] - loss: 0.000469 
Batch[4980] - loss: 0.000720 
Batch[4981] - loss: 0.000819 
Batch[4982] - loss: 0.000878 
Batch[4983] - loss: 0.001169 
Batch[4984] - loss: 0.000429 
Batch[4985] - loss: 0.000724 
Batch[4986] - loss: 0.000952 
Batch[4987] - loss: 0.001103 
Batch[4988] - loss: 0.000921 
Batch[4989] - loss: 0.001125 
Batch[4990] - loss: 0.000670 
Batch[4991] - loss: 0.000778 
Batch[4992] - loss: 0.000925 
Batch[4993] - loss: 0.000581 
Batch[4994] - loss: 0.001140 
Batch[4995] - loss: 0.000610 
Batch[4996] - loss: 0.000614 
Batch[4997] - loss: 0.000629 
Batch[4998] - loss: 0.000619 
Batch[4999] - loss: 0.000973 
Batch[5000] - loss: 0.001678 

Evaluation - loss: 0.000068 pearson: 0.5574 

Batch[5001] - loss: 0.000720 
Batch[5002] - loss: 0.001238 
Batch[5003] - loss: 0.000533 
Batch[5004] - loss: 0.001316 
Batch[5005] - loss: 0.000786 
Batch[5006] - loss: 0.000723 
Batch[5007] - loss: 0.000687 
Batch[5008] - loss: 0.001169 
Batch[5009] - loss: 0.001149 
Batch[5010] - loss: 0.000706 
Batch[5011] - loss: 0.000704 
Batch[5012] - loss: 0.001077 
Batch[5013] - loss: 0.001052 
Batch[5014] - loss: 0.000929 
Batch[5015] - loss: 0.000827 
Batch[5016] - loss: 0.000289 
Batch[5017] - loss: 0.000669 
Batch[5018] - loss: 0.001034 
Batch[5019] - loss: 0.000939 
Batch[5020] - loss: 0.000593 
Batch[5021] - loss: 0.000607 
Batch[5022] - loss: 0.000672 
Batch[5023] - loss: 0.000516 
Batch[5024] - loss: 0.000699 
Batch[5025] - loss: 0.000325 
Batch[5026] - loss: 0.000611 
Batch[5027] - loss: 0.000790 
Batch[5028] - loss: 0.000471 
Batch[5029] - loss: 0.000664 
Batch[5030] - loss: 0.000633 
Batch[5031] - loss: 0.000831 
Batch[5032] - loss: 0.001126 
Batch[5033] - loss: 0.000521 
Batch[5034] - loss: 0.000591 
Batch[5035] - loss: 0.000515 
Batch[5036] - loss: 0.000874 
Batch[5037] - loss: 0.000629 
Batch[5038] - loss: 0.000651 
Batch[5039] - loss: 0.000373 
Batch[5040] - loss: 0.000663 
Batch[5041] - loss: 0.000700 
Batch[5042] - loss: 0.000559 
Batch[5043] - loss: 0.000507 
Batch[5044] - loss: 0.001040 
Batch[5045] - loss: 0.000607 
Batch[5046] - loss: 0.000897 
Batch[5047] - loss: 0.000580 
Batch[5048] - loss: 0.000533 
Batch[5049] - loss: 0.000863 
Batch[5050] - loss: 0.001042 
Batch[5051] - loss: 0.000904 
Batch[5052] - loss: 0.001645 
Batch[5053] - loss: 0.000966 
Batch[5054] - loss: 0.000858 
Batch[5055] - loss: 0.000630 
Batch[5056] - loss: 0.000786 
Batch[5057] - loss: 0.000499 
Batch[5058] - loss: 0.000693 
Batch[5059] - loss: 0.000869 
Batch[5060] - loss: 0.001162 
Batch[5061] - loss: 0.000758 
Batch[5062] - loss: 0.000663 
Batch[5063] - loss: 0.000611 
Batch[5064] - loss: 0.000708 
Batch[5065] - loss: 0.000651 
Batch[5066] - loss: 0.000715 
Batch[5067] - loss: 0.000693 
Batch[5068] - loss: 0.000896 
Batch[5069] - loss: 0.001129 
Batch[5070] - loss: 0.000507 
Batch[5071] - loss: 0.000368 
Batch[5072] - loss: 0.000505 
Batch[5073] - loss: 0.000502 
Batch[5074] - loss: 0.000523 
Batch[5075] - loss: 0.001645 
Batch[5076] - loss: 0.000764 
Batch[5077] - loss: 0.000702 
Batch[5078] - loss: 0.001082 
Batch[5079] - loss: 0.000805 
Batch[5080] - loss: 0.000690 
Batch[5081] - loss: 0.000805 
Batch[5082] - loss: 0.000639 
Batch[5083] - loss: 0.000553 
Batch[5084] - loss: 0.000850 
Batch[5085] - loss: 0.000783 
Batch[5086] - loss: 0.000824 
Batch[5087] - loss: 0.000533 
Batch[5088] - loss: 0.000640 
Batch[5089] - loss: 0.001043 
Batch[5090] - loss: 0.000815 
Batch[5091] - loss: 0.000447 
Batch[5092] - loss: 0.000464 
Batch[5093] - loss: 0.000854 
Batch[5094] - loss: 0.000973 
Batch[5095] - loss: 0.001135 
Batch[5096] - loss: 0.000669 
Batch[5097] - loss: 0.001031 
Batch[5098] - loss: 0.001052 
Batch[5099] - loss: 0.000856 
Batch[5100] - loss: 0.000576 

Evaluation - loss: 0.000068 pearson: 0.5544 

early stop by 1500 steps.
Batch[5101] - loss: 0.000513 
Batch[5102] - loss: 0.000851 
Batch[5103] - loss: 0.000465 
Batch[5104] - loss: 0.000666 
Batch[5105] - loss: 0.000569 
Batch[5106] - loss: 0.000437 
Batch[5107] - loss: 0.000430 
Batch[5108] - loss: 0.001241 
Batch[5109] - loss: 0.000430 
Batch[5110] - loss: 0.000588 
Batch[5111] - loss: 0.000769 
Batch[5112] - loss: 0.000445 
Batch[5113] - loss: 0.000825 
Batch[5114] - loss: 0.001120 
Batch[5115] - loss: 0.000463 
Batch[5116] - loss: 0.000779 
Batch[5117] - loss: 0.001127 
Batch[5118] - loss: 0.000697 
Batch[5119] - loss: 0.000624 
Batch[5120] - loss: 0.000707 
Batch[5121] - loss: 0.000898 
Batch[5122] - loss: 0.000708 
Batch[5123] - loss: 0.000397 
Batch[5124] - loss: 0.000532 
Batch[5125] - loss: 0.000336 
Batch[5126] - loss: 0.000610 
Batch[5127] - loss: 0.000310 
Batch[5128] - loss: 0.001185 
Batch[5129] - loss: 0.001102 
Batch[5130] - loss: 0.000913 
Batch[5131] - loss: 0.000941 
Batch[5132] - loss: 0.001283 
Batch[5133] - loss: 0.000429 
Batch[5134] - loss: 0.000559 
Batch[5135] - loss: 0.001131 
Batch[5136] - loss: 0.000805 
Batch[5137] - loss: 0.001198 
Batch[5138] - loss: 0.000690 
Batch[5139] - loss: 0.000550 
Batch[5140] - loss: 0.001574 
Batch[5141] - loss: 0.000760 
Batch[5142] - loss: 0.001343 
Batch[5143] - loss: 0.000663 
Batch[5144] - loss: 0.000826 
Batch[5145] - loss: 0.000681 
Batch[5146] - loss: 0.000754 
Batch[5147] - loss: 0.000837 
Batch[5148] - loss: 0.000920 
Batch[5149] - loss: 0.000795 
Batch[5150] - loss: 0.000491 
Batch[5151] - loss: 0.000506 
Batch[5152] - loss: 0.000523 
Batch[5153] - loss: 0.000720 
Batch[5154] - loss: 0.001267 
Batch[5155] - loss: 0.000566 
Batch[5156] - loss: 0.000453 
Batch[5157] - loss: 0.000891 
Batch[5158] - loss: 0.001022 
Batch[5159] - loss: 0.000704 
Batch[5160] - loss: 0.000430 
Batch[5161] - loss: 0.000808 
Batch[5162] - loss: 0.000452 
Batch[5163] - loss: 0.000522 
Batch[5164] - loss: 0.001153 
Batch[5165] - loss: 0.000715 
Batch[5166] - loss: 0.000602 
Batch[5167] - loss: 0.000539 
Batch[5168] - loss: 0.000758 
Batch[5169] - loss: 0.000618 
Batch[5170] - loss: 0.000519 
Batch[5171] - loss: 0.000389 
Batch[5172] - loss: 0.000464 
Batch[5173] - loss: 0.000558 
Batch[5174] - loss: 0.000822 
Batch[5175] - loss: 0.001278 
Batch[5176] - loss: 0.001555 
Batch[5177] - loss: 0.000494 
Batch[5178] - loss: 0.000439 
Batch[5179] - loss: 0.000575 
Batch[5180] - loss: 0.000542 
Batch[5181] - loss: 0.001023 
Batch[5182] - loss: 0.000828 
Batch[5183] - loss: 0.000914 
Batch[5184] - loss: 0.001069 
Batch[5185] - loss: 0.000547 
Batch[5186] - loss: 0.000855 
Batch[5187] - loss: 0.000700 
Batch[5188] - loss: 0.001716 
Batch[5189] - loss: 0.000957 
Batch[5190] - loss: 0.000809 
Batch[5191] - loss: 0.000729 
Batch[5192] - loss: 0.000541 
Batch[5193] - loss: 0.000679 
Batch[5194] - loss: 0.000631 
Batch[5195] - loss: 0.000814 
Batch[5196] - loss: 0.000613 
Batch[5197] - loss: 0.000586 
Batch[5198] - loss: 0.000537 
Batch[5199] - loss: 0.000712 
Batch[5200] - loss: 0.000454 

Evaluation - loss: 0.000068 pearson: 0.5567 

early stop by 1500 steps.
Batch[5201] - loss: 0.000485 
Batch[5202] - loss: 0.000966 
Batch[5203] - loss: 0.000954 
Batch[5204] - loss: 0.000619 
Batch[5205] - loss: 0.001416 
Batch[5206] - loss: 0.000584 
Batch[5207] - loss: 0.000589 
Batch[5208] - loss: 0.001290 
Batch[5209] - loss: 0.000590 
Batch[5210] - loss: 0.000467 
Batch[5211] - loss: 0.000324 
Batch[5212] - loss: 0.001470 
Batch[5213] - loss: 0.000918 
Batch[5214] - loss: 0.000585 
Batch[5215] - loss: 0.000648 
Batch[5216] - loss: 0.001057 
Batch[5217] - loss: 0.000709 
Batch[5218] - loss: 0.000834 
Batch[5219] - loss: 0.001019 
Batch[5220] - loss: 0.000892 
Batch[5221] - loss: 0.000852 
Batch[5222] - loss: 0.000680 
Batch[5223] - loss: 0.000894 
Batch[5224] - loss: 0.000700 
Batch[5225] - loss: 0.000668 
Batch[5226] - loss: 0.000675 
Batch[5227] - loss: 0.000515 
Batch[5228] - loss: 0.000928 
Batch[5229] - loss: 0.000789 
Batch[5230] - loss: 0.000697 
Batch[5231] - loss: 0.000857 
Batch[5232] - loss: 0.000643 
Batch[5233] - loss: 0.000385 
Batch[5234] - loss: 0.001123 
Batch[5235] - loss: 0.000358 
Batch[5236] - loss: 0.000726 
Batch[5237] - loss: 0.000547 
Batch[5238] - loss: 0.001200 
Batch[5239] - loss: 0.000630 
Batch[5240] - loss: 0.000511 
Batch[5241] - loss: 0.000686 
Batch[5242] - loss: 0.000419 
Batch[5243] - loss: 0.000667 
Batch[5244] - loss: 0.000757 
Batch[5245] - loss: 0.000573 
Batch[5246] - loss: 0.001023 
Batch[5247] - loss: 0.000842 
Batch[5248] - loss: 0.000488 
Batch[5249] - loss: 0.000646 
Batch[5250] - loss: 0.000924 
Batch[5251] - loss: 0.000498 
Batch[5252] - loss: 0.000578 
Batch[5253] - loss: 0.000757 
Batch[5254] - loss: 0.000766 
Batch[5255] - loss: 0.001139 
Batch[5256] - loss: 0.000427 
Batch[5257] - loss: 0.000736 
Batch[5258] - loss: 0.000968 
Batch[5259] - loss: 0.000674 
Batch[5260] - loss: 0.000732 
Batch[5261] - loss: 0.000888 
Batch[5262] - loss: 0.000461 
Batch[5263] - loss: 0.000512 
Batch[5264] - loss: 0.000641 
Batch[5265] - loss: 0.001200 
Batch[5266] - loss: 0.000769 
Batch[5267] - loss: 0.001046 
Batch[5268] - loss: 0.000982 
Batch[5269] - loss: 0.000638 
Batch[5270] - loss: 0.000394 
Batch[5271] - loss: 0.000840 
Batch[5272] - loss: 0.000476 
Batch[5273] - loss: 0.000951 
Batch[5274] - loss: 0.000603 
Batch[5275] - loss: 0.000507 
Batch[5276] - loss: 0.000607 
Batch[5277] - loss: 0.000523 
Batch[5278] - loss: 0.001219 
Batch[5279] - loss: 0.000770 
Batch[5280] - loss: 0.000728 
Batch[5281] - loss: 0.000758 
Batch[5282] - loss: 0.000702 
Batch[5283] - loss: 0.000565 
Batch[5284] - loss: 0.000617 
Batch[5285] - loss: 0.000355 
Batch[5286] - loss: 0.000624 
Batch[5287] - loss: 0.000476 
Batch[5288] - loss: 0.000588 
Batch[5289] - loss: 0.000493 
Batch[5290] - loss: 0.001102 
Batch[5291] - loss: 0.000865 
Batch[5292] - loss: 0.001030 
Batch[5293] - loss: 0.000353 
Batch[5294] - loss: 0.000896 
Batch[5295] - loss: 0.000829 
Batch[5296] - loss: 0.000778 
Batch[5297] - loss: 0.000832 
Batch[5298] - loss: 0.000744 
Batch[5299] - loss: 0.000632 
Batch[5300] - loss: 0.000691 

Evaluation - loss: 0.000068 pearson: 0.5583 

early stop by 1500 steps.
Batch[5301] - loss: 0.000462 
Batch[5302] - loss: 0.000947 
Batch[5303] - loss: 0.000557 
Batch[5304] - loss: 0.000782 
Batch[5305] - loss: 0.000499 
Batch[5306] - loss: 0.000740 
Batch[5307] - loss: 0.000928 
Batch[5308] - loss: 0.000597 
Batch[5309] - loss: 0.000519 
Batch[5310] - loss: 0.000654 
Batch[5311] - loss: 0.000979 
Batch[5312] - loss: 0.001237 
Batch[5313] - loss: 0.001307 
Batch[5314] - loss: 0.000216 
Batch[5315] - loss: 0.000405 
Batch[5316] - loss: 0.000744 
Batch[5317] - loss: 0.000505 
Batch[5318] - loss: 0.000393 
Batch[5319] - loss: 0.000504 
Batch[5320] - loss: 0.000771 
Batch[5321] - loss: 0.000505 
Batch[5322] - loss: 0.000601 
Batch[5323] - loss: 0.001785 
Batch[5324] - loss: 0.000614 
Batch[5325] - loss: 0.001026 
Batch[5326] - loss: 0.000442 
Batch[5327] - loss: 0.000970 
Batch[5328] - loss: 0.000926 
Batch[5329] - loss: 0.000601 
Batch[5330] - loss: 0.001129 
Batch[5331] - loss: 0.000507 
Batch[5332] - loss: 0.000351 
Batch[5333] - loss: 0.000713 
Batch[5334] - loss: 0.000702 
Batch[5335] - loss: 0.000703 
Batch[5336] - loss: 0.000816 
Batch[5337] - loss: 0.001260 
Batch[5338] - loss: 0.000641 
Batch[5339] - loss: 0.000606 
Batch[5340] - loss: 0.000651 
Batch[5341] - loss: 0.000744 
Batch[5342] - loss: 0.000862 
Batch[5343] - loss: 0.000646 
Batch[5344] - loss: 0.000795 
Batch[5345] - loss: 0.000914 
Batch[5346] - loss: 0.000366 
Batch[5347] - loss: 0.000578 
Batch[5348] - loss: 0.000694 
Batch[5349] - loss: 0.000558 
Batch[5350] - loss: 0.000904 
Batch[5351] - loss: 0.000633 
Batch[5352] - loss: 0.000445 
Batch[5353] - loss: 0.000669 
Batch[5354] - loss: 0.000613 
Batch[5355] - loss: 0.000591 
Batch[5356] - loss: 0.001016 
Batch[5357] - loss: 0.000482 
Batch[5358] - loss: 0.000852 
Batch[5359] - loss: 0.000349 
Batch[5360] - loss: 0.000509 
Batch[5361] - loss: 0.001002 
Batch[5362] - loss: 0.001140 
Batch[5363] - loss: 0.000761 
Batch[5364] - loss: 0.000657 
Batch[5365] - loss: 0.000860 
Batch[5366] - loss: 0.000532 
Batch[5367] - loss: 0.000448 
Batch[5368] - loss: 0.000844 
Batch[5369] - loss: 0.001052 
Batch[5370] - loss: 0.001306 
Batch[5371] - loss: 0.000948 
Batch[5372] - loss: 0.000929 
Batch[5373] - loss: 0.000536 
Batch[5374] - loss: 0.000843 
Batch[5375] - loss: 0.000774 
Batch[5376] - loss: 0.000497 
Batch[5377] - loss: 0.000874 
Batch[5378] - loss: 0.000774 
Batch[5379] - loss: 0.000841 
Batch[5380] - loss: 0.000708 
Batch[5381] - loss: 0.000861 
Batch[5382] - loss: 0.001618 
Batch[5383] - loss: 0.000503 
Batch[5384] - loss: 0.000437 
Batch[5385] - loss: 0.001021 
Batch[5386] - loss: 0.000348 
Batch[5387] - loss: 0.000678 
Batch[5388] - loss: 0.000369 
Batch[5389] - loss: 0.000579 
Batch[5390] - loss: 0.000609 
Batch[5391] - loss: 0.000740 
Batch[5392] - loss: 0.000838 
Batch[5393] - loss: 0.000513 
Batch[5394] - loss: 0.000652 
Batch[5395] - loss: 0.000521 
Batch[5396] - loss: 0.000603 
Batch[5397] - loss: 0.000952 
Batch[5398] - loss: 0.000733 
Batch[5399] - loss: 0.000812 
Batch[5400] - loss: 0.000778 

Evaluation - loss: 0.000068 pearson: 0.5569 

early stop by 1500 steps.
Batch[5401] - loss: 0.001483 
Batch[5402] - loss: 0.000586 
Batch[5403] - loss: 0.000416 
Batch[5404] - loss: 0.000738 
Batch[5405] - loss: 0.001548 
Batch[5406] - loss: 0.000312 
Batch[5407] - loss: 0.001228 
Batch[5408] - loss: 0.000682 
Batch[5409] - loss: 0.000639 
Batch[5410] - loss: 0.000529 
Batch[5411] - loss: 0.000852 
Batch[5412] - loss: 0.000501 
Batch[5413] - loss: 0.000435 
Batch[5414] - loss: 0.000945 
Batch[5415] - loss: 0.000863 
Batch[5416] - loss: 0.000761 
Batch[5417] - loss: 0.000511 
Batch[5418] - loss: 0.001067 
Batch[5419] - loss: 0.000893 
Batch[5420] - loss: 0.001080 
Batch[5421] - loss: 0.000875 
Batch[5422] - loss: 0.000745 
Batch[5423] - loss: 0.001215 
Batch[5424] - loss: 0.000634 
Batch[5425] - loss: 0.000577 
Batch[5426] - loss: 0.001073 
Batch[5427] - loss: 0.000311 
Batch[5428] - loss: 0.000984 
Batch[5429] - loss: 0.000895 
Batch[5430] - loss: 0.000824 
Batch[5431] - loss: 0.000706 
Batch[5432] - loss: 0.001203 
Batch[5433] - loss: 0.000874 
Batch[5434] - loss: 0.000444 
Batch[5435] - loss: 0.000441 
Batch[5436] - loss: 0.001997 
Batch[5437] - loss: 0.000670 
Batch[5438] - loss: 0.000998 
Batch[5439] - loss: 0.000724 
Batch[5440] - loss: 0.000467 
Batch[5441] - loss: 0.000574 
Batch[5442] - loss: 0.001674 
Batch[5443] - loss: 0.000681 
Batch[5444] - loss: 0.000766 
Batch[5445] - loss: 0.001226 
Batch[5446] - loss: 0.000768 
Batch[5447] - loss: 0.001470 
Batch[5448] - loss: 0.000866 
Batch[5449] - loss: 0.000460 
Batch[5450] - loss: 0.000338 
Batch[5451] - loss: 0.000465 
Batch[5452] - loss: 0.000513 
Batch[5453] - loss: 0.000659 
Batch[5454] - loss: 0.000670 
Batch[5455] - loss: 0.001069 
Batch[5456] - loss: 0.000818 
Batch[5457] - loss: 0.000674 
Batch[5458] - loss: 0.000803 
Batch[5459] - loss: 0.000454 
Batch[5460] - loss: 0.000994 
Batch[5461] - loss: 0.000369 
Batch[5462] - loss: 0.000732 
Batch[5463] - loss: 0.000569 
Batch[5464] - loss: 0.000996 
Batch[5465] - loss: 0.000829 
Batch[5466] - loss: 0.000897 
Batch[5467] - loss: 0.000615 
Batch[5468] - loss: 0.000852 
Batch[5469] - loss: 0.001167 
Batch[5470] - loss: 0.001733 
Batch[5471] - loss: 0.000456 
Batch[5472] - loss: 0.000708 
Batch[5473] - loss: 0.001101 
Batch[5474] - loss: 0.000894 
Batch[5475] - loss: 0.000638 
Batch[5476] - loss: 0.000673 
Batch[5477] - loss: 0.000612 
Batch[5478] - loss: 0.001472 
Batch[5479] - loss: 0.001393 
Batch[5480] - loss: 0.000814 
Batch[5481] - loss: 0.000641 
Batch[5482] - loss: 0.000628 
Batch[5483] - loss: 0.000553 
Batch[5484] - loss: 0.000709 
Batch[5485] - loss: 0.000441 
Batch[5486] - loss: 0.001012 
Batch[5487] - loss: 0.000637 
Batch[5488] - loss: 0.000593 
Batch[5489] - loss: 0.000566 
Batch[5490] - loss: 0.000310 
Batch[5491] - loss: 0.000748 
Batch[5492] - loss: 0.000641 
Batch[5493] - loss: 0.000735 
Batch[5494] - loss: 0.000809 
Batch[5495] - loss: 0.000567 
Batch[5496] - loss: 0.000715 
Batch[5497] - loss: 0.000634 
Batch[5498] - loss: 0.000543 
Batch[5499] - loss: 0.000535 
Batch[5500] - loss: 0.000872 

Evaluation - loss: 0.000068 pearson: 0.5581 

early stop by 1500 steps.
Batch[5501] - loss: 0.000854 
Batch[5502] - loss: 0.000812 
Batch[5503] - loss: 0.000644 
Batch[5504] - loss: 0.000546 
Batch[5505] - loss: 0.000738 
Batch[5506] - loss: 0.001390 
Batch[5507] - loss: 0.000831 
Batch[5508] - loss: 0.000910 
Batch[5509] - loss: 0.000657 
Batch[5510] - loss: 0.000726 
Batch[5511] - loss: 0.000472 
Batch[5512] - loss: 0.000442 
Batch[5513] - loss: 0.000801 
Batch[5514] - loss: 0.000461 
Batch[5515] - loss: 0.000628 
Batch[5516] - loss: 0.000619 
Batch[5517] - loss: 0.000724 
Batch[5518] - loss: 0.000780 
Batch[5519] - loss: 0.000286 
Batch[5520] - loss: 0.000435 
Batch[5521] - loss: 0.000598 
Batch[5522] - loss: 0.000511 
Batch[5523] - loss: 0.000412 
Batch[5524] - loss: 0.000842 
Batch[5525] - loss: 0.000804 
Batch[5526] - loss: 0.000440 
Batch[5527] - loss: 0.000702 
Batch[5528] - loss: 0.000419 
Batch[5529] - loss: 0.001073 
Batch[5530] - loss: 0.000892 
Batch[5531] - loss: 0.000437 
Batch[5532] - loss: 0.000452 
Batch[5533] - loss: 0.000753 
Batch[5534] - loss: 0.000506 
Batch[5535] - loss: 0.000645 
Batch[5536] - loss: 0.000972 
Batch[5537] - loss: 0.001062 
Batch[5538] - loss: 0.000681 
Batch[5539] - loss: 0.000652 
Batch[5540] - loss: 0.000864 
Batch[5541] - loss: 0.000448 
Batch[5542] - loss: 0.000711 
Batch[5543] - loss: 0.000423 
Batch[5544] - loss: 0.000589 
Batch[5545] - loss: 0.000598 
Batch[5546] - loss: 0.000664 
Batch[5547] - loss: 0.000786 
Batch[5548] - loss: 0.000399 
Batch[5549] - loss: 0.000776 
Batch[5550] - loss: 0.000502 
Batch[5551] - loss: 0.000505 
Batch[5552] - loss: 0.001005 
Batch[5553] - loss: 0.001177 
Batch[5554] - loss: 0.000585 
Batch[5555] - loss: 0.000511 
Batch[5556] - loss: 0.001060 
Batch[5557] - loss: 0.000319 
Batch[5558] - loss: 0.000592 
Batch[5559] - loss: 0.000534 
Batch[5560] - loss: 0.000355 
Batch[5561] - loss: 0.000662 
Batch[5562] - loss: 0.001062 
Batch[5563] - loss: 0.001077 
Batch[5564] - loss: 0.000611 
Batch[5565] - loss: 0.000414 
Batch[5566] - loss: 0.000333 
Batch[5567] - loss: 0.000440 
Batch[5568] - loss: 0.000853 
Batch[5569] - loss: 0.000400 
Batch[5570] - loss: 0.000687 
Batch[5571] - loss: 0.000464 
Batch[5572] - loss: 0.000897 
Batch[5573] - loss: 0.000808 
Batch[5574] - loss: 0.000336 
Batch[5575] - loss: 0.000909 
Batch[5576] - loss: 0.000612 
Batch[5577] - loss: 0.000625 
Batch[5578] - loss: 0.000439 
Batch[5579] - loss: 0.000657 
Batch[5580] - loss: 0.001026 
Batch[5581] - loss: 0.000668 
Batch[5582] - loss: 0.000610 
Batch[5583] - loss: 0.000646 
Batch[5584] - loss: 0.000840 
Batch[5585] - loss: 0.000783 
Batch[5586] - loss: 0.000612 
Batch[5587] - loss: 0.001217 
Batch[5588] - loss: 0.000790 
Batch[5589] - loss: 0.000318 
Batch[5590] - loss: 0.000410 
Batch[5591] - loss: 0.000714 
Batch[5592] - loss: 0.000546 
Batch[5593] - loss: 0.001403 
Batch[5594] - loss: 0.000374 
Batch[5595] - loss: 0.000162 
Batch[5596] - loss: 0.000893 
Batch[5597] - loss: 0.000896 
Batch[5598] - loss: 0.000766 
Batch[5599] - loss: 0.000664 
Batch[5600] - loss: 0.000508 

Evaluation - loss: 0.000068 pearson: 0.5557 

early stop by 1500 steps.
Batch[5601] - loss: 0.000703 
Batch[5602] - loss: 0.000581 
Batch[5603] - loss: 0.000515 
Batch[5604] - loss: 0.000569 
Batch[5605] - loss: 0.000853 
Batch[5606] - loss: 0.000532 
Batch[5607] - loss: 0.000568 
Batch[5608] - loss: 0.000329 
Batch[5609] - loss: 0.000412 
Batch[5610] - loss: 0.000784 
Batch[5611] - loss: 0.000919 
Batch[5612] - loss: 0.001163 
Batch[5613] - loss: 0.000481 
Batch[5614] - loss: 0.000638 
Batch[5615] - loss: 0.000619 
Batch[5616] - loss: 0.000835 
Batch[5617] - loss: 0.000413 
Batch[5618] - loss: 0.001018 
Batch[5619] - loss: 0.000677 
Batch[5620] - loss: 0.000523 
Batch[5621] - loss: 0.000307 
Batch[5622] - loss: 0.000479 
Batch[5623] - loss: 0.000758 
Batch[5624] - loss: 0.001061 
Batch[5625] - loss: 0.000687 
Batch[5626] - loss: 0.000868 
Batch[5627] - loss: 0.000857 
Batch[5628] - loss: 0.000743 
Batch[5629] - loss: 0.000464 
Batch[5630] - loss: 0.000798 
Batch[5631] - loss: 0.000616 
Batch[5632] - loss: 0.000672 
Batch[5633] - loss: 0.000514 
Batch[5634] - loss: 0.001329 
Batch[5635] - loss: 0.000458 
Batch[5636] - loss: 0.000607 
Batch[5637] - loss: 0.000673 
Batch[5638] - loss: 0.000508 
Batch[5639] - loss: 0.000390 
Batch[5640] - loss: 0.000472 
Batch[5641] - loss: 0.000859 
Batch[5642] - loss: 0.000729 
Batch[5643] - loss: 0.000676 
Batch[5644] - loss: 0.000932 
Batch[5645] - loss: 0.000779 
Batch[5646] - loss: 0.000448 
Batch[5647] - loss: 0.000465 
Batch[5648] - loss: 0.000769 
Batch[5649] - loss: 0.000569 
Batch[5650] - loss: 0.000538 
Batch[5651] - loss: 0.000706 
Batch[5652] - loss: 0.000445 
Batch[5653] - loss: 0.000752 
Batch[5654] - loss: 0.000573 
Batch[5655] - loss: 0.000364 
Batch[5656] - loss: 0.000488 
Batch[5657] - loss: 0.000481 
Batch[5658] - loss: 0.000550 
Batch[5659] - loss: 0.000484 
Batch[5660] - loss: 0.000455 
Batch[5661] - loss: 0.000390 
Batch[5662] - loss: 0.000752 
Batch[5663] - loss: 0.000421 
Batch[5664] - loss: 0.000785 
Batch[5665] - loss: 0.000712 
Batch[5666] - loss: 0.000542 
Batch[5667] - loss: 0.000864 
Batch[5668] - loss: 0.000688 
Batch[5669] - loss: 0.000409 
Batch[5670] - loss: 0.001256 
Batch[5671] - loss: 0.000774 
Batch[5672] - loss: 0.000584 
Batch[5673] - loss: 0.000708 
Batch[5674] - loss: 0.001034 
Batch[5675] - loss: 0.000445 
Batch[5676] - loss: 0.000389 
Batch[5677] - loss: 0.000889 
Batch[5678] - loss: 0.000550 
Batch[5679] - loss: 0.000824 
Batch[5680] - loss: 0.000438 
Batch[5681] - loss: 0.000596 
Batch[5682] - loss: 0.000573 
Batch[5683] - loss: 0.000715 
Batch[5684] - loss: 0.000556 
Batch[5685] - loss: 0.000497 
Batch[5686] - loss: 0.000618 
Batch[5687] - loss: 0.000651 
Batch[5688] - loss: 0.000513 
Batch[5689] - loss: 0.000677 
Batch[5690] - loss: 0.000720 
Batch[5691] - loss: 0.000681 
Batch[5692] - loss: 0.000550 
Batch[5693] - loss: 0.000719 
Batch[5694] - loss: 0.000742 
Batch[5695] - loss: 0.001062 
Batch[5696] - loss: 0.000543 
Batch[5697] - loss: 0.000939 
Batch[5698] - loss: 0.000357 
Batch[5699] - loss: 0.000475 
Batch[5700] - loss: 0.000657 

Evaluation - loss: 0.000068 pearson: 0.5564 

early stop by 1500 steps.
Batch[5701] - loss: 0.000440 
Batch[5702] - loss: 0.000867 
Batch[5703] - loss: 0.000861 
Batch[5704] - loss: 0.001133 
Batch[5705] - loss: 0.000970 
Batch[5706] - loss: 0.000444 
Batch[5707] - loss: 0.000593 
Batch[5708] - loss: 0.001194 
Batch[5709] - loss: 0.000708 
Batch[5710] - loss: 0.000660 
Batch[5711] - loss: 0.000882 
Batch[5712] - loss: 0.000427 
Batch[5713] - loss: 0.000474 
Batch[5714] - loss: 0.001288 
Batch[5715] - loss: 0.000743 
Batch[5716] - loss: 0.001172 
Batch[5717] - loss: 0.000619 
Batch[5718] - loss: 0.000690 
Batch[5719] - loss: 0.000546 
Batch[5720] - loss: 0.000785 
Batch[5721] - loss: 0.000655 
Batch[5722] - loss: 0.000634 
Batch[5723] - loss: 0.000338 
Batch[5724] - loss: 0.000452 
Batch[5725] - loss: 0.000568 
Batch[5726] - loss: 0.001065 
Batch[5727] - loss: 0.000326 
Batch[5728] - loss: 0.000594 
Batch[5729] - loss: 0.000569 
Batch[5730] - loss: 0.000626 
Batch[5731] - loss: 0.000617 
Batch[5732] - loss: 0.000623 
Batch[5733] - loss: 0.000509 
Batch[5734] - loss: 0.000524 
Batch[5735] - loss: 0.000497 
Batch[5736] - loss: 0.000401 
Batch[5737] - loss: 0.000910 
Batch[5738] - loss: 0.000444 
Batch[5739] - loss: 0.000615 
Batch[5740] - loss: 0.000547 
Batch[5741] - loss: 0.000754 
Batch[5742] - loss: 0.000955 
Batch[5743] - loss: 0.000697 
Batch[5744] - loss: 0.000565 
Batch[5745] - loss: 0.000673 
Batch[5746] - loss: 0.000639 
Batch[5747] - loss: 0.000919 
Batch[5748] - loss: 0.000862 
Batch[5749] - loss: 0.000379 
Batch[5750] - loss: 0.000385 
Batch[5751] - loss: 0.000333 
Batch[5752] - loss: 0.001034 
Batch[5753] - loss: 0.000775 
Batch[5754] - loss: 0.000935 
Batch[5755] - loss: 0.000496 
Batch[5756] - loss: 0.000744 
Batch[5757] - loss: 0.000428 
Batch[5758] - loss: 0.001119 
Batch[5759] - loss: 0.000728 
Batch[5760] - loss: 0.000873 
Batch[5761] - loss: 0.000563 
Batch[5762] - loss: 0.000859 
Batch[5763] - loss: 0.000777 
Batch[5764] - loss: 0.000678 
Batch[5765] - loss: 0.000455 
Batch[5766] - loss: 0.000758 
Batch[5767] - loss: 0.001655 
Batch[5768] - loss: 0.000712 
Batch[5769] - loss: 0.000950 
Batch[5770] - loss: 0.000670 
Batch[5771] - loss: 0.000841 
Batch[5772] - loss: 0.000697 
Batch[5773] - loss: 0.000393 
Batch[5774] - loss: 0.000764 
Batch[5775] - loss: 0.000651 
Batch[5776] - loss: 0.000578 
Batch[5777] - loss: 0.000821 
Batch[5778] - loss: 0.000909 
Batch[5779] - loss: 0.000508 
Batch[5780] - loss: 0.000439 
Batch[5781] - loss: 0.000774 
Batch[5782] - loss: 0.000907 
Batch[5783] - loss: 0.000508 
Batch[5784] - loss: 0.000615 
Batch[5785] - loss: 0.000504 
Batch[5786] - loss: 0.000278 
Batch[5787] - loss: 0.000555 
Batch[5788] - loss: 0.000809 
Batch[5789] - loss: 0.000803 
Batch[5790] - loss: 0.000457 
Batch[5791] - loss: 0.000601 
Batch[5792] - loss: 0.000670 
Batch[5793] - loss: 0.000962 
Batch[5794] - loss: 0.000501 
Batch[5795] - loss: 0.001126 
Batch[5796] - loss: 0.000860 
Batch[5797] - loss: 0.000742 
Batch[5798] - loss: 0.000419 
Batch[5799] - loss: 0.001184 
Batch[5800] - loss: 0.000683 

Evaluation - loss: 0.000068 pearson: 0.5562 

early stop by 1500 steps.
Batch[5801] - loss: 0.000520 
Batch[5802] - loss: 0.000624 
Batch[5803] - loss: 0.000636 
Batch[5804] - loss: 0.000622 
Batch[5805] - loss: 0.000517 
Batch[5806] - loss: 0.000384 
Batch[5807] - loss: 0.000974 
Batch[5808] - loss: 0.000684 
Batch[5809] - loss: 0.000318 
Batch[5810] - loss: 0.000910 
Batch[5811] - loss: 0.000666 
Batch[5812] - loss: 0.000664 
Batch[5813] - loss: 0.000560 
Batch[5814] - loss: 0.000554 
Batch[5815] - loss: 0.001644 
Batch[5816] - loss: 0.001350 
Batch[5817] - loss: 0.000760 
Batch[5818] - loss: 0.000696 
Batch[5819] - loss: 0.001135 
Batch[5820] - loss: 0.000367 
Batch[5821] - loss: 0.000706 
Batch[5822] - loss: 0.000546 
Batch[5823] - loss: 0.001005 
Batch[5824] - loss: 0.000688 
Batch[5825] - loss: 0.001191 
Batch[5826] - loss: 0.000614 
Batch[5827] - loss: 0.000731 
Batch[5828] - loss: 0.000788 
Batch[5829] - loss: 0.000493 
Batch[5830] - loss: 0.000657 
Batch[5831] - loss: 0.000746 
Batch[5832] - loss: 0.000879 
Batch[5833] - loss: 0.000439 
Batch[5834] - loss: 0.000842 
Batch[5835] - loss: 0.000616 
Batch[5836] - loss: 0.000583 
Batch[5837] - loss: 0.000946 
Batch[5838] - loss: 0.001162 
Batch[5839] - loss: 0.001049 
Batch[5840] - loss: 0.000700 
Batch[5841] - loss: 0.000606 
Batch[5842] - loss: 0.000411 
Batch[5843] - loss: 0.000479 
Batch[5844] - loss: 0.000857 
Batch[5845] - loss: 0.000972 
Batch[5846] - loss: 0.001065 
Batch[5847] - loss: 0.001076 
Batch[5848] - loss: 0.000486 
Batch[5849] - loss: 0.000570 
Batch[5850] - loss: 0.000748 
Batch[5851] - loss: 0.000960 
Batch[5852] - loss: 0.000777 
Batch[5853] - loss: 0.000615 
Batch[5854] - loss: 0.000706 
Batch[5855] - loss: 0.001098 
Batch[5856] - loss: 0.000621 
Batch[5857] - loss: 0.000590 
Batch[5858] - loss: 0.000493 
Batch[5859] - loss: 0.000707 
Batch[5860] - loss: 0.000564 
Batch[5861] - loss: 0.000953 
Batch[5862] - loss: 0.000618 
Batch[5863] - loss: 0.000770 
Batch[5864] - loss: 0.000388 
Batch[5865] - loss: 0.000566 
Batch[5866] - loss: 0.000573 
Batch[5867] - loss: 0.000717 
Batch[5868] - loss: 0.000444 
Batch[5869] - loss: 0.000664 
Batch[5870] - loss: 0.001085 
Batch[5871] - loss: 0.000621 
Batch[5872] - loss: 0.000807 
Batch[5873] - loss: 0.000720 
Batch[5874] - loss: 0.000396 
Batch[5875] - loss: 0.000448 
Batch[5876] - loss: 0.000461 
Batch[5877] - loss: 0.000499 
Batch[5878] - loss: 0.000496 
Batch[5879] - loss: 0.000671 
Batch[5880] - loss: 0.000528 
Batch[5881] - loss: 0.000820 
Batch[5882] - loss: 0.000846 
Batch[5883] - loss: 0.000623 
Batch[5884] - loss: 0.000530 
Batch[5885] - loss: 0.000968 
Batch[5886] - loss: 0.000542 
Batch[5887] - loss: 0.000290 
Batch[5888] - loss: 0.000574 
Batch[5889] - loss: 0.000602 
Batch[5890] - loss: 0.000672 
Batch[5891] - loss: 0.000508 
Batch[5892] - loss: 0.001078 
Batch[5893] - loss: 0.000843 
Batch[5894] - loss: 0.000723 
Batch[5895] - loss: 0.001149 
Batch[5896] - loss: 0.000759 
Batch[5897] - loss: 0.000539 
Batch[5898] - loss: 0.000834 
Batch[5899] - loss: 0.000384 
Batch[5900] - loss: 0.000893 

Evaluation - loss: 0.000068 pearson: 0.5546 

early stop by 1500 steps.
Batch[5901] - loss: 0.000754 
Batch[5902] - loss: 0.001172 
Batch[5903] - loss: 0.000707 
Batch[5904] - loss: 0.000749 
Batch[5905] - loss: 0.000438 
Batch[5906] - loss: 0.000333 
Batch[5907] - loss: 0.000546 
Batch[5908] - loss: 0.000579 
Batch[5909] - loss: 0.000515 
Batch[5910] - loss: 0.000461 
Batch[5911] - loss: 0.000745 
Batch[5912] - loss: 0.000521 
Batch[5913] - loss: 0.000440 
Batch[5914] - loss: 0.000785 
Batch[5915] - loss: 0.001213 
Batch[5916] - loss: 0.000614 
Batch[5917] - loss: 0.000908 
Batch[5918] - loss: 0.000891 
Batch[5919] - loss: 0.000489 
Batch[5920] - loss: 0.000773 
Batch[5921] - loss: 0.001057 
Batch[5922] - loss: 0.000527 
Batch[5923] - loss: 0.000835 
Batch[5924] - loss: 0.000437 
Batch[5925] - loss: 0.000466 
Batch[5926] - loss: 0.000639 
Batch[5927] - loss: 0.000708 
Batch[5928] - loss: 0.000478 
Batch[5929] - loss: 0.000889 
Batch[5930] - loss: 0.000919 
Batch[5931] - loss: 0.000312 
Batch[5932] - loss: 0.000545 
Batch[5933] - loss: 0.000387 
Batch[5934] - loss: 0.000920 
Batch[5935] - loss: 0.000764 
Batch[5936] - loss: 0.000468 
Batch[5937] - loss: 0.000616 
Batch[5938] - loss: 0.000691 
Batch[5939] - loss: 0.000743 
Batch[5940] - loss: 0.000380 
Batch[5941] - loss: 0.000602 
Batch[5942] - loss: 0.000381 
Batch[5943] - loss: 0.000802 
Batch[5944] - loss: 0.000301 
Batch[5945] - loss: 0.000549 
Batch[5946] - loss: 0.000689 
Batch[5947] - loss: 0.000539 
Batch[5948] - loss: 0.000539 
Batch[5949] - loss: 0.000332 
Batch[5950] - loss: 0.000678 
Batch[5951] - loss: 0.000885 
Batch[5952] - loss: 0.001381 
Batch[5953] - loss: 0.001004 
Batch[5954] - loss: 0.000748 
Batch[5955] - loss: 0.001016 
Batch[5956] - loss: 0.000843 
Batch[5957] - loss: 0.000428 
Batch[5958] - loss: 0.000792 
Batch[5959] - loss: 0.000319 
Batch[5960] - loss: 0.000728 
Batch[5961] - loss: 0.000755 
Batch[5962] - loss: 0.000701 
Batch[5963] - loss: 0.000364 
Batch[5964] - loss: 0.001274 
Batch[5965] - loss: 0.001033 
Batch[5966] - loss: 0.000716 
Batch[5967] - loss: 0.000676 
Batch[5968] - loss: 0.000905 
Batch[5969] - loss: 0.001012 
Batch[5970] - loss: 0.000994 
Batch[5971] - loss: 0.000571 
Batch[5972] - loss: 0.000781 
Batch[5973] - loss: 0.000443 
Batch[5974] - loss: 0.000783 
Batch[5975] - loss: 0.001889 
Batch[5976] - loss: 0.000708 
Batch[5977] - loss: 0.000973 
Batch[5978] - loss: 0.000769 
Batch[5979] - loss: 0.000351 
Batch[5980] - loss: 0.000743 
Batch[5981] - loss: 0.000768 
Batch[5982] - loss: 0.000834 
Batch[5983] - loss: 0.000820 
Batch[5984] - loss: 0.000428 
Batch[5985] - loss: 0.000516 
Batch[5986] - loss: 0.000480 
Batch[5987] - loss: 0.000516 
Batch[5988] - loss: 0.000420 
Batch[5989] - loss: 0.000992 
Batch[5990] - loss: 0.000661 
Batch[5991] - loss: 0.000472 
Batch[5992] - loss: 0.000537 
Batch[5993] - loss: 0.000635 
Batch[5994] - loss: 0.000571 
Batch[5995] - loss: 0.001033 
Batch[5996] - loss: 0.000796 
Batch[5997] - loss: 0.000396 
Batch[5998] - loss: 0.000474 
Batch[5999] - loss: 0.000522 
Batch[6000] - loss: 0.000821 

Evaluation - loss: 0.000068 pearson: 0.5582 

early stop by 1500 steps.
Batch[6001] - loss: 0.000442 
Batch[6002] - loss: 0.000736 
Batch[6003] - loss: 0.000584 
Batch[6004] - loss: 0.000778 
Batch[6005] - loss: 0.000549 
Batch[6006] - loss: 0.000432 
Batch[6007] - loss: 0.000866 
Batch[6008] - loss: 0.000420 
Batch[6009] - loss: 0.000594 
Batch[6010] - loss: 0.000665 
Batch[6011] - loss: 0.000642 
Batch[6012] - loss: 0.000605 
Batch[6013] - loss: 0.000422 
Batch[6014] - loss: 0.000532 
Batch[6015] - loss: 0.000486 
Batch[6016] - loss: 0.000718 
Batch[6017] - loss: 0.000545 
Batch[6018] - loss: 0.000570 
Batch[6019] - loss: 0.000666 
Batch[6020] - loss: 0.001107 
Batch[6021] - loss: 0.000674 
Batch[6022] - loss: 0.000560 
Batch[6023] - loss: 0.001380 
Batch[6024] - loss: 0.000693 
Batch[6025] - loss: 0.000661 
Batch[6026] - loss: 0.000607 
Batch[6027] - loss: 0.000828 
Batch[6028] - loss: 0.000801 
Batch[6029] - loss: 0.000513 
Batch[6030] - loss: 0.000787 
Batch[6031] - loss: 0.000845 
Batch[6032] - loss: 0.000802 
Batch[6033] - loss: 0.000704 
Batch[6034] - loss: 0.000529 
Batch[6035] - loss: 0.000691 
Batch[6036] - loss: 0.000555 
Batch[6037] - loss: 0.000282 
Batch[6038] - loss: 0.000712 
Batch[6039] - loss: 0.000571 
Batch[6040] - loss: 0.000590 
Batch[6041] - loss: 0.000536 
Batch[6042] - loss: 0.000724 
Batch[6043] - loss: 0.000553 
Batch[6044] - loss: 0.000444 
Batch[6045] - loss: 0.000842 
Batch[6046] - loss: 0.000696 
Batch[6047] - loss: 0.000487 
Batch[6048] - loss: 0.000575 
Batch[6049] - loss: 0.001001 
Batch[6050] - loss: 0.000420 
Batch[6051] - loss: 0.000746 
Batch[6052] - loss: 0.000545 
Batch[6053] - loss: 0.000680 
Batch[6054] - loss: 0.000520 
Batch[6055] - loss: 0.000528 
Batch[6056] - loss: 0.001566 
Batch[6057] - loss: 0.000438 
Batch[6058] - loss: 0.000840 
Batch[6059] - loss: 0.000619 
Batch[6060] - loss: 0.000917 
Batch[6061] - loss: 0.000636 
Batch[6062] - loss: 0.000536 
Batch[6063] - loss: 0.000736 
Batch[6064] - loss: 0.000726 
Batch[6065] - loss: 0.000588 
Batch[6066] - loss: 0.000491 
Batch[6067] - loss: 0.000489 
Batch[6068] - loss: 0.000701 
Batch[6069] - loss: 0.000601 
Batch[6070] - loss: 0.000468 
Batch[6071] - loss: 0.000401 
Batch[6072] - loss: 0.000293 
Batch[6073] - loss: 0.000568 
Batch[6074] - loss: 0.000457 
Batch[6075] - loss: 0.000476 
Batch[6076] - loss: 0.000291 
Batch[6077] - loss: 0.000488 
Batch[6078] - loss: 0.000613 
Batch[6079] - loss: 0.000526 
Batch[6080] - loss: 0.000764 
Batch[6081] - loss: 0.001008 
Batch[6082] - loss: 0.000628 
Batch[6083] - loss: 0.000931 
Batch[6084] - loss: 0.000389 
Batch[6085] - loss: 0.000847 
Batch[6086] - loss: 0.000626 
Batch[6087] - loss: 0.000509 
Batch[6088] - loss: 0.000916 
Batch[6089] - loss: 0.000660 
Batch[6090] - loss: 0.000604 
Batch[6091] - loss: 0.000583 
Batch[6092] - loss: 0.000614 
Batch[6093] - loss: 0.000705 
Batch[6094] - loss: 0.000660 
Batch[6095] - loss: 0.000461 
Batch[6096] - loss: 0.000606 
Batch[6097] - loss: 0.000361 
Batch[6098] - loss: 0.000798 
Batch[6099] - loss: 0.000512 
Batch[6100] - loss: 0.000720 

Evaluation - loss: 0.000068 pearson: 0.5563 

early stop by 1500 steps.
Batch[6101] - loss: 0.001150 
Batch[6102] - loss: 0.000528 
Batch[6103] - loss: 0.000452 
Batch[6104] - loss: 0.000449 
Batch[6105] - loss: 0.000765 
Batch[6106] - loss: 0.000531 
Batch[6107] - loss: 0.000978 
Batch[6108] - loss: 0.000913 
Batch[6109] - loss: 0.000540 
Batch[6110] - loss: 0.000555 
Batch[6111] - loss: 0.000674 
Batch[6112] - loss: 0.000636 
Batch[6113] - loss: 0.000729 
Batch[6114] - loss: 0.000526 
Batch[6115] - loss: 0.000571 
Batch[6116] - loss: 0.000593 
Batch[6117] - loss: 0.000652 
Batch[6118] - loss: 0.000510 
Batch[6119] - loss: 0.000709 
Batch[6120] - loss: 0.000666 
Batch[6121] - loss: 0.000308 
Batch[6122] - loss: 0.000436 
Batch[6123] - loss: 0.000618 
Batch[6124] - loss: 0.000960 
Batch[6125] - loss: 0.000713 
Batch[6126] - loss: 0.000637 
Batch[6127] - loss: 0.000703 
Batch[6128] - loss: 0.000464 
Batch[6129] - loss: 0.000610 
Batch[6130] - loss: 0.000613 
Batch[6131] - loss: 0.000514 
Batch[6132] - loss: 0.000489 
Batch[6133] - loss: 0.000683 
Batch[6134] - loss: 0.000914 
Batch[6135] - loss: 0.000615 
Batch[6136] - loss: 0.000429 
Batch[6137] - loss: 0.000304 
Batch[6138] - loss: 0.000621 
Batch[6139] - loss: 0.001034 
Batch[6140] - loss: 0.000328 
Batch[6141] - loss: 0.000315 
Batch[6142] - loss: 0.000517 
Batch[6143] - loss: 0.001085 
Batch[6144] - loss: 0.000526 
Batch[6145] - loss: 0.000747 
Batch[6146] - loss: 0.000484 
Batch[6147] - loss: 0.000742 
Batch[6148] - loss: 0.000463 
Batch[6149] - loss: 0.000499 
Batch[6150] - loss: 0.000806 
Batch[6151] - loss: 0.000290 
Batch[6152] - loss: 0.000560 
Batch[6153] - loss: 0.000449 
Batch[6154] - loss: 0.000559 
Batch[6155] - loss: 0.000368 
Batch[6156] - loss: 0.001046 
Batch[6157] - loss: 0.001257 
Batch[6158] - loss: 0.000538 
Batch[6159] - loss: 0.000958 
Batch[6160] - loss: 0.000981 
Batch[6161] - loss: 0.000734 
Batch[6162] - loss: 0.000660 
Batch[6163] - loss: 0.000868 
Batch[6164] - loss: 0.000552 
Batch[6165] - loss: 0.000654 
Batch[6166] - loss: 0.000632 
Batch[6167] - loss: 0.000751 
Batch[6168] - loss: 0.000968 
Batch[6169] - loss: 0.000518 
Batch[6170] - loss: 0.000441 
Batch[6171] - loss: 0.000453 
Batch[6172] - loss: 0.000377 
Batch[6173] - loss: 0.000503 
Batch[6174] - loss: 0.000689 
Batch[6175] - loss: 0.000765 
Batch[6176] - loss: 0.000710 
Batch[6177] - loss: 0.000355 
Batch[6178] - loss: 0.000712 
Batch[6179] - loss: 0.001153 
Batch[6180] - loss: 0.000686 
Batch[6181] - loss: 0.000731 
Batch[6182] - loss: 0.000608 
Batch[6183] - loss: 0.000887 
Batch[6184] - loss: 0.000515 
Batch[6185] - loss: 0.000414 
Batch[6186] - loss: 0.000413 
Batch[6187] - loss: 0.000869 
Batch[6188] - loss: 0.000909 
Batch[6189] - loss: 0.000715 
Batch[6190] - loss: 0.000822 
Batch[6191] - loss: 0.000580 
Batch[6192] - loss: 0.000511 
Batch[6193] - loss: 0.000618 
Batch[6194] - loss: 0.000556 
Batch[6195] - loss: 0.000592 
Batch[6196] - loss: 0.000445 
Batch[6197] - loss: 0.000523 
Batch[6198] - loss: 0.000469 
Batch[6199] - loss: 0.000588 
Batch[6200] - loss: 0.000461 

Evaluation - loss: 0.000068 pearson: 0.5568 

early stop by 1500 steps.
Batch[6201] - loss: 0.000455 
Batch[6202] - loss: 0.000462 
Batch[6203] - loss: 0.000388 
Batch[6204] - loss: 0.000493 
Batch[6205] - loss: 0.000641 
Batch[6206] - loss: 0.000610 
Batch[6207] - loss: 0.000956 
Batch[6208] - loss: 0.000391 
Batch[6209] - loss: 0.000520 
Batch[6210] - loss: 0.000513 
Batch[6211] - loss: 0.000592 
Batch[6212] - loss: 0.000764 
Batch[6213] - loss: 0.000816 
Batch[6214] - loss: 0.000667 
Batch[6215] - loss: 0.000364 
Batch[6216] - loss: 0.000558 
Batch[6217] - loss: 0.000437 
Batch[6218] - loss: 0.000870 
Batch[6219] - loss: 0.000571 
Batch[6220] - loss: 0.000547 
Batch[6221] - loss: 0.000731 
Batch[6222] - loss: 0.000473 
Batch[6223] - loss: 0.001300 
Batch[6224] - loss: 0.000690 
Batch[6225] - loss: 0.000467 
Batch[6226] - loss: 0.000775 
Batch[6227] - loss: 0.000563 
Batch[6228] - loss: 0.000461 
Batch[6229] - loss: 0.000494 
Batch[6230] - loss: 0.000750 
Batch[6231] - loss: 0.000457 
Batch[6232] - loss: 0.000635 
Batch[6233] - loss: 0.000560 
Batch[6234] - loss: 0.001267 
Batch[6235] - loss: 0.000399 
Batch[6236] - loss: 0.000740 
Batch[6237] - loss: 0.000729 
Batch[6238] - loss: 0.000675 
Batch[6239] - loss: 0.000552 
Batch[6240] - loss: 0.000449 
Batch[6241] - loss: 0.000465 
Batch[6242] - loss: 0.000701 
Batch[6243] - loss: 0.000712 
Batch[6244] - loss: 0.000879 
Batch[6245] - loss: 0.000617 
Batch[6246] - loss: 0.000538 
Batch[6247] - loss: 0.000526 
Batch[6248] - loss: 0.000705 
Batch[6249] - loss: 0.000667 
Batch[6250] - loss: 0.000660 
Batch[6251] - loss: 0.000427 
Batch[6252] - loss: 0.000517 
Batch[6253] - loss: 0.000530 
Batch[6254] - loss: 0.000590 
Batch[6255] - loss: 0.000687 
Batch[6256] - loss: 0.000459 
Batch[6257] - loss: 0.000508 
Batch[6258] - loss: 0.000876 
Batch[6259] - loss: 0.000742 
Batch[6260] - loss: 0.000364 
Batch[6261] - loss: 0.000514 
Batch[6262] - loss: 0.000451 
Batch[6263] - loss: 0.000766 
Batch[6264] - loss: 0.000556 
Batch[6265] - loss: 0.000808 
Batch[6266] - loss: 0.000525 
Batch[6267] - loss: 0.000488 
Batch[6268] - loss: 0.000826 
Batch[6269] - loss: 0.000559 
Batch[6270] - loss: 0.000505 
Batch[6271] - loss: 0.000634 
Batch[6272] - loss: 0.000677 
Batch[6273] - loss: 0.000396 
Batch[6274] - loss: 0.000887 
Batch[6275] - loss: 0.000766 
Batch[6276] - loss: 0.000311 
Batch[6277] - loss: 0.000400 
Batch[6278] - loss: 0.000946 
Batch[6279] - loss: 0.000894 
Batch[6280] - loss: 0.000782 
Batch[6281] - loss: 0.000543 
Batch[6282] - loss: 0.000866 
Batch[6283] - loss: 0.000558 
Batch[6284] - loss: 0.000407 
Batch[6285] - loss: 0.000657 
Batch[6286] - loss: 0.000481 
Batch[6287] - loss: 0.000465 
Batch[6288] - loss: 0.000558 
Batch[6289] - loss: 0.000614 
Batch[6290] - loss: 0.000485 
Batch[6291] - loss: 0.000600 
Batch[6292] - loss: 0.000516 
Batch[6293] - loss: 0.000416 
Batch[6294] - loss: 0.000388 
Batch[6295] - loss: 0.000837 
Batch[6296] - loss: 0.000503 
Batch[6297] - loss: 0.000621 
Batch[6298] - loss: 0.000447 
Batch[6299] - loss: 0.000448 
Batch[6300] - loss: 0.000516 

Evaluation - loss: 0.000068 pearson: 0.5544 

early stop by 1500 steps.
Batch[6301] - loss: 0.000659 
Batch[6302] - loss: 0.001061 
Batch[6303] - loss: 0.001248 
Batch[6304] - loss: 0.000812 
Batch[6305] - loss: 0.000331 
Batch[6306] - loss: 0.000617 
Batch[6307] - loss: 0.000378 
Batch[6308] - loss: 0.000586 
Batch[6309] - loss: 0.001319 
Batch[6310] - loss: 0.000620 
Batch[6311] - loss: 0.000464 
Batch[6312] - loss: 0.000521 
Batch[6313] - loss: 0.000569 
Batch[6314] - loss: 0.000475 
Batch[6315] - loss: 0.000653 
Batch[6316] - loss: 0.000394 
Batch[6317] - loss: 0.001658 
Batch[6318] - loss: 0.000422 
Batch[6319] - loss: 0.000372 
Batch[6320] - loss: 0.000491 
Batch[6321] - loss: 0.000807 
Batch[6322] - loss: 0.000564 
Batch[6323] - loss: 0.000412 
Batch[6324] - loss: 0.000637 
Batch[6325] - loss: 0.000679 
Batch[6326] - loss: 0.000574 
Batch[6327] - loss: 0.000458 
Batch[6328] - loss: 0.000565 
Batch[6329] - loss: 0.000598 
Batch[6330] - loss: 0.000569 
Batch[6331] - loss: 0.000460 
Batch[6332] - loss: 0.000878 
Batch[6333] - loss: 0.000526 
Batch[6334] - loss: 0.000823 
Batch[6335] - loss: 0.000528 
Batch[6336] - loss: 0.000503 
Batch[6337] - loss: 0.000562 
Batch[6338] - loss: 0.000483 
Batch[6339] - loss: 0.000469 
Batch[6340] - loss: 0.000545 
Batch[6341] - loss: 0.001019 
Batch[6342] - loss: 0.000619 
Batch[6343] - loss: 0.000750 
Batch[6344] - loss: 0.000619 
Batch[6345] - loss: 0.000989 
Batch[6346] - loss: 0.000742 
Batch[6347] - loss: 0.000461 
Batch[6348] - loss: 0.000607 
Batch[6349] - loss: 0.000686 
Batch[6350] - loss: 0.000652 
Batch[6351] - loss: 0.000381 
Batch[6352] - loss: 0.000719 
Batch[6353] - loss: 0.000410 
Batch[6354] - loss: 0.000493 
Batch[6355] - loss: 0.000757 
Batch[6356] - loss: 0.000735 
Batch[6357] - loss: 0.000572 
Batch[6358] - loss: 0.000659 
Batch[6359] - loss: 0.001112 
Batch[6360] - loss: 0.000541 
Batch[6361] - loss: 0.000410 
Batch[6362] - loss: 0.000882 
Batch[6363] - loss: 0.000705 
Batch[6364] - loss: 0.000754 
Batch[6365] - loss: 0.000415 
Batch[6366] - loss: 0.000527 
Batch[6367] - loss: 0.000854 
Batch[6368] - loss: 0.000394 
Batch[6369] - loss: 0.000508 
Batch[6370] - loss: 0.000424 
Batch[6371] - loss: 0.000376 
Batch[6372] - loss: 0.000426 
Batch[6373] - loss: 0.000728 
Batch[6374] - loss: 0.000728 
Batch[6375] - loss: 0.000405 
Batch[6376] - loss: 0.000559 
Batch[6377] - loss: 0.001182 
Batch[6378] - loss: 0.000855 
Batch[6379] - loss: 0.000313 
Batch[6380] - loss: 0.000457 
Batch[6381] - loss: 0.000609 
Batch[6382] - loss: 0.000654 
Batch[6383] - loss: 0.000542 
Batch[6384] - loss: 0.000352 
Batch[6385] - loss: 0.000528 
Batch[6386] - loss: 0.000995 
Batch[6387] - loss: 0.000576 
Batch[6388] - loss: 0.000509 
Batch[6389] - loss: 0.000521 
Batch[6390] - loss: 0.000557 
Batch[6391] - loss: 0.000512 
Batch[6392] - loss: 0.000899 
Batch[6393] - loss: 0.000534 
Batch[6394] - loss: 0.000741 
Batch[6395] - loss: 0.000677 
Batch[6396] - loss: 0.000587 
Batch[6397] - loss: 0.000407 
Batch[6398] - loss: 0.000719 
Batch[6399] - loss: 0.001163 
Batch[6400] - loss: 0.000494 

Evaluation - loss: 0.000069 pearson: 0.5525 

early stop by 1500 steps.
Batch[6401] - loss: 0.000435 
Batch[6402] - loss: 0.000765 
Batch[6403] - loss: 0.000508 
Batch[6404] - loss: 0.000765 
Batch[6405] - loss: 0.000523 
Batch[6406] - loss: 0.000466 
Batch[6407] - loss: 0.000467 
Batch[6408] - loss: 0.001018 
Batch[6409] - loss: 0.000582 
Batch[6410] - loss: 0.000391 
Batch[6411] - loss: 0.000438 
Batch[6412] - loss: 0.000492 
Batch[6413] - loss: 0.000924 
Batch[6414] - loss: 0.000387 
Batch[6415] - loss: 0.000721 
Batch[6416] - loss: 0.000555 
Batch[6417] - loss: 0.000310 
Batch[6418] - loss: 0.000404 
Batch[6419] - loss: 0.000487 
Batch[6420] - loss: 0.000430 
Batch[6421] - loss: 0.001142 
Batch[6422] - loss: 0.000469 
Batch[6423] - loss: 0.000583 
Batch[6424] - loss: 0.000453 
Batch[6425] - loss: 0.000851 
Batch[6426] - loss: 0.000732 
Batch[6427] - loss: 0.000770 
Batch[6428] - loss: 0.000786 
Batch[6429] - loss: 0.000613 
Batch[6430] - loss: 0.000704 
Batch[6431] - loss: 0.000826 
Batch[6432] - loss: 0.000301 
Batch[6433] - loss: 0.000446 
Batch[6434] - loss: 0.000714 
Batch[6435] - loss: 0.000632 
Batch[6436] - loss: 0.000489 
Batch[6437] - loss: 0.000680 
Batch[6438] - loss: 0.000685 
Batch[6439] - loss: 0.000702 
Batch[6440] - loss: 0.000539 
Batch[6441] - loss: 0.000418 
Batch[6442] - loss: 0.000415 
Batch[6443] - loss: 0.000505 
Batch[6444] - loss: 0.000591 
Batch[6445] - loss: 0.000608 
Batch[6446] - loss: 0.000570 
Batch[6447] - loss: 0.000461 
Batch[6448] - loss: 0.000425 
Batch[6449] - loss: 0.000648 
Batch[6450] - loss: 0.000628 
Batch[6451] - loss: 0.000442 
Batch[6452] - loss: 0.001521 
Batch[6453] - loss: 0.000597 
Batch[6454] - loss: 0.000862 
Batch[6455] - loss: 0.000512 
Batch[6456] - loss: 0.000486 
Batch[6457] - loss: 0.000748 
Batch[6458] - loss: 0.000494 
Batch[6459] - loss: 0.000544 
Batch[6460] - loss: 0.000436 
Batch[6461] - loss: 0.000754 
Batch[6462] - loss: 0.000481 
Batch[6463] - loss: 0.000585 
Batch[6464] - loss: 0.001087 
Batch[6465] - loss: 0.000616 
Batch[6466] - loss: 0.000617 
Batch[6467] - loss: 0.000749 
Batch[6468] - loss: 0.000950 
Batch[6469] - loss: 0.000471 
Batch[6470] - loss: 0.000685 
Batch[6471] - loss: 0.000434 
Batch[6472] - loss: 0.000329 
Batch[6473] - loss: 0.000584 
Batch[6474] - loss: 0.000483 
Batch[6475] - loss: 0.000846 
Batch[6476] - loss: 0.000714 
Batch[6477] - loss: 0.000628 
Batch[6478] - loss: 0.000617 
Batch[6479] - loss: 0.000508 
Batch[6480] - loss: 0.000536 
Batch[6481] - loss: 0.000669 
Batch[6482] - loss: 0.000564 
Batch[6483] - loss: 0.000499 
Batch[6484] - loss: 0.000625 
Batch[6485] - loss: 0.000508 
Batch[6486] - loss: 0.000355 
Batch[6487] - loss: 0.000884 
Batch[6488] - loss: 0.000572 
Batch[6489] - loss: 0.000312 
Batch[6490] - loss: 0.000354 
Batch[6491] - loss: 0.000668 
Batch[6492] - loss: 0.000426 
Batch[6493] - loss: 0.000827 
Batch[6494] - loss: 0.000621 
Batch[6495] - loss: 0.000637 
Batch[6496] - loss: 0.000695 
Batch[6497] - loss: 0.000455 
Batch[6498] - loss: 0.000355 
Batch[6499] - loss: 0.001266 
Batch[6500] - loss: 0.000424 

Evaluation - loss: 0.000068 pearson: 0.5578 

early stop by 1500 steps.
Batch[6501] - loss: 0.000337 
Batch[6502] - loss: 0.000631 
Batch[6503] - loss: 0.000408 
Batch[6504] - loss: 0.000645 
Batch[6505] - loss: 0.000837 
Batch[6506] - loss: 0.000948 
Batch[6507] - loss: 0.000444 
Batch[6508] - loss: 0.000721 
Batch[6509] - loss: 0.000774 
Batch[6510] - loss: 0.000444 
Batch[6511] - loss: 0.001202 
Batch[6512] - loss: 0.000358 
Batch[6513] - loss: 0.000960 
Batch[6514] - loss: 0.000475 
Batch[6515] - loss: 0.001238 
Batch[6516] - loss: 0.001000 
Batch[6517] - loss: 0.000675 
Batch[6518] - loss: 0.000510 
Batch[6519] - loss: 0.000684 
Batch[6520] - loss: 0.000667 
Batch[6521] - loss: 0.000402 
Batch[6522] - loss: 0.000672 
Batch[6523] - loss: 0.001001 
Batch[6524] - loss: 0.000483 
Batch[6525] - loss: 0.000570 
Batch[6526] - loss: 0.000816 
Batch[6527] - loss: 0.000831 
Batch[6528] - loss: 0.000452 
Batch[6529] - loss: 0.000580 
Batch[6530] - loss: 0.000688 
Batch[6531] - loss: 0.000495 
Batch[6532] - loss: 0.000707 
Batch[6533] - loss: 0.000793 
Batch[6534] - loss: 0.000419 
Batch[6535] - loss: 0.000293 
Batch[6536] - loss: 0.000576 
Batch[6537] - loss: 0.000559 
Batch[6538] - loss: 0.000429 
Batch[6539] - loss: 0.000397 
Batch[6540] - loss: 0.000649 
Batch[6541] - loss: 0.000697 
Batch[6542] - loss: 0.001083 
Batch[6543] - loss: 0.000368 
Batch[6544] - loss: 0.000566 
Batch[6545] - loss: 0.000590 
Batch[6546] - loss: 0.000673 
Batch[6547] - loss: 0.000490 
Batch[6548] - loss: 0.000939 
Batch[6549] - loss: 0.000493 
Batch[6550] - loss: 0.000474 
Batch[6551] - loss: 0.000507 
Batch[6552] - loss: 0.000445 
Batch[6553] - loss: 0.000855 
Batch[6554] - loss: 0.000687 
Batch[6555] - loss: 0.000737 
Batch[6556] - loss: 0.000639 
Batch[6557] - loss: 0.000552 
Batch[6558] - loss: 0.000809 
Batch[6559] - loss: 0.000859 
Batch[6560] - loss: 0.000667 
Batch[6561] - loss: 0.000466 
Batch[6562] - loss: 0.000664 
Batch[6563] - loss: 0.000843 
Batch[6564] - loss: 0.000692 
Batch[6565] - loss: 0.000484 
Batch[6566] - loss: 0.000753 
Batch[6567] - loss: 0.000537 
Batch[6568] - loss: 0.000447 
Batch[6569] - loss: 0.000542 
Batch[6570] - loss: 0.000428 
Batch[6571] - loss: 0.000506 
Batch[6572] - loss: 0.000493 
Batch[6573] - loss: 0.000577 
Batch[6574] - loss: 0.000386 
Batch[6575] - loss: 0.000912 
Batch[6576] - loss: 0.000742 
Batch[6577] - loss: 0.000378 
Batch[6578] - loss: 0.000851 
Batch[6579] - loss: 0.000633 
Batch[6580] - loss: 0.000916 
Batch[6581] - loss: 0.001301 
Batch[6582] - loss: 0.000777 
Batch[6583] - loss: 0.000783 
Batch[6584] - loss: 0.000465 
Batch[6585] - loss: 0.000776 
Batch[6586] - loss: 0.000759 
Batch[6587] - loss: 0.000493 
Batch[6588] - loss: 0.000416 
Batch[6589] - loss: 0.000621 
Batch[6590] - loss: 0.000392 
Batch[6591] - loss: 0.000668 
Batch[6592] - loss: 0.000740 
Batch[6593] - loss: 0.000452 
Batch[6594] - loss: 0.000355 
Batch[6595] - loss: 0.000745 
Batch[6596] - loss: 0.000495 
Batch[6597] - loss: 0.000570 
Batch[6598] - loss: 0.000852 
Batch[6599] - loss: 0.000553 
Batch[6600] - loss: 0.000495 

Evaluation - loss: 0.000069 pearson: 0.5557 

early stop by 1500 steps.
Batch[6601] - loss: 0.000764 
Batch[6602] - loss: 0.000804 
Batch[6603] - loss: 0.000223 
Batch[6604] - loss: 0.000386 
Batch[6605] - loss: 0.000809 
Batch[6606] - loss: 0.000746 
Batch[6607] - loss: 0.000615 
Batch[6608] - loss: 0.000752 
Batch[6609] - loss: 0.000738 
Batch[6610] - loss: 0.000431 
Batch[6611] - loss: 0.000597 
Batch[6612] - loss: 0.000610 
Batch[6613] - loss: 0.000862 
Batch[6614] - loss: 0.000660 
Batch[6615] - loss: 0.000588 
Batch[6616] - loss: 0.000393 
Batch[6617] - loss: 0.000410 
Batch[6618] - loss: 0.000689 
Batch[6619] - loss: 0.000778 
Batch[6620] - loss: 0.000553 
Batch[6621] - loss: 0.000499 
Batch[6622] - loss: 0.000382 
Batch[6623] - loss: 0.000639 
Batch[6624] - loss: 0.000810 
Batch[6625] - loss: 0.000681 
Batch[6626] - loss: 0.000630 
Batch[6627] - loss: 0.000400 
Batch[6628] - loss: 0.000860 
Batch[6629] - loss: 0.000575 
Batch[6630] - loss: 0.000670 
Batch[6631] - loss: 0.000807 
Batch[6632] - loss: 0.000877 
Batch[6633] - loss: 0.000511 
Batch[6634] - loss: 0.000621 
Batch[6635] - loss: 0.000694 
Batch[6636] - loss: 0.000280 
Batch[6637] - loss: 0.001598 
Batch[6638] - loss: 0.000315 
Batch[6639] - loss: 0.000939 
Batch[6640] - loss: 0.000764 
Batch[6641] - loss: 0.000869 
Batch[6642] - loss: 0.000636 
Batch[6643] - loss: 0.001204 
Batch[6644] - loss: 0.000804 
Batch[6645] - loss: 0.000872 
Batch[6646] - loss: 0.001558 
Batch[6647] - loss: 0.000886 
Batch[6648] - loss: 0.001071 
Batch[6649] - loss: 0.000454 
Batch[6650] - loss: 0.000495 
Batch[6651] - loss: 0.000372 
Batch[6652] - loss: 0.000313 
Batch[6653] - loss: 0.000830 
Batch[6654] - loss: 0.000602 
Batch[6655] - loss: 0.000467 
Batch[6656] - loss: 0.000393 
Batch[6657] - loss: 0.000441 
Batch[6658] - loss: 0.000484 
Batch[6659] - loss: 0.000732 
Batch[6660] - loss: 0.000543 
Batch[6661] - loss: 0.000392 
Batch[6662] - loss: 0.000668 
Batch[6663] - loss: 0.000722 
Batch[6664] - loss: 0.000438 
Batch[6665] - loss: 0.000317 
Batch[6666] - loss: 0.000275 
Batch[6667] - loss: 0.000771 
Batch[6668] - loss: 0.000813 
Batch[6669] - loss: 0.000680 
Batch[6670] - loss: 0.000632 
Batch[6671] - loss: 0.000301 
Batch[6672] - loss: 0.000414 
Batch[6673] - loss: 0.000316 
Batch[6674] - loss: 0.000639 
Batch[6675] - loss: 0.000740 
Batch[6676] - loss: 0.000412 
Batch[6677] - loss: 0.000850 
Batch[6678] - loss: 0.000492 
Batch[6679] - loss: 0.000880 
Batch[6680] - loss: 0.000645 
Batch[6681] - loss: 0.000414 
Batch[6682] - loss: 0.000868 
Batch[6683] - loss: 0.000691 
Batch[6684] - loss: 0.000552 
Batch[6685] - loss: 0.000622 
Batch[6686] - loss: 0.000711 
Batch[6687] - loss: 0.000613 
Batch[6688] - loss: 0.000604 
Batch[6689] - loss: 0.000548 
Batch[6690] - loss: 0.000673 
Batch[6691] - loss: 0.000809 
Batch[6692] - loss: 0.000780 
Batch[6693] - loss: 0.000279 
Batch[6694] - loss: 0.001047 
Batch[6695] - loss: 0.000441 
Batch[6696] - loss: 0.000331 
Batch[6697] - loss: 0.000403 
Batch[6698] - loss: 0.000368 
Batch[6699] - loss: 0.000332 
Batch[6700] - loss: 0.000740 

Evaluation - loss: 0.000069 pearson: 0.5533 

early stop by 1500 steps.
Batch[6701] - loss: 0.000373 
Batch[6702] - loss: 0.000647 
Batch[6703] - loss: 0.000844 
Batch[6704] - loss: 0.000412 
Batch[6705] - loss: 0.000799 
Batch[6706] - loss: 0.000457 
Batch[6707] - loss: 0.000423 
Batch[6708] - loss: 0.000512 
Batch[6709] - loss: 0.000823 
Batch[6710] - loss: 0.000711 
Batch[6711] - loss: 0.000539 
Batch[6712] - loss: 0.000686 
Batch[6713] - loss: 0.000552 
Batch[6714] - loss: 0.000619 
Batch[6715] - loss: 0.000484 
Batch[6716] - loss: 0.000618 
Batch[6717] - loss: 0.000745 
Batch[6718] - loss: 0.000460 
Batch[6719] - loss: 0.000609 
Batch[6720] - loss: 0.000769 
Batch[6721] - loss: 0.000507 
Batch[6722] - loss: 0.000749 
Batch[6723] - loss: 0.000510 
Batch[6724] - loss: 0.000690 
Batch[6725] - loss: 0.000456 
Batch[6726] - loss: 0.000673 
Batch[6727] - loss: 0.000844 
Batch[6728] - loss: 0.000813 
Batch[6729] - loss: 0.000642 
Batch[6730] - loss: 0.000867 
Batch[6731] - loss: 0.000837 
Batch[6732] - loss: 0.000616 
Batch[6733] - loss: 0.000613 
Batch[6734] - loss: 0.000658 
Batch[6735] - loss: 0.000615 
Batch[6736] - loss: 0.000688 
Batch[6737] - loss: 0.000546 
Batch[6738] - loss: 0.000419 
Batch[6739] - loss: 0.000345 
Batch[6740] - loss: 0.000442 
Batch[6741] - loss: 0.001025 
Batch[6742] - loss: 0.000375 
Batch[6743] - loss: 0.000487 
Batch[6744] - loss: 0.000362 
Batch[6745] - loss: 0.000476 
Batch[6746] - loss: 0.000597 
Batch[6747] - loss: 0.000434 
Batch[6748] - loss: 0.000472 
Batch[6749] - loss: 0.000981 
Batch[6750] - loss: 0.000487 
Batch[6751] - loss: 0.000619 
Batch[6752] - loss: 0.000484 
Batch[6753] - loss: 0.000483 
Batch[6754] - loss: 0.000704 
Batch[6755] - loss: 0.000580 
Batch[6756] - loss: 0.001051 
Batch[6757] - loss: 0.000634 
Batch[6758] - loss: 0.000756 
Batch[6759] - loss: 0.000506 
Batch[6760] - loss: 0.000481 
Batch[6761] - loss: 0.000601 
Batch[6762] - loss: 0.000824 
Batch[6763] - loss: 0.000534 
Batch[6764] - loss: 0.001270 
Batch[6765] - loss: 0.000866 
Batch[6766] - loss: 0.000460 
Batch[6767] - loss: 0.000677 
Batch[6768] - loss: 0.000753 
Batch[6769] - loss: 0.000542 
Batch[6770] - loss: 0.000350 
Batch[6771] - loss: 0.000846 
Batch[6772] - loss: 0.000573 
Batch[6773] - loss: 0.000521 
Batch[6774] - loss: 0.001292 
Batch[6775] - loss: 0.000438 
Batch[6776] - loss: 0.000926 
Batch[6777] - loss: 0.000938 
Batch[6778] - loss: 0.001036 
Batch[6779] - loss: 0.000440 
Batch[6780] - loss: 0.000606 
Batch[6781] - loss: 0.000817 
Batch[6782] - loss: 0.000346 
Batch[6783] - loss: 0.000587 
Batch[6784] - loss: 0.000586 
Batch[6785] - loss: 0.000818 
Batch[6786] - loss: 0.001053 
Batch[6787] - loss: 0.000777 
Batch[6788] - loss: 0.000380 
Batch[6789] - loss: 0.000740 
Batch[6790] - loss: 0.000858 
Batch[6791] - loss: 0.000506 
Batch[6792] - loss: 0.000702 
Batch[6793] - loss: 0.000335 
Batch[6794] - loss: 0.000825 
Batch[6795] - loss: 0.000448 
Batch[6796] - loss: 0.000601 
Batch[6797] - loss: 0.000585 
Batch[6798] - loss: 0.000549 
Batch[6799] - loss: 0.000549 
Batch[6800] - loss: 0.000545 

Evaluation - loss: 0.000069 pearson: 0.5532 

early stop by 1500 steps.
Batch[6801] - loss: 0.000485 
Batch[6802] - loss: 0.000727 
Batch[6803] - loss: 0.000438 
Batch[6804] - loss: 0.000392 
Batch[6805] - loss: 0.000764 
Batch[6806] - loss: 0.000552 
Batch[6807] - loss: 0.000517 
Batch[6808] - loss: 0.000252 
Batch[6809] - loss: 0.000342 
Batch[6810] - loss: 0.001144 
Batch[6811] - loss: 0.000445 
Batch[6812] - loss: 0.000350 
Batch[6813] - loss: 0.000698 
Batch[6814] - loss: 0.000586 
Batch[6815] - loss: 0.000456 
Batch[6816] - loss: 0.000414 
Batch[6817] - loss: 0.000665 
Batch[6818] - loss: 0.000399 
Batch[6819] - loss: 0.000396 
Batch[6820] - loss: 0.000411 
Batch[6821] - loss: 0.000518 
Batch[6822] - loss: 0.000643 
Batch[6823] - loss: 0.000561 
Batch[6824] - loss: 0.000237 
Batch[6825] - loss: 0.000462 
Batch[6826] - loss: 0.000753 
Batch[6827] - loss: 0.000435 
Batch[6828] - loss: 0.000363 
Batch[6829] - loss: 0.000696 
Batch[6830] - loss: 0.000828 
Batch[6831] - loss: 0.000525 
Batch[6832] - loss: 0.000515 
Batch[6833] - loss: 0.000898 
Batch[6834] - loss: 0.000604 
Batch[6835] - loss: 0.000399 
Batch[6836] - loss: 0.000436 
Batch[6837] - loss: 0.000393 
Batch[6838] - loss: 0.000485 
Batch[6839] - loss: 0.000292 
Batch[6840] - loss: 0.000714 
Batch[6841] - loss: 0.000956 
Batch[6842] - loss: 0.000654 
Batch[6843] - loss: 0.000515 
Batch[6844] - loss: 0.000291 
Batch[6845] - loss: 0.000506 
Batch[6846] - loss: 0.000679 
Batch[6847] - loss: 0.000722 
Batch[6848] - loss: 0.000369 
Batch[6849] - loss: 0.000303 
Batch[6850] - loss: 0.000633 
Batch[6851] - loss: 0.000966 
Batch[6852] - loss: 0.000453 
Batch[6853] - loss: 0.000788 
Batch[6854] - loss: 0.000347 
Batch[6855] - loss: 0.000937 
Batch[6856] - loss: 0.001265 
Batch[6857] - loss: 0.000609 
Batch[6858] - loss: 0.000473 
Batch[6859] - loss: 0.000648 
Batch[6860] - loss: 0.000800 
Batch[6861] - loss: 0.000556 
Batch[6862] - loss: 0.001160 
Batch[6863] - loss: 0.000523 
Batch[6864] - loss: 0.000609 
Batch[6865] - loss: 0.000803 
Batch[6866] - loss: 0.000291 
Batch[6867] - loss: 0.000388 
Batch[6868] - loss: 0.000358 
Batch[6869] - loss: 0.000592 
Batch[6870] - loss: 0.000536 
Batch[6871] - loss: 0.000620 
Batch[6872] - loss: 0.000599 
Batch[6873] - loss: 0.000679 
Batch[6874] - loss: 0.000478 
Batch[6875] - loss: 0.000586 
Batch[6876] - loss: 0.000616 
Batch[6877] - loss: 0.000547 
Batch[6878] - loss: 0.000570 
Batch[6879] - loss: 0.000283 
Batch[6880] - loss: 0.000499 
Batch[6881] - loss: 0.000419 
Batch[6882] - loss: 0.000639 
Batch[6883] - loss: 0.000598 
Batch[6884] - loss: 0.000698 
Batch[6885] - loss: 0.000640 
Batch[6886] - loss: 0.000941 
Batch[6887] - loss: 0.000828 
Batch[6888] - loss: 0.000509 
Batch[6889] - loss: 0.000546 
Batch[6890] - loss: 0.000285 
Batch[6891] - loss: 0.000520 
Batch[6892] - loss: 0.000407 
Batch[6893] - loss: 0.000680 
Batch[6894] - loss: 0.000515 
Batch[6895] - loss: 0.000827 
Batch[6896] - loss: 0.000568 
Batch[6897] - loss: 0.000582 
Batch[6898] - loss: 0.000575 
Batch[6899] - loss: 0.001476 
Batch[6900] - loss: 0.000628 

Evaluation - loss: 0.000068 pearson: 0.5588 

early stop by 1500 steps.
Batch[6901] - loss: 0.000597 
Batch[6902] - loss: 0.000510 
Batch[6903] - loss: 0.000359 
Batch[6904] - loss: 0.000687 
Batch[6905] - loss: 0.000434 
Batch[6906] - loss: 0.000542 
Batch[6907] - loss: 0.000443 
Batch[6908] - loss: 0.000430 
Batch[6909] - loss: 0.000376 
Batch[6910] - loss: 0.000985 
Batch[6911] - loss: 0.000344 
Batch[6912] - loss: 0.000297 
Batch[6913] - loss: 0.000546 
Batch[6914] - loss: 0.000662 
Batch[6915] - loss: 0.000403 
Batch[6916] - loss: 0.001049 
Batch[6917] - loss: 0.000697 
Batch[6918] - loss: 0.000577 
Batch[6919] - loss: 0.001070 
Batch[6920] - loss: 0.000594 
Batch[6921] - loss: 0.000644 
Batch[6922] - loss: 0.000700 
Batch[6923] - loss: 0.000584 
Batch[6924] - loss: 0.000559 
Batch[6925] - loss: 0.000659 
Batch[6926] - loss: 0.000268 
Batch[6927] - loss: 0.000618 
Batch[6928] - loss: 0.000435 
Batch[6929] - loss: 0.000895 
Batch[6930] - loss: 0.000678 
Batch[6931] - loss: 0.000425 
Batch[6932] - loss: 0.000962 
Batch[6933] - loss: 0.000895 
Batch[6934] - loss: 0.000414 
Batch[6935] - loss: 0.000666 
Batch[6936] - loss: 0.000413 
Batch[6937] - loss: 0.000287 
Batch[6938] - loss: 0.000366 
Batch[6939] - loss: 0.000873 
Batch[6940] - loss: 0.000859 
Batch[6941] - loss: 0.000486 
Batch[6942] - loss: 0.000662 
Batch[6943] - loss: 0.000388 
Batch[6944] - loss: 0.000757 
Batch[6945] - loss: 0.000612 
Batch[6946] - loss: 0.000688 
Batch[6947] - loss: 0.000198 
Batch[6948] - loss: 0.000482 
Batch[6949] - loss: 0.000707 
Batch[6950] - loss: 0.000367 
Batch[6951] - loss: 0.000611 
Batch[6952] - loss: 0.000956 
Batch[6953] - loss: 0.000475 
Batch[6954] - loss: 0.000545 
Batch[6955] - loss: 0.000613 
Batch[6956] - loss: 0.000821 
Batch[6957] - loss: 0.000959 
Batch[6958] - loss: 0.000491 
Batch[6959] - loss: 0.000504 
Batch[6960] - loss: 0.000677 
Batch[6961] - loss: 0.000659 
Batch[6962] - loss: 0.000409 
Batch[6963] - loss: 0.000663 
Batch[6964] - loss: 0.000920 
Batch[6965] - loss: 0.000504 
Batch[6966] - loss: 0.000606 
Batch[6967] - loss: 0.000455 
Batch[6968] - loss: 0.000469 
Batch[6969] - loss: 0.000415 
Batch[6970] - loss: 0.000943 
Batch[6971] - loss: 0.000295 
Batch[6972] - loss: 0.000481 
Batch[6973] - loss: 0.000735 
Batch[6974] - loss: 0.000711 
Batch[6975] - loss: 0.000720 
Batch[6976] - loss: 0.000572 
Batch[6977] - loss: 0.000491 
Batch[6978] - loss: 0.000462 
Batch[6979] - loss: 0.000389 
Batch[6980] - loss: 0.000591 
Batch[6981] - loss: 0.000701 
Batch[6982] - loss: 0.000469 
Batch[6983] - loss: 0.000860 
Batch[6984] - loss: 0.000638 
Batch[6985] - loss: 0.000681 
Batch[6986] - loss: 0.000657 
Batch[6987] - loss: 0.000487 
Batch[6988] - loss: 0.000716 
Batch[6989] - loss: 0.000482 
Batch[6990] - loss: 0.000674 
Batch[6991] - loss: 0.000723 
Batch[6992] - loss: 0.000402 
Batch[6993] - loss: 0.000851 
Batch[6994] - loss: 0.000880 
Batch[6995] - loss: 0.000595 
Batch[6996] - loss: 0.000763 
Batch[6997] - loss: 0.000900 
Batch[6998] - loss: 0.001111 
Batch[6999] - loss: 0.000599 
Batch[7000] - loss: 0.000524 

Evaluation - loss: 0.000068 pearson: 0.5565 

early stop by 1500 steps.
Batch[7001] - loss: 0.000589 
Batch[7002] - loss: 0.000536 
Batch[7003] - loss: 0.000910 
Batch[7004] - loss: 0.001018 
Batch[7005] - loss: 0.000582 
Batch[7006] - loss: 0.000585 
Batch[7007] - loss: 0.000653 
Batch[7008] - loss: 0.000254 
Batch[7009] - loss: 0.000521 
Batch[7010] - loss: 0.000744 
Batch[7011] - loss: 0.000632 
Batch[7012] - loss: 0.000935 
Batch[7013] - loss: 0.000626 
Batch[7014] - loss: 0.000494 
Batch[7015] - loss: 0.000500 
Batch[7016] - loss: 0.000412 
Batch[7017] - loss: 0.000402 
Batch[7018] - loss: 0.001228 
Batch[7019] - loss: 0.000320 
Batch[7020] - loss: 0.000375 
Batch[7021] - loss: 0.000415 
Batch[7022] - loss: 0.000446 
Batch[7023] - loss: 0.000436 
Batch[7024] - loss: 0.000484 
Batch[7025] - loss: 0.000674 
Batch[7026] - loss: 0.000508 
Batch[7027] - loss: 0.000383 
Batch[7028] - loss: 0.000834 
Batch[7029] - loss: 0.000429 
Batch[7030] - loss: 0.000780 
Batch[7031] - loss: 0.000610 
Batch[7032] - loss: 0.000553 
Batch[7033] - loss: 0.000470 
Batch[7034] - loss: 0.000985 
Batch[7035] - loss: 0.000715 
Batch[7036] - loss: 0.000910 
Batch[7037] - loss: 0.000606 
Batch[7038] - loss: 0.000386 
Batch[7039] - loss: 0.000511 
Batch[7040] - loss: 0.000447 
Batch[7041] - loss: 0.000579 
Batch[7042] - loss: 0.000738 
Batch[7043] - loss: 0.000468 
Batch[7044] - loss: 0.000385 
Batch[7045] - loss: 0.000313 
Batch[7046] - loss: 0.000678 
Batch[7047] - loss: 0.001353 
Batch[7048] - loss: 0.000825 
Batch[7049] - loss: 0.000757 
Batch[7050] - loss: 0.000642 
Batch[7051] - loss: 0.000471 
Batch[7052] - loss: 0.000942 
Batch[7053] - loss: 0.000335 
Batch[7054] - loss: 0.000651 
Batch[7055] - loss: 0.000403 
Batch[7056] - loss: 0.000667 
Batch[7057] - loss: 0.000854 
Batch[7058] - loss: 0.000702 
Batch[7059] - loss: 0.000549 
Batch[7060] - loss: 0.000893 
Batch[7061] - loss: 0.000724 
Batch[7062] - loss: 0.000327 
Batch[7063] - loss: 0.000688 
Batch[7064] - loss: 0.000364 
Batch[7065] - loss: 0.000549 
Batch[7066] - loss: 0.000817 
Batch[7067] - loss: 0.000541 
Batch[7068] - loss: 0.001056 
Batch[7069] - loss: 0.000723 
Batch[7070] - loss: 0.000576 
Batch[7071] - loss: 0.000467 
Batch[7072] - loss: 0.001271 
Batch[7073] - loss: 0.000531 
Batch[7074] - loss: 0.000437 
Batch[7075] - loss: 0.001116 
Batch[7076] - loss: 0.000553 
Batch[7077] - loss: 0.000461 
Batch[7078] - loss: 0.000755 
Batch[7079] - loss: 0.000805 
Batch[7080] - loss: 0.000536 
Batch[7081] - loss: 0.000656 
Batch[7082] - loss: 0.000514 
Batch[7083] - loss: 0.000657 
Batch[7084] - loss: 0.000719 
Batch[7085] - loss: 0.000407 
Batch[7086] - loss: 0.000594 
Batch[7087] - loss: 0.001252 
Batch[7088] - loss: 0.000683 
Batch[7089] - loss: 0.000404 
Batch[7090] - loss: 0.000329 
Batch[7091] - loss: 0.000467 
Batch[7092] - loss: 0.000723 
Batch[7093] - loss: 0.000585 
Batch[7094] - loss: 0.000579 
Batch[7095] - loss: 0.000825 
Batch[7096] - loss: 0.000429 
Batch[7097] - loss: 0.000517 
Batch[7098] - loss: 0.000668 
Batch[7099] - loss: 0.000725 
Batch[7100] - loss: 0.000993 

Evaluation - loss: 0.000069 pearson: 0.5543 

early stop by 1500 steps.
Batch[7101] - loss: 0.000531 
Batch[7102] - loss: 0.000635 
Batch[7103] - loss: 0.000460 
Batch[7104] - loss: 0.000347 
Batch[7105] - loss: 0.000534 
Batch[7106] - loss: 0.000872 
Batch[7107] - loss: 0.000345 
Batch[7108] - loss: 0.000402 
Batch[7109] - loss: 0.000567 
Batch[7110] - loss: 0.000598 
Batch[7111] - loss: 0.000380 
Batch[7112] - loss: 0.000547 
Batch[7113] - loss: 0.000608 
Batch[7114] - loss: 0.000415 
Batch[7115] - loss: 0.000583 
Batch[7116] - loss: 0.000391 
Batch[7117] - loss: 0.000648 
Batch[7118] - loss: 0.000861 
Batch[7119] - loss: 0.000435 
Batch[7120] - loss: 0.000350 
Batch[7121] - loss: 0.000563 
Batch[7122] - loss: 0.000652 
Batch[7123] - loss: 0.000311 
Batch[7124] - loss: 0.000512 
Batch[7125] - loss: 0.000405 
Batch[7126] - loss: 0.000723 
Batch[7127] - loss: 0.000606 
Batch[7128] - loss: 0.000651 
Batch[7129] - loss: 0.000927 
Batch[7130] - loss: 0.000557 
Batch[7131] - loss: 0.000876 
Batch[7132] - loss: 0.000586 
Batch[7133] - loss: 0.000426 
Batch[7134] - loss: 0.000856 
Batch[7135] - loss: 0.000618 
Batch[7136] - loss: 0.000766 
Batch[7137] - loss: 0.000758 
Batch[7138] - loss: 0.000778 
Batch[7139] - loss: 0.000477 
Batch[7140] - loss: 0.000408 
Batch[7141] - loss: 0.000703 
Batch[7142] - loss: 0.000826 
Batch[7143] - loss: 0.000805 
Batch[7144] - loss: 0.000893 
Batch[7145] - loss: 0.000512 
Batch[7146] - loss: 0.000324 
Batch[7147] - loss: 0.000689 
Batch[7148] - loss: 0.000983 
Batch[7149] - loss: 0.000941 
Batch[7150] - loss: 0.000680 
Batch[7151] - loss: 0.000390 
Batch[7152] - loss: 0.000331 
Batch[7153] - loss: 0.000771 
Batch[7154] - loss: 0.000539 
Batch[7155] - loss: 0.000409 
Batch[7156] - loss: 0.000737 
Batch[7157] - loss: 0.000653 
Batch[7158] - loss: 0.000559 
Batch[7159] - loss: 0.000396 
Batch[7160] - loss: 0.000372 
Batch[7161] - loss: 0.000806 
Batch[7162] - loss: 0.000499 
Batch[7163] - loss: 0.000511 
Batch[7164] - loss: 0.000872 
Batch[7165] - loss: 0.000608 
Batch[7166] - loss: 0.000710 
Batch[7167] - loss: 0.000548 
Batch[7168] - loss: 0.000557 
Batch[7169] - loss: 0.000675 
Batch[7170] - loss: 0.000342 
Batch[7171] - loss: 0.000530 
Batch[7172] - loss: 0.000451 
Batch[7173] - loss: 0.000628 
Batch[7174] - loss: 0.000632 
Batch[7175] - loss: 0.000628 
Batch[7176] - loss: 0.000397 
Batch[7177] - loss: 0.000455 
Batch[7178] - loss: 0.000334 
Batch[7179] - loss: 0.000447 
Batch[7180] - loss: 0.000563 
Batch[7181] - loss: 0.000288 
Batch[7182] - loss: 0.000541 
Batch[7183] - loss: 0.000667 
Batch[7184] - loss: 0.000559 
Batch[7185] - loss: 0.000550 
Batch[7186] - loss: 0.000502 
Batch[7187] - loss: 0.000691 
Batch[7188] - loss: 0.000775 
Batch[7189] - loss: 0.000418 
Batch[7190] - loss: 0.000619 
Batch[7191] - loss: 0.000387 
Batch[7192] - loss: 0.000471 
Batch[7193] - loss: 0.000688 
Batch[7194] - loss: 0.000712 
Batch[7195] - loss: 0.000943 
Batch[7196] - loss: 0.000701 
Batch[7197] - loss: 0.000484 
Batch[7198] - loss: 0.000682 
Batch[7199] - loss: 0.000397 
Batch[7200] - loss: 0.000513 

Evaluation - loss: 0.000068 pearson: 0.5582 

early stop by 1500 steps.
Batch[7201] - loss: 0.000834 
Batch[7202] - loss: 0.000314 
Batch[7203] - loss: 0.000571 
Batch[7204] - loss: 0.000462 
Batch[7205] - loss: 0.000583 
Batch[7206] - loss: 0.000802 
Batch[7207] - loss: 0.000767 
Batch[7208] - loss: 0.000744 
Batch[7209] - loss: 0.000542 
Batch[7210] - loss: 0.001236 
Batch[7211] - loss: 0.000685 
Batch[7212] - loss: 0.000423 
Batch[7213] - loss: 0.000639 
Batch[7214] - loss: 0.000430 
Batch[7215] - loss: 0.000837 
Batch[7216] - loss: 0.001915 
Batch[7217] - loss: 0.000316 
Batch[7218] - loss: 0.000708 
Batch[7219] - loss: 0.000404 
Batch[7220] - loss: 0.000454 
Batch[7221] - loss: 0.000334 
Batch[7222] - loss: 0.000388 
Batch[7223] - loss: 0.000494 
Batch[7224] - loss: 0.000544 
Batch[7225] - loss: 0.000326 
Batch[7226] - loss: 0.000716 
Batch[7227] - loss: 0.000563 
Batch[7228] - loss: 0.000628 
Batch[7229] - loss: 0.000616 
Batch[7230] - loss: 0.000689 
Batch[7231] - loss: 0.000418 
Batch[7232] - loss: 0.001043 
Batch[7233] - loss: 0.000301 
Batch[7234] - loss: 0.000428 
Batch[7235] - loss: 0.000322 
Batch[7236] - loss: 0.000680 
Batch[7237] - loss: 0.000469 
Batch[7238] - loss: 0.000655 
Batch[7239] - loss: 0.000635 
Batch[7240] - loss: 0.000560 
Batch[7241] - loss: 0.000259 
Batch[7242] - loss: 0.000589 
Batch[7243] - loss: 0.000713 
Batch[7244] - loss: 0.000523 
Batch[7245] - loss: 0.000353 
Batch[7246] - loss: 0.000352 
Batch[7247] - loss: 0.000774 
Batch[7248] - loss: 0.000733 
Batch[7249] - loss: 0.000538 
Batch[7250] - loss: 0.000492 
Batch[7251] - loss: 0.000592 
Batch[7252] - loss: 0.000328 
Batch[7253] - loss: 0.000430 
Batch[7254] - loss: 0.000476 
Batch[7255] - loss: 0.000320 
Batch[7256] - loss: 0.000728 
Batch[7257] - loss: 0.000680 
Batch[7258] - loss: 0.000373 
Batch[7259] - loss: 0.000558 
Batch[7260] - loss: 0.000916 
Batch[7261] - loss: 0.000458 
Batch[7262] - loss: 0.000706 
Batch[7263] - loss: 0.000561 
Batch[7264] - loss: 0.001237 
Batch[7265] - loss: 0.000742 
Batch[7266] - loss: 0.000533 
Batch[7267] - loss: 0.000381 
Batch[7268] - loss: 0.000597 
Batch[7269] - loss: 0.000827 
Batch[7270] - loss: 0.000587 
Batch[7271] - loss: 0.000747 
Batch[7272] - loss: 0.000699 
Batch[7273] - loss: 0.000659 
Batch[7274] - loss: 0.000742 
Batch[7275] - loss: 0.000348 
Batch[7276] - loss: 0.000427 
Batch[7277] - loss: 0.000273 
Batch[7278] - loss: 0.000724 
Batch[7279] - loss: 0.000595 
Batch[7280] - loss: 0.000986 
Batch[7281] - loss: 0.000912 
Batch[7282] - loss: 0.000539 
Batch[7283] - loss: 0.000205 
Batch[7284] - loss: 0.000400 
Batch[7285] - loss: 0.000556 
Batch[7286] - loss: 0.000918 
Batch[7287] - loss: 0.000431 
Batch[7288] - loss: 0.000991 
Batch[7289] - loss: 0.000588 
Batch[7290] - loss: 0.000363 
Batch[7291] - loss: 0.000665 
Batch[7292] - loss: 0.000309 
Batch[7293] - loss: 0.000435 
Batch[7294] - loss: 0.000614 
Batch[7295] - loss: 0.000700 
Batch[7296] - loss: 0.000594 
Batch[7297] - loss: 0.000343 
Batch[7298] - loss: 0.000538 
Batch[7299] - loss: 0.000691 
Batch[7300] - loss: 0.000554 

Evaluation - loss: 0.000069 pearson: 0.5570 

early stop by 1500 steps.
Batch[7301] - loss: 0.000733 
Batch[7302] - loss: 0.000437 
Batch[7303] - loss: 0.000594 
Batch[7304] - loss: 0.000924 
Batch[7305] - loss: 0.000524 
Batch[7306] - loss: 0.000428 
Batch[7307] - loss: 0.000653 
Batch[7308] - loss: 0.000765 
Batch[7309] - loss: 0.000467 
Batch[7310] - loss: 0.000667 
Batch[7311] - loss: 0.000375 
Batch[7312] - loss: 0.000472 
Batch[7313] - loss: 0.000506 
Batch[7314] - loss: 0.000460 
Batch[7315] - loss: 0.000406 
Batch[7316] - loss: 0.000323 
Batch[7317] - loss: 0.000350 
Batch[7318] - loss: 0.000607 
Batch[7319] - loss: 0.000324 
Batch[7320] - loss: 0.000431 
Batch[7321] - loss: 0.000573 
Batch[7322] - loss: 0.000477 
Batch[7323] - loss: 0.000532 
Batch[7324] - loss: 0.000220 
Batch[7325] - loss: 0.000544 
Batch[7326] - loss: 0.000477 
Batch[7327] - loss: 0.000524 
Batch[7328] - loss: 0.001094 
Batch[7329] - loss: 0.000556 
Batch[7330] - loss: 0.000783 
Batch[7331] - loss: 0.000850 
Batch[7332] - loss: 0.000449 
Batch[7333] - loss: 0.000533 
Batch[7334] - loss: 0.000328 
Batch[7335] - loss: 0.000406 
Batch[7336] - loss: 0.000841 
Batch[7337] - loss: 0.000751 
Batch[7338] - loss: 0.000403 
Batch[7339] - loss: 0.000934 
Batch[7340] - loss: 0.000709 
Batch[7341] - loss: 0.000390 
Batch[7342] - loss: 0.000979 
Batch[7343] - loss: 0.000284 
Batch[7344] - loss: 0.000677 
Batch[7345] - loss: 0.000593 
Batch[7346] - loss: 0.000487 
Batch[7347] - loss: 0.000640 
Batch[7348] - loss: 0.000542 
Batch[7349] - loss: 0.000803 
Batch[7350] - loss: 0.000592 
Batch[7351] - loss: 0.000623 
Batch[7352] - loss: 0.000669 
Batch[7353] - loss: 0.000342 
Batch[7354] - loss: 0.000612 
Batch[7355] - loss: 0.000904 
Batch[7356] - loss: 0.000773 
Batch[7357] - loss: 0.000524 
Batch[7358] - loss: 0.000589 
Batch[7359] - loss: 0.001010 
Batch[7360] - loss: 0.001070 
Batch[7361] - loss: 0.001021 
Batch[7362] - loss: 0.000601 
Batch[7363] - loss: 0.000619 
Batch[7364] - loss: 0.000564 
Batch[7365] - loss: 0.000659 
Batch[7366] - loss: 0.000380 
Batch[7367] - loss: 0.000817 
Batch[7368] - loss: 0.000531 
Batch[7369] - loss: 0.000930 
Batch[7370] - loss: 0.000592 
Batch[7371] - loss: 0.000925 
Batch[7372] - loss: 0.000392 
Batch[7373] - loss: 0.000363 
Batch[7374] - loss: 0.000356 
Batch[7375] - loss: 0.000416 
Batch[7376] - loss: 0.000611 
Batch[7377] - loss: 0.000670 
Batch[7378] - loss: 0.000502 
Batch[7379] - loss: 0.000548 
Batch[7380] - loss: 0.000427 
Batch[7381] - loss: 0.000594 
Batch[7382] - loss: 0.000488 
Batch[7383] - loss: 0.000657 
Batch[7384] - loss: 0.000471 
Batch[7385] - loss: 0.000473 
Batch[7386] - loss: 0.000446 
Batch[7387] - loss: 0.001099 
Batch[7388] - loss: 0.000916 
Batch[7389] - loss: 0.000922 
Batch[7390] - loss: 0.000765 
Batch[7391] - loss: 0.000478 
Batch[7392] - loss: 0.000525 
Batch[7393] - loss: 0.000607 
Batch[7394] - loss: 0.000428 
Batch[7395] - loss: 0.000713 
Batch[7396] - loss: 0.000381 
Batch[7397] - loss: 0.000691 
Batch[7398] - loss: 0.000446 
Batch[7399] - loss: 0.000698 
Batch[7400] - loss: 0.000556 

Evaluation - loss: 0.000069 pearson: 0.5554 

early stop by 1500 steps.
Batch[7401] - loss: 0.000589 
Batch[7402] - loss: 0.000480 
Batch[7403] - loss: 0.000805 
Batch[7404] - loss: 0.000655 
Batch[7405] - loss: 0.000640 
Batch[7406] - loss: 0.000438 
Batch[7407] - loss: 0.000769 
Batch[7408] - loss: 0.000411 
Batch[7409] - loss: 0.000517 
Batch[7410] - loss: 0.000450 
Batch[7411] - loss: 0.000347 
Batch[7412] - loss: 0.000393 
Batch[7413] - loss: 0.000464 
Batch[7414] - loss: 0.000458 
Batch[7415] - loss: 0.000435 
Batch[7416] - loss: 0.000318 
Batch[7417] - loss: 0.000410 
Batch[7418] - loss: 0.000658 
Batch[7419] - loss: 0.001846 
Batch[7420] - loss: 0.000383 
Batch[7421] - loss: 0.000401 
Batch[7422] - loss: 0.000832 
Batch[7423] - loss: 0.000676 
Batch[7424] - loss: 0.001195 
Batch[7425] - loss: 0.000707 
Batch[7426] - loss: 0.000328 
Batch[7427] - loss: 0.000787 
Batch[7428] - loss: 0.000605 
Batch[7429] - loss: 0.000527 
Batch[7430] - loss: 0.000457 
Batch[7431] - loss: 0.000565 
Batch[7432] - loss: 0.000523 
Batch[7433] - loss: 0.000429 
Batch[7434] - loss: 0.000546 
Batch[7435] - loss: 0.000426 
Batch[7436] - loss: 0.000568 
Batch[7437] - loss: 0.000707 
Batch[7438] - loss: 0.001095 
Batch[7439] - loss: 0.001207 
Batch[7440] - loss: 0.000740 
Batch[7441] - loss: 0.000595 
Batch[7442] - loss: 0.000359 
Batch[7443] - loss: 0.000582 
Batch[7444] - loss: 0.000582 
Batch[7445] - loss: 0.000457 
Batch[7446] - loss: 0.000642 
Batch[7447] - loss: 0.000496 
Batch[7448] - loss: 0.000660 
Batch[7449] - loss: 0.000385 
Batch[7450] - loss: 0.000516 
Batch[7451] - loss: 0.000436 
Batch[7452] - loss: 0.000284 
Batch[7453] - loss: 0.000362 
Batch[7454] - loss: 0.000588 
Batch[7455] - loss: 0.000357 
Batch[7456] - loss: 0.000448 
Batch[7457] - loss: 0.000562 
Batch[7458] - loss: 0.000322 
Batch[7459] - loss: 0.000775 
Batch[7460] - loss: 0.000443 
Batch[7461] - loss: 0.000682 
Batch[7462] - loss: 0.000229 
Batch[7463] - loss: 0.000599 
Batch[7464] - loss: 0.000559 
Batch[7465] - loss: 0.000375 
Batch[7466] - loss: 0.000449 
Batch[7467] - loss: 0.000674 
Batch[7468] - loss: 0.000844 
Batch[7469] - loss: 0.000529 
Batch[7470] - loss: 0.000548 
Batch[7471] - loss: 0.000355 
Batch[7472] - loss: 0.000641 
Batch[7473] - loss: 0.000444 
Batch[7474] - loss: 0.000385 
Batch[7475] - loss: 0.001130 
Batch[7476] - loss: 0.000570 
Batch[7477] - loss: 0.000324 
Batch[7478] - loss: 0.000607 
Batch[7479] - loss: 0.000333 
Batch[7480] - loss: 0.000803 
Batch[7481] - loss: 0.000519 
Batch[7482] - loss: 0.000721 
Batch[7483] - loss: 0.000436 
Batch[7484] - loss: 0.000948 
Batch[7485] - loss: 0.000495 
Batch[7486] - loss: 0.000627 
Batch[7487] - loss: 0.000806 
Batch[7488] - loss: 0.000631 
Batch[7489] - loss: 0.000282 
Batch[7490] - loss: 0.000618 
Batch[7491] - loss: 0.000533 
Batch[7492] - loss: 0.000416 
Batch[7493] - loss: 0.000589 
Batch[7494] - loss: 0.000817 
Batch[7495] - loss: 0.000892 
Batch[7496] - loss: 0.000819 
Batch[7497] - loss: 0.000682 
Batch[7498] - loss: 0.000642 
Batch[7499] - loss: 0.000556 
Batch[7500] - loss: 0.000391 

Evaluation - loss: 0.000069 pearson: 0.5534 

early stop by 1500 steps.
Batch[7501] - loss: 0.000379 
Batch[7502] - loss: 0.000471 
Batch[7503] - loss: 0.000435 
Batch[7504] - loss: 0.000659 
Batch[7505] - loss: 0.000538 
Batch[7506] - loss: 0.000469 
Batch[7507] - loss: 0.000460 
Batch[7508] - loss: 0.000415 
Batch[7509] - loss: 0.000406 
Batch[7510] - loss: 0.000661 
Batch[7511] - loss: 0.000478 
Batch[7512] - loss: 0.000504 
Batch[7513] - loss: 0.000927 
Batch[7514] - loss: 0.000901 
Batch[7515] - loss: 0.000480 
Batch[7516] - loss: 0.000628 
Batch[7517] - loss: 0.000701 
Batch[7518] - loss: 0.000410 
Batch[7519] - loss: 0.000358 
Batch[7520] - loss: 0.000308 
Batch[7521] - loss: 0.000432 
Batch[7522] - loss: 0.000639 
Batch[7523] - loss: 0.000346 
Batch[7524] - loss: 0.000489 
Batch[7525] - loss: 0.000305 
Batch[7526] - loss: 0.000609 
Batch[7527] - loss: 0.000494 
Batch[7528] - loss: 0.000296 
Batch[7529] - loss: 0.000303 
Batch[7530] - loss: 0.000799 
Batch[7531] - loss: 0.000579 
Batch[7532] - loss: 0.000677 
Batch[7533] - loss: 0.000440 
Batch[7534] - loss: 0.000548 
Batch[7535] - loss: 0.000765 
Batch[7536] - loss: 0.000441 
Batch[7537] - loss: 0.000400 
Batch[7538] - loss: 0.000419 
Batch[7539] - loss: 0.000254 
Batch[7540] - loss: 0.000325 
Batch[7541] - loss: 0.000410 
Batch[7542] - loss: 0.000426 
Batch[7543] - loss: 0.000642 
Batch[7544] - loss: 0.000607 
Batch[7545] - loss: 0.000376 
Batch[7546] - loss: 0.001459 
Batch[7547] - loss: 0.000249 
Batch[7548] - loss: 0.000453 
Batch[7549] - loss: 0.000398 
Batch[7550] - loss: 0.000791 
Batch[7551] - loss: 0.000424 
Batch[7552] - loss: 0.000641 
Batch[7553] - loss: 0.000418 
Batch[7554] - loss: 0.000542 
Batch[7555] - loss: 0.000652 
Batch[7556] - loss: 0.000436 
Batch[7557] - loss: 0.000610 
Batch[7558] - loss: 0.000452 
Batch[7559] - loss: 0.000633 
Batch[7560] - loss: 0.000372 
Batch[7561] - loss: 0.000317 
Batch[7562] - loss: 0.000323 
Batch[7563] - loss: 0.000807 
Batch[7564] - loss: 0.000391 
Batch[7565] - loss: 0.000611 
Batch[7566] - loss: 0.000653 
Batch[7567] - loss: 0.000465 
Batch[7568] - loss: 0.000568 
Batch[7569] - loss: 0.000616 
Batch[7570] - loss: 0.000605 
Batch[7571] - loss: 0.000612 
Batch[7572] - loss: 0.000513 
Batch[7573] - loss: 0.000955 
Batch[7574] - loss: 0.000457 
Batch[7575] - loss: 0.000480 
Batch[7576] - loss: 0.000455 
Batch[7577] - loss: 0.000380 
Batch[7578] - loss: 0.000527 
Batch[7579] - loss: 0.000624 
Batch[7580] - loss: 0.000624 
Batch[7581] - loss: 0.000558 
Batch[7582] - loss: 0.000475 
Batch[7583] - loss: 0.000444 
Batch[7584] - loss: 0.000448 
Batch[7585] - loss: 0.000491 
Batch[7586] - loss: 0.000267 
Batch[7587] - loss: 0.000614 
Batch[7588] - loss: 0.000685 
Batch[7589] - loss: 0.000513 
Batch[7590] - loss: 0.000606 
Batch[7591] - loss: 0.000562 
Batch[7592] - loss: 0.000417 
Batch[7593] - loss: 0.000416 
Batch[7594] - loss: 0.000611 
Batch[7595] - loss: 0.000496 
Batch[7596] - loss: 0.000315 
Batch[7597] - loss: 0.000323 
Batch[7598] - loss: 0.000398 
Batch[7599] - loss: 0.000585 
Batch[7600] - loss: 0.000646 

Evaluation - loss: 0.000069 pearson: 0.5570 

early stop by 1500 steps.
Batch[7601] - loss: 0.000332 
Batch[7602] - loss: 0.000549 
Batch[7603] - loss: 0.000555 
Batch[7604] - loss: 0.000410 
Batch[7605] - loss: 0.000305 
Batch[7606] - loss: 0.000277 
Batch[7607] - loss: 0.000422 
Batch[7608] - loss: 0.000535 
Batch[7609] - loss: 0.000640 
Batch[7610] - loss: 0.000453 
Batch[7611] - loss: 0.000771 
Batch[7612] - loss: 0.000985 
Batch[7613] - loss: 0.000510 
Batch[7614] - loss: 0.000535 
Batch[7615] - loss: 0.000480 
Batch[7616] - loss: 0.000484 
Batch[7617] - loss: 0.000711 
Batch[7618] - loss: 0.000498 
Batch[7619] - loss: 0.000529 
Batch[7620] - loss: 0.000614 
Batch[7621] - loss: 0.000576 
Batch[7622] - loss: 0.000356 
Batch[7623] - loss: 0.001204 
Batch[7624] - loss: 0.000650 
Batch[7625] - loss: 0.000401 
Batch[7626] - loss: 0.000330 
Batch[7627] - loss: 0.000369 
Batch[7628] - loss: 0.000672 
Batch[7629] - loss: 0.000522 
Batch[7630] - loss: 0.000321 
Batch[7631] - loss: 0.000348 
Batch[7632] - loss: 0.000291 
Batch[7633] - loss: 0.000515 
Batch[7634] - loss: 0.000313 
Batch[7635] - loss: 0.000563 
Batch[7636] - loss: 0.000389 
Batch[7637] - loss: 0.000603 
Batch[7638] - loss: 0.000733 
Batch[7639] - loss: 0.000673 
Batch[7640] - loss: 0.000389 
Batch[7641] - loss: 0.000394 
Batch[7642] - loss: 0.000591 
Batch[7643] - loss: 0.000592 
Batch[7644] - loss: 0.000359 
Batch[7645] - loss: 0.000591 
Batch[7646] - loss: 0.000517 
Batch[7647] - loss: 0.000772 
Batch[7648] - loss: 0.000684 
Batch[7649] - loss: 0.000372 
Batch[7650] - loss: 0.000606 
Batch[7651] - loss: 0.000328 
Batch[7652] - loss: 0.000300 
Batch[7653] - loss: 0.000466 
Batch[7654] - loss: 0.000394 
Batch[7655] - loss: 0.000385 
Batch[7656] - loss: 0.000472 
Batch[7657] - loss: 0.000564 
Batch[7658] - loss: 0.000648 
Batch[7659] - loss: 0.000411 
Batch[7660] - loss: 0.000275 
Batch[7661] - loss: 0.000328 
Batch[7662] - loss: 0.000320 
Batch[7663] - loss: 0.000308 
Batch[7664] - loss: 0.000468 
Batch[7665] - loss: 0.000384 
Batch[7666] - loss: 0.000535 
Batch[7667] - loss: 0.000364 
Batch[7668] - loss: 0.000316 
Batch[7669] - loss: 0.000485 
Batch[7670] - loss: 0.000445 
Batch[7671] - loss: 0.000437 
Batch[7672] - loss: 0.000397 
Batch[7673] - loss: 0.000689 
Batch[7674] - loss: 0.000455 
Batch[7675] - loss: 0.000696 
Batch[7676] - loss: 0.000414 
Batch[7677] - loss: 0.000449 
Batch[7678] - loss: 0.001095 
Batch[7679] - loss: 0.000286 
Batch[7680] - loss: 0.000764 
Batch[7681] - loss: 0.000370 
Batch[7682] - loss: 0.000431 
Batch[7683] - loss: 0.001215 
Batch[7684] - loss: 0.000566 
Batch[7685] - loss: 0.000699 
Batch[7686] - loss: 0.000649 
Batch[7687] - loss: 0.000571 
Batch[7688] - loss: 0.000832 
Batch[7689] - loss: 0.000385 
Batch[7690] - loss: 0.000572 
Batch[7691] - loss: 0.000399 
Batch[7692] - loss: 0.000655 
Batch[7693] - loss: 0.000653 
Batch[7694] - loss: 0.000418 
Batch[7695] - loss: 0.000546 
Batch[7696] - loss: 0.000466 
Batch[7697] - loss: 0.000465 
Batch[7698] - loss: 0.000433 
Batch[7699] - loss: 0.000830 
Batch[7700] - loss: 0.000255 

Evaluation - loss: 0.000069 pearson: 0.5531 

early stop by 1500 steps.
Batch[7701] - loss: 0.000561 
Batch[7702] - loss: 0.000510 
Batch[7703] - loss: 0.000318 
Batch[7704] - loss: 0.000392 
Batch[7705] - loss: 0.000411 
Batch[7706] - loss: 0.000329 
Batch[7707] - loss: 0.000547 
Batch[7708] - loss: 0.000729 
Batch[7709] - loss: 0.000408 
Batch[7710] - loss: 0.000375 
Batch[7711] - loss: 0.000320 
Batch[7712] - loss: 0.000557 
Batch[7713] - loss: 0.000600 
Batch[7714] - loss: 0.000535 
Batch[7715] - loss: 0.000374 
Batch[7716] - loss: 0.000444 
Batch[7717] - loss: 0.000316 
Batch[7718] - loss: 0.000291 
Batch[7719] - loss: 0.000429 
Batch[7720] - loss: 0.000284 
Batch[7721] - loss: 0.000881 
Batch[7722] - loss: 0.000743 
Batch[7723] - loss: 0.000750 
Batch[7724] - loss: 0.000341 
Batch[7725] - loss: 0.000370 
Batch[7726] - loss: 0.000271 
Batch[7727] - loss: 0.000385 
Batch[7728] - loss: 0.000494 
Batch[7729] - loss: 0.000687 
Batch[7730] - loss: 0.000310 
Batch[7731] - loss: 0.000820 
Batch[7732] - loss: 0.000383 
Batch[7733] - loss: 0.000851 
Batch[7734] - loss: 0.000835 
Batch[7735] - loss: 0.000310 
Batch[7736] - loss: 0.000365 
Batch[7737] - loss: 0.000437 
Batch[7738] - loss: 0.000471 
Batch[7739] - loss: 0.000471 
Batch[7740] - loss: 0.000469 
Batch[7741] - loss: 0.000428 
Batch[7742] - loss: 0.000577 
Batch[7743] - loss: 0.000796 
Batch[7744] - loss: 0.000553 
Batch[7745] - loss: 0.000488 
Batch[7746] - loss: 0.000541 
Batch[7747] - loss: 0.000627 
Batch[7748] - loss: 0.000441 
Batch[7749] - loss: 0.000508 
Batch[7750] - loss: 0.000612 
Batch[7751] - loss: 0.000305 
Batch[7752] - loss: 0.000390 
Batch[7753] - loss: 0.001192 
Batch[7754] - loss: 0.000910 
Batch[7755] - loss: 0.000435 
Batch[7756] - loss: 0.000617 
Batch[7757] - loss: 0.000313 
Batch[7758] - loss: 0.000483 
Batch[7759] - loss: 0.000629 
Batch[7760] - loss: 0.000395 
Batch[7761] - loss: 0.000410 
Batch[7762] - loss: 0.000601 
Batch[7763] - loss: 0.000454 
Batch[7764] - loss: 0.000339 
Batch[7765] - loss: 0.000588 
Batch[7766] - loss: 0.000664 
Batch[7767] - loss: 0.000312 
Batch[7768] - loss: 0.000334 
Batch[7769] - loss: 0.000462 
Batch[7770] - loss: 0.000518 
Batch[7771] - loss: 0.000387 
Batch[7772] - loss: 0.000530 
Batch[7773] - loss: 0.000434 
Batch[7774] - loss: 0.000823 
Batch[7775] - loss: 0.000510 
Batch[7776] - loss: 0.000341 
Batch[7777] - loss: 0.000279 
Batch[7778] - loss: 0.000408 
Batch[7779] - loss: 0.000492 
Batch[7780] - loss: 0.000460 
Batch[7781] - loss: 0.000483 
Batch[7782] - loss: 0.000492 
Batch[7783] - loss: 0.000489 
Batch[7784] - loss: 0.000646 
Batch[7785] - loss: 0.000578 
Batch[7786] - loss: 0.000556 
Batch[7787] - loss: 0.000517 
Batch[7788] - loss: 0.000226 
Batch[7789] - loss: 0.000306 
Batch[7790] - loss: 0.000460 
Batch[7791] - loss: 0.000552 
Batch[7792] - loss: 0.000496 
Batch[7793] - loss: 0.000740 
Batch[7794] - loss: 0.000374 
Batch[7795] - loss: 0.000308 
Batch[7796] - loss: 0.000305 
Batch[7797] - loss: 0.000510 
Batch[7798] - loss: 0.000322 
Batch[7799] - loss: 0.000279 
Batch[7800] - loss: 0.000290 

Evaluation - loss: 0.000069 pearson: 0.5545 

early stop by 1500 steps.
Batch[7801] - loss: 0.000271 
Batch[7802] - loss: 0.000350 
Batch[7803] - loss: 0.000399 
Batch[7804] - loss: 0.000444 
Batch[7805] - loss: 0.000198 
Batch[7806] - loss: 0.000544 
Batch[7807] - loss: 0.000338 
Batch[7808] - loss: 0.000402 
Batch[7809] - loss: 0.001101 
Batch[7810] - loss: 0.000328 
Batch[7811] - loss: 0.000316 
Batch[7812] - loss: 0.001069 
Batch[7813] - loss: 0.000425 
Batch[7814] - loss: 0.000536 
Batch[7815] - loss: 0.000647 
Batch[7816] - loss: 0.000503 
Batch[7817] - loss: 0.000411 
Batch[7818] - loss: 0.000288 
Batch[7819] - loss: 0.000413 
Batch[7820] - loss: 0.000590 
Batch[7821] - loss: 0.000424 
Batch[7822] - loss: 0.000353 
Batch[7823] - loss: 0.000473 
Batch[7824] - loss: 0.000419 
Batch[7825] - loss: 0.000642 
Batch[7826] - loss: 0.000279 
Batch[7827] - loss: 0.000445 
Batch[7828] - loss: 0.000538 
Batch[7829] - loss: 0.000622 
Batch[7830] - loss: 0.000494 
Batch[7831] - loss: 0.000383 
Batch[7832] - loss: 0.000510 
Batch[7833] - loss: 0.000423 
Batch[7834] - loss: 0.000334 
Batch[7835] - loss: 0.000361 
Batch[7836] - loss: 0.000578 
Batch[7837] - loss: 0.000525 
Batch[7838] - loss: 0.000467 
Batch[7839] - loss: 0.000416 
Batch[7840] - loss: 0.000858 
Batch[7841] - loss: 0.000447 
Batch[7842] - loss: 0.000562 
Batch[7843] - loss: 0.000486 
Batch[7844] - loss: 0.000608 
Batch[7845] - loss: 0.000465 
Batch[7846] - loss: 0.000701 
Batch[7847] - loss: 0.000397 
Batch[7848] - loss: 0.000516 
Batch[7849] - loss: 0.000361 
Batch[7850] - loss: 0.000790 
Batch[7851] - loss: 0.000664 
Batch[7852] - loss: 0.000779 
Batch[7853] - loss: 0.000405 
Batch[7854] - loss: 0.000358 
Batch[7855] - loss: 0.000410 
Batch[7856] - loss: 0.000574 
Batch[7857] - loss: 0.000339 
Batch[7858] - loss: 0.000708 
Batch[7859] - loss: 0.000701 
Batch[7860] - loss: 0.000442 
Batch[7861] - loss: 0.000370 
Batch[7862] - loss: 0.000438 
Batch[7863] - loss: 0.000803 
Batch[7864] - loss: 0.000339 
Batch[7865] - loss: 0.000296 
Batch[7866] - loss: 0.000433 
Batch[7867] - loss: 0.000686 
Batch[7868] - loss: 0.000246 
Batch[7869] - loss: 0.000335 
Batch[7870] - loss: 0.000709 
Batch[7871] - loss: 0.000451 
Batch[7872] - loss: 0.000454 
Batch[7873] - loss: 0.000494 
Batch[7874] - loss: 0.000219 
Batch[7875] - loss: 0.000467 
Batch[7876] - loss: 0.000397 
Batch[7877] - loss: 0.000643 
Batch[7878] - loss: 0.000399 
Batch[7879] - loss: 0.000616 
Batch[7880] - loss: 0.000402 
Batch[7881] - loss: 0.000376 
Batch[7882] - loss: 0.000372 
Batch[7883] - loss: 0.000430 
Batch[7884] - loss: 0.000275 
Batch[7885] - loss: 0.000603 
Batch[7886] - loss: 0.000297 
Batch[7887] - loss: 0.000470 
Batch[7888] - loss: 0.000610 
Batch[7889] - loss: 0.000381 
Batch[7890] - loss: 0.000327 
Batch[7891] - loss: 0.000345 
Batch[7892] - loss: 0.000439 
Batch[7893] - loss: 0.000858 
Batch[7894] - loss: 0.000424 
Batch[7895] - loss: 0.000296 
Batch[7896] - loss: 0.000431 
Batch[7897] - loss: 0.000703 
Batch[7898] - loss: 0.000470 
Batch[7899] - loss: 0.000396 
Batch[7900] - loss: 0.000530 

Evaluation - loss: 0.000069 pearson: 0.5545 

early stop by 1500 steps.
Batch[7901] - loss: 0.000293 
Batch[7902] - loss: 0.000511 
Batch[7903] - loss: 0.000502 
Batch[7904] - loss: 0.001026 
Batch[7905] - loss: 0.000367 
Batch[7906] - loss: 0.000333 
Batch[7907] - loss: 0.000536 
Batch[7908] - loss: 0.000518 
Batch[7909] - loss: 0.000475 
Batch[7910] - loss: 0.000556 
Batch[7911] - loss: 0.000629 
Batch[7912] - loss: 0.000379 
Batch[7913] - loss: 0.000655 
Batch[7914] - loss: 0.000665 
Batch[7915] - loss: 0.000409 
Batch[7916] - loss: 0.000363 
Batch[7917] - loss: 0.000461 
Batch[7918] - loss: 0.000429 
Batch[7919] - loss: 0.000335 
Batch[7920] - loss: 0.000237 
Batch[7921] - loss: 0.000441 
Batch[7922] - loss: 0.000305 
Batch[7923] - loss: 0.000386 
Batch[7924] - loss: 0.000547 
Batch[7925] - loss: 0.000456 
Batch[7926] - loss: 0.000462 
Batch[7927] - loss: 0.000490 
Batch[7928] - loss: 0.000484 
Batch[7929] - loss: 0.000430 
Batch[7930] - loss: 0.000305 
Batch[7931] - loss: 0.000650 
Batch[7932] - loss: 0.000824 
Batch[7933] - loss: 0.000589 
Batch[7934] - loss: 0.000444 
Batch[7935] - loss: 0.000843 
Batch[7936] - loss: 0.000399 
Batch[7937] - loss: 0.000251 
Batch[7938] - loss: 0.000213 
Batch[7939] - loss: 0.000324 
Batch[7940] - loss: 0.000518 
Batch[7941] - loss: 0.000168 
Batch[7942] - loss: 0.000496 
Batch[7943] - loss: 0.000402 
Batch[7944] - loss: 0.000514 
Batch[7945] - loss: 0.000557 
Batch[7946] - loss: 0.000328 
Batch[7947] - loss: 0.000442 
Batch[7948] - loss: 0.000510 
Batch[7949] - loss: 0.000275 
Batch[7950] - loss: 0.000789 
Batch[7951] - loss: 0.000498 
Batch[7952] - loss: 0.000219 
Batch[7953] - loss: 0.000779 
Batch[7954] - loss: 0.000565 
Batch[7955] - loss: 0.000341 
Batch[7956] - loss: 0.000430 
Batch[7957] - loss: 0.000528 
Batch[7958] - loss: 0.000642 
Batch[7959] - loss: 0.000520 
Batch[7960] - loss: 0.000723 
Batch[7961] - loss: 0.000559 
Batch[7962] - loss: 0.000768 
Batch[7963] - loss: 0.000649 
Batch[7964] - loss: 0.000530 
Batch[7965] - loss: 0.000675 
Batch[7966] - loss: 0.000608 
Batch[7967] - loss: 0.000989 
Batch[7968] - loss: 0.000534 
Batch[7969] - loss: 0.000450 
Batch[7970] - loss: 0.000837 
Batch[7971] - loss: 0.000583 
Batch[7972] - loss: 0.001038 
Batch[7973] - loss: 0.000508 
Batch[7974] - loss: 0.000729 
Batch[7975] - loss: 0.000657 
Batch[7976] - loss: 0.000385 
Batch[7977] - loss: 0.000487 
Batch[7978] - loss: 0.000993 
Batch[7979] - loss: 0.000690 
Batch[7980] - loss: 0.000430 
Batch[7981] - loss: 0.000687 
Batch[7982] - loss: 0.000653 
Batch[7983] - loss: 0.000353 
Batch[7984] - loss: 0.000810 
Batch[7985] - loss: 0.000341 
Batch[7986] - loss: 0.000394 
Batch[7987] - loss: 0.000609 
Batch[7988] - loss: 0.000488 
Batch[7989] - loss: 0.000398 
Batch[7990] - loss: 0.000297 
Batch[7991] - loss: 0.000595 
Batch[7992] - loss: 0.000485 
Batch[7993] - loss: 0.000444 
Batch[7994] - loss: 0.000379 
Batch[7995] - loss: 0.000504 
Batch[7996] - loss: 0.000319 
Batch[7997] - loss: 0.000374 
Batch[7998] - loss: 0.000314 
Batch[7999] - loss: 0.000314 
Batch[8000] - loss: 0.000447 

Evaluation - loss: 0.000069 pearson: 0.5572 

early stop by 1500 steps.
Batch[8001] - loss: 0.000509 
Batch[8002] - loss: 0.000415 
Batch[8003] - loss: 0.000327 
Batch[8004] - loss: 0.000466 
Batch[8005] - loss: 0.001028 
Batch[8006] - loss: 0.000583 
Batch[8007] - loss: 0.000567 
Batch[8008] - loss: 0.000545 
Batch[8009] - loss: 0.000202 
Batch[8010] - loss: 0.000479 
Batch[8011] - loss: 0.000493 
Batch[8012] - loss: 0.000311 
Batch[8013] - loss: 0.000356 
Batch[8014] - loss: 0.000627 
Batch[8015] - loss: 0.000257 
Batch[8016] - loss: 0.000486 
Batch[8017] - loss: 0.000876 
Batch[8018] - loss: 0.000447 
Batch[8019] - loss: 0.001063 
Batch[8020] - loss: 0.000812 
Batch[8021] - loss: 0.000378 
Batch[8022] - loss: 0.000417 
Batch[8023] - loss: 0.000811 
Batch[8024] - loss: 0.000452 
Batch[8025] - loss: 0.000578 
Batch[8026] - loss: 0.000552 
Batch[8027] - loss: 0.000409 
Batch[8028] - loss: 0.000663 
Batch[8029] - loss: 0.000832 
Batch[8030] - loss: 0.000470 
Batch[8031] - loss: 0.000701 
Batch[8032] - loss: 0.000582 
Batch[8033] - loss: 0.000573 
Batch[8034] - loss: 0.000461 
Batch[8035] - loss: 0.000344 
Batch[8036] - loss: 0.000573 
Batch[8037] - loss: 0.000478 
Batch[8038] - loss: 0.000230 
Batch[8039] - loss: 0.000422 
Batch[8040] - loss: 0.000627 
Batch[8041] - loss: 0.000508 
Batch[8042] - loss: 0.000679 
Batch[8043] - loss: 0.000506 
Batch[8044] - loss: 0.000699 
Batch[8045] - loss: 0.000709 
Batch[8046] - loss: 0.000579 
Batch[8047] - loss: 0.000364 
Batch[8048] - loss: 0.000569 
Batch[8049] - loss: 0.000274 
Batch[8050] - loss: 0.000513 
Batch[8051] - loss: 0.000814 
Batch[8052] - loss: 0.000478 
Batch[8053] - loss: 0.000585 
Batch[8054] - loss: 0.000381 
Batch[8055] - loss: 0.000701 
Batch[8056] - loss: 0.000452 
Batch[8057] - loss: 0.000492 
Batch[8058] - loss: 0.000438 
Batch[8059] - loss: 0.000785 
Batch[8060] - loss: 0.000639 
Batch[8061] - loss: 0.000356 
Batch[8062] - loss: 0.000524 
Batch[8063] - loss: 0.000719 
Batch[8064] - loss: 0.000439 
Batch[8065] - loss: 0.000704 
Batch[8066] - loss: 0.000368 
Batch[8067] - loss: 0.000418 
Batch[8068] - loss: 0.000316 
Batch[8069] - loss: 0.000420 
Batch[8070] - loss: 0.000320 
Batch[8071] - loss: 0.000568 
Batch[8072] - loss: 0.000690 
Batch[8073] - loss: 0.000304 
Batch[8074] - loss: 0.000523 
Batch[8075] - loss: 0.000408 
Batch[8076] - loss: 0.000373 
Batch[8077] - loss: 0.000422 
Batch[8078] - loss: 0.000627 
Batch[8079] - loss: 0.000356 
Batch[8080] - loss: 0.000653 
Batch[8081] - loss: 0.000696 
Batch[8082] - loss: 0.001011 
Batch[8083] - loss: 0.000402 
Batch[8084] - loss: 0.000369 
Batch[8085] - loss: 0.000740 
Batch[8086] - loss: 0.000719 
Batch[8087] - loss: 0.000534 
Batch[8088] - loss: 0.000311 
Batch[8089] - loss: 0.000244 
Batch[8090] - loss: 0.000582 
Batch[8091] - loss: 0.000349 
Batch[8092] - loss: 0.000983 
Batch[8093] - loss: 0.000932 
Batch[8094] - loss: 0.000521 
Batch[8095] - loss: 0.000617 
Batch[8096] - loss: 0.000314 
Batch[8097] - loss: 0.000295 
Batch[8098] - loss: 0.000382 
Batch[8099] - loss: 0.000489 
Batch[8100] - loss: 0.000456 

Evaluation - loss: 0.000069 pearson: 0.5570 

early stop by 1500 steps.
Batch[8101] - loss: 0.000571 
Batch[8102] - loss: 0.000444 
Batch[8103] - loss: 0.001033 
Batch[8104] - loss: 0.000782 
Batch[8105] - loss: 0.000440 
Batch[8106] - loss: 0.000453 
Batch[8107] - loss: 0.000386 
Batch[8108] - loss: 0.000566 
Batch[8109] - loss: 0.000522 
Batch[8110] - loss: 0.000634 
Batch[8111] - loss: 0.000541 
Batch[8112] - loss: 0.000327 
Batch[8113] - loss: 0.000437 
Batch[8114] - loss: 0.000631 
Batch[8115] - loss: 0.000298 
Batch[8116] - loss: 0.000492 
Batch[8117] - loss: 0.000605 
Batch[8118] - loss: 0.000532 
Batch[8119] - loss: 0.000945 
Batch[8120] - loss: 0.000225 
Batch[8121] - loss: 0.000409 
Batch[8122] - loss: 0.000373 
Batch[8123] - loss: 0.000299 
Batch[8124] - loss: 0.000243 
Batch[8125] - loss: 0.000311 
Batch[8126] - loss: 0.000493 
Batch[8127] - loss: 0.000272 
Batch[8128] - loss: 0.000795 
Batch[8129] - loss: 0.000444 
Batch[8130] - loss: 0.000407 
Batch[8131] - loss: 0.000539 
Batch[8132] - loss: 0.000419 
Batch[8133] - loss: 0.000309 
Batch[8134] - loss: 0.000427 
Batch[8135] - loss: 0.000428 
Batch[8136] - loss: 0.000623 
Batch[8137] - loss: 0.000427 
Batch[8138] - loss: 0.000411 
Batch[8139] - loss: 0.000342 
Batch[8140] - loss: 0.000715 
Batch[8141] - loss: 0.000431 
Batch[8142] - loss: 0.000423 
Batch[8143] - loss: 0.000335 
Batch[8144] - loss: 0.000456 
Batch[8145] - loss: 0.000597 
Batch[8146] - loss: 0.000443 
Batch[8147] - loss: 0.000628 
Batch[8148] - loss: 0.000424 
Batch[8149] - loss: 0.000229 
Batch[8150] - loss: 0.000517 
Batch[8151] - loss: 0.000444 
Batch[8152] - loss: 0.000479 
Batch[8153] - loss: 0.000524 
Batch[8154] - loss: 0.000264 
Batch[8155] - loss: 0.000747 
Batch[8156] - loss: 0.000277 
Batch[8157] - loss: 0.000587 
Batch[8158] - loss: 0.000606 
Batch[8159] - loss: 0.000509 
Batch[8160] - loss: 0.000351 
Batch[8161] - loss: 0.000338 
Batch[8162] - loss: 0.000376 
Batch[8163] - loss: 0.000418 
Batch[8164] - loss: 0.001093 
Batch[8165] - loss: 0.000365 
Batch[8166] - loss: 0.000446 
Batch[8167] - loss: 0.000197 
Batch[8168] - loss: 0.000511 
Batch[8169] - loss: 0.000546 
Batch[8170] - loss: 0.000270 
Batch[8171] - loss: 0.000733 
Batch[8172] - loss: 0.000349 
Batch[8173] - loss: 0.000569 
Batch[8174] - loss: 0.000501 
Batch[8175] - loss: 0.000359 
Batch[8176] - loss: 0.000572 
Batch[8177] - loss: 0.000375 
Batch[8178] - loss: 0.000660 
Batch[8179] - loss: 0.000314 
Batch[8180] - loss: 0.000314 
Batch[8181] - loss: 0.000425 
Batch[8182] - loss: 0.000377 
Batch[8183] - loss: 0.000335 
Batch[8184] - loss: 0.000621 
Batch[8185] - loss: 0.001046 
Batch[8186] - loss: 0.000633 
Batch[8187] - loss: 0.000326 
Batch[8188] - loss: 0.000519 
Batch[8189] - loss: 0.000700 
Batch[8190] - loss: 0.000377 
Batch[8191] - loss: 0.000433 
Batch[8192] - loss: 0.000382 
Batch[8193] - loss: 0.000335 
Batch[8194] - loss: 0.000521 
Batch[8195] - loss: 0.000629 
Batch[8196] - loss: 0.000651 
Batch[8197] - loss: 0.000684 
Batch[8198] - loss: 0.000363 
Batch[8199] - loss: 0.000467 
Batch[8200] - loss: 0.000337 

Evaluation - loss: 0.000069 pearson: 0.5538 

early stop by 1500 steps.
Batch[8201] - loss: 0.000410 
Batch[8202] - loss: 0.000375 
Batch[8203] - loss: 0.000414 
Batch[8204] - loss: 0.000451 
Batch[8205] - loss: 0.000390 
Batch[8206] - loss: 0.000515 
Batch[8207] - loss: 0.000612 
Batch[8208] - loss: 0.000329 
Batch[8209] - loss: 0.000728 
Batch[8210] - loss: 0.000471 
Batch[8211] - loss: 0.000314 
Batch[8212] - loss: 0.000236 
Batch[8213] - loss: 0.000849 
Batch[8214] - loss: 0.000248 
Batch[8215] - loss: 0.000443 
Batch[8216] - loss: 0.000325 
Batch[8217] - loss: 0.000304 
Batch[8218] - loss: 0.000454 
Batch[8219] - loss: 0.000355 
Batch[8220] - loss: 0.000407 
Batch[8221] - loss: 0.000494 
Batch[8222] - loss: 0.000447 
Batch[8223] - loss: 0.000527 
Batch[8224] - loss: 0.000537 
Batch[8225] - loss: 0.000525 
Batch[8226] - loss: 0.000530 
Batch[8227] - loss: 0.000654 
Batch[8228] - loss: 0.000394 
Batch[8229] - loss: 0.000839 
Batch[8230] - loss: 0.000433 
Batch[8231] - loss: 0.000691 
Batch[8232] - loss: 0.000582 
Batch[8233] - loss: 0.000560 
Batch[8234] - loss: 0.000511 
Batch[8235] - loss: 0.000326 
Batch[8236] - loss: 0.000395 
Batch[8237] - loss: 0.000439 
Batch[8238] - loss: 0.000587 
Batch[8239] - loss: 0.000675 
Batch[8240] - loss: 0.000457 
Batch[8241] - loss: 0.000288 
Batch[8242] - loss: 0.000453 
Batch[8243] - loss: 0.000869 
Batch[8244] - loss: 0.000625 
Batch[8245] - loss: 0.000449 
Batch[8246] - loss: 0.000436 
Batch[8247] - loss: 0.000368 
Batch[8248] - loss: 0.000780 
Batch[8249] - loss: 0.000349 
Batch[8250] - loss: 0.000653 
Batch[8251] - loss: 0.000253 
Batch[8252] - loss: 0.000437 
Batch[8253] - loss: 0.000508 
Batch[8254] - loss: 0.000397 
Batch[8255] - loss: 0.000719 
Batch[8256] - loss: 0.000729 
Batch[8257] - loss: 0.000381 
Batch[8258] - loss: 0.000493 
Batch[8259] - loss: 0.000704 
Batch[8260] - loss: 0.000505 
Batch[8261] - loss: 0.000570 
Batch[8262] - loss: 0.000437 
Batch[8263] - loss: 0.000308 
Batch[8264] - loss: 0.000516 
Batch[8265] - loss: 0.000529 
Batch[8266] - loss: 0.000221 
Batch[8267] - loss: 0.000452 
Batch[8268] - loss: 0.000259 
Batch[8269] - loss: 0.000277 
Batch[8270] - loss: 0.000471 
Batch[8271] - loss: 0.000450 
Batch[8272] - loss: 0.000445 
Batch[8273] - loss: 0.000393 
Batch[8274] - loss: 0.000483 
Batch[8275] - loss: 0.000461 
Batch[8276] - loss: 0.000720 
Batch[8277] - loss: 0.000437 
Batch[8278] - loss: 0.000342 
Batch[8279] - loss: 0.000270 
Batch[8280] - loss: 0.000452 
Batch[8281] - loss: 0.000292 
Batch[8282] - loss: 0.000266 
Batch[8283] - loss: 0.000266 
Batch[8284] - loss: 0.000367 
Batch[8285] - loss: 0.000507 
Batch[8286] - loss: 0.000502 
Batch[8287] - loss: 0.000684 
Batch[8288] - loss: 0.000340 
Batch[8289] - loss: 0.000432 
Batch[8290] - loss: 0.000301 
Batch[8291] - loss: 0.000442 
Batch[8292] - loss: 0.000287 
Batch[8293] - loss: 0.000426 
Batch[8294] - loss: 0.000517 
Batch[8295] - loss: 0.000306 
Batch[8296] - loss: 0.000536 
Batch[8297] - loss: 0.000431 
Batch[8298] - loss: 0.000298 
Batch[8299] - loss: 0.000735 
Batch[8300] - loss: 0.000722 

Evaluation - loss: 0.000069 pearson: 0.5558 

early stop by 1500 steps.
Batch[8301] - loss: 0.000573 
Batch[8302] - loss: 0.000452 
Batch[8303] - loss: 0.000613 
Batch[8304] - loss: 0.000560 
Batch[8305] - loss: 0.000448 
Batch[8306] - loss: 0.000361 
Batch[8307] - loss: 0.000702 
Batch[8308] - loss: 0.000867 
Batch[8309] - loss: 0.000432 
Batch[8310] - loss: 0.000334 
Batch[8311] - loss: 0.000508 
Batch[8312] - loss: 0.000365 
Batch[8313] - loss: 0.000344 
Batch[8314] - loss: 0.000383 
Batch[8315] - loss: 0.000480 
Batch[8316] - loss: 0.000548 
Batch[8317] - loss: 0.000433 
Batch[8318] - loss: 0.000842 
Batch[8319] - loss: 0.000595 
Batch[8320] - loss: 0.000551 
Batch[8321] - loss: 0.000703 
Batch[8322] - loss: 0.000273 
Batch[8323] - loss: 0.000550 
Batch[8324] - loss: 0.000534 
Batch[8325] - loss: 0.000422 
Batch[8326] - loss: 0.000562 
Batch[8327] - loss: 0.000410 
Batch[8328] - loss: 0.000617 
Batch[8329] - loss: 0.000468 
Batch[8330] - loss: 0.000815 
Batch[8331] - loss: 0.000781 
Batch[8332] - loss: 0.000783 
Batch[8333] - loss: 0.000527 
Batch[8334] - loss: 0.000313 
Batch[8335] - loss: 0.000857 
Batch[8336] - loss: 0.000366 
Batch[8337] - loss: 0.000451 
Batch[8338] - loss: 0.000486 
Batch[8339] - loss: 0.000400 
Batch[8340] - loss: 0.000821 
Batch[8341] - loss: 0.000283 
Batch[8342] - loss: 0.000490 
Batch[8343] - loss: 0.000607 
Batch[8344] - loss: 0.000517 
Batch[8345] - loss: 0.000584 
Batch[8346] - loss: 0.000633 
Batch[8347] - loss: 0.000474 
Batch[8348] - loss: 0.000295 
Batch[8349] - loss: 0.000235 
Batch[8350] - loss: 0.000360 
Batch[8351] - loss: 0.000555 
Batch[8352] - loss: 0.000416 
Batch[8353] - loss: 0.000362 
Batch[8354] - loss: 0.000505 
Batch[8355] - loss: 0.000369 
Batch[8356] - loss: 0.000413 
Batch[8357] - loss: 0.000527 
Batch[8358] - loss: 0.000744 
Batch[8359] - loss: 0.000801 
Batch[8360] - loss: 0.000295 
Batch[8361] - loss: 0.000649 
Batch[8362] - loss: 0.000566 
Batch[8363] - loss: 0.000528 
Batch[8364] - loss: 0.000297 
Batch[8365] - loss: 0.000652 
Batch[8366] - loss: 0.000489 
Batch[8367] - loss: 0.000801 
Batch[8368] - loss: 0.000319 
Batch[8369] - loss: 0.000794 
Batch[8370] - loss: 0.000434 
Batch[8371] - loss: 0.000428 
Batch[8372] - loss: 0.000477 
Batch[8373] - loss: 0.000440 
Batch[8374] - loss: 0.000659 
Batch[8375] - loss: 0.000509 
Batch[8376] - loss: 0.000565 
Batch[8377] - loss: 0.000501 
Batch[8378] - loss: 0.000499 
Batch[8379] - loss: 0.000492 
Batch[8380] - loss: 0.000680 
Batch[8381] - loss: 0.000520 
Batch[8382] - loss: 0.000692 
Batch[8383] - loss: 0.000511 
Batch[8384] - loss: 0.000571 
Batch[8385] - loss: 0.000732 
Batch[8386] - loss: 0.000605 
Batch[8387] - loss: 0.000601 
Batch[8388] - loss: 0.000497 
Batch[8389] - loss: 0.000454 
Batch[8390] - loss: 0.000558 
Batch[8391] - loss: 0.000394 
Batch[8392] - loss: 0.000609 
Batch[8393] - loss: 0.000364 
Batch[8394] - loss: 0.000362 
Batch[8395] - loss: 0.000453 
Batch[8396] - loss: 0.000624 
Batch[8397] - loss: 0.000481 
Batch[8398] - loss: 0.001598 
Batch[8399] - loss: 0.000779 
Batch[8400] - loss: 0.000467 

Evaluation - loss: 0.000069 pearson: 0.5575 

early stop by 1500 steps.
Batch[8401] - loss: 0.000251 
Batch[8402] - loss: 0.000461 
Batch[8403] - loss: 0.000744 
Batch[8404] - loss: 0.000391 
Batch[8405] - loss: 0.000419 
Batch[8406] - loss: 0.000635 
Batch[8407] - loss: 0.000490 
Batch[8408] - loss: 0.000516 
Batch[8409] - loss: 0.000338 
Batch[8410] - loss: 0.000517 
Batch[8411] - loss: 0.000258 
Batch[8412] - loss: 0.000590 
Batch[8413] - loss: 0.000475 
Batch[8414] - loss: 0.000467 
Batch[8415] - loss: 0.000478 
Batch[8416] - loss: 0.000636 
Batch[8417] - loss: 0.000290 
Batch[8418] - loss: 0.000298 
Batch[8419] - loss: 0.000449 
Batch[8420] - loss: 0.000490 
Batch[8421] - loss: 0.000226 
Batch[8422] - loss: 0.000468 
Batch[8423] - loss: 0.000349 
Batch[8424] - loss: 0.000408 
Batch[8425] - loss: 0.000525 
Batch[8426] - loss: 0.000530 
Batch[8427] - loss: 0.000854 
Batch[8428] - loss: 0.000345 
Batch[8429] - loss: 0.000486 
Batch[8430] - loss: 0.000529 
Batch[8431] - loss: 0.000316 
Batch[8432] - loss: 0.000621 
Batch[8433] - loss: 0.000276 
Batch[8434] - loss: 0.000594 
Batch[8435] - loss: 0.000439 
Batch[8436] - loss: 0.000461 
Batch[8437] - loss: 0.000757 
Batch[8438] - loss: 0.000339 
Batch[8439] - loss: 0.000604 
Batch[8440] - loss: 0.000295 
Batch[8441] - loss: 0.000357 
Batch[8442] - loss: 0.000848 
Batch[8443] - loss: 0.000622 
Batch[8444] - loss: 0.000530 
Batch[8445] - loss: 0.000511 
Batch[8446] - loss: 0.000386 
Batch[8447] - loss: 0.000448 
Batch[8448] - loss: 0.000572 
Batch[8449] - loss: 0.000617 
Batch[8450] - loss: 0.000496 
Batch[8451] - loss: 0.000435 
Batch[8452] - loss: 0.000351 
Batch[8453] - loss: 0.000673 
Batch[8454] - loss: 0.000544 
Batch[8455] - loss: 0.000474 
Batch[8456] - loss: 0.000546 
Batch[8457] - loss: 0.000414 
Batch[8458] - loss: 0.000489 
Batch[8459] - loss: 0.000466 
Batch[8460] - loss: 0.000589 
Batch[8461] - loss: 0.000408 
Batch[8462] - loss: 0.000447 
Batch[8463] - loss: 0.000430 
Batch[8464] - loss: 0.000450 
Batch[8465] - loss: 0.000396 
Batch[8466] - loss: 0.000332 
Batch[8467] - loss: 0.000334 
Batch[8468] - loss: 0.000520 
Batch[8469] - loss: 0.000672 
Batch[8470] - loss: 0.000470 
Batch[8471] - loss: 0.000380 
Batch[8472] - loss: 0.000808 
Batch[8473] - loss: 0.000274 
Batch[8474] - loss: 0.000649 
Batch[8475] - loss: 0.000397 
Batch[8476] - loss: 0.000497 
Batch[8477] - loss: 0.000430 
Batch[8478] - loss: 0.000423 
Batch[8479] - loss: 0.000983 
Batch[8480] - loss: 0.000514 
Batch[8481] - loss: 0.000368 
Batch[8482] - loss: 0.000359 
Batch[8483] - loss: 0.000492 
Batch[8484] - loss: 0.000717 
Batch[8485] - loss: 0.000621 
Batch[8486] - loss: 0.000472 
Batch[8487] - loss: 0.000247 
Batch[8488] - loss: 0.000382 
Batch[8489] - loss: 0.000235 
Batch[8490] - loss: 0.000552 
Batch[8491] - loss: 0.000709 
Batch[8492] - loss: 0.000473 
Batch[8493] - loss: 0.000415 
Batch[8494] - loss: 0.000563 
Batch[8495] - loss: 0.000423 
Batch[8496] - loss: 0.000352 
Batch[8497] - loss: 0.000527 
Batch[8498] - loss: 0.000832 
Batch[8499] - loss: 0.000339 
Batch[8500] - loss: 0.000453 

Evaluation - loss: 0.000069 pearson: 0.5541 

early stop by 1500 steps.
Batch[8501] - loss: 0.000282 
Batch[8502] - loss: 0.000564 
Batch[8503] - loss: 0.000659 
Batch[8504] - loss: 0.000545 
Batch[8505] - loss: 0.000411 
Batch[8506] - loss: 0.000426 
Batch[8507] - loss: 0.000622 
Batch[8508] - loss: 0.000596 
Batch[8509] - loss: 0.000494 
Batch[8510] - loss: 0.000386 
Batch[8511] - loss: 0.000478 
Batch[8512] - loss: 0.000299 
Batch[8513] - loss: 0.000339 
Batch[8514] - loss: 0.000439 
Batch[8515] - loss: 0.000838 
Batch[8516] - loss: 0.000390 
Batch[8517] - loss: 0.000668 
Batch[8518] - loss: 0.000269 
Batch[8519] - loss: 0.000453 
Batch[8520] - loss: 0.000579 
Batch[8521] - loss: 0.000409 
Batch[8522] - loss: 0.000682 
Batch[8523] - loss: 0.000659 
Batch[8524] - loss: 0.000409 
Batch[8525] - loss: 0.000278 
Batch[8526] - loss: 0.000370 
Batch[8527] - loss: 0.000438 
Batch[8528] - loss: 0.000456 
Batch[8529] - loss: 0.000877 
Batch[8530] - loss: 0.000392 
Batch[8531] - loss: 0.000246 
Batch[8532] - loss: 0.000615 
Batch[8533] - loss: 0.000521 
Batch[8534] - loss: 0.000540 
Batch[8535] - loss: 0.000611 
Batch[8536] - loss: 0.000454 
Batch[8537] - loss: 0.000344 
Batch[8538] - loss: 0.000303 
Batch[8539] - loss: 0.000652 
Batch[8540] - loss: 0.000432 
Batch[8541] - loss: 0.000342 
Batch[8542] - loss: 0.000420 
Batch[8543] - loss: 0.000671 
Batch[8544] - loss: 0.000386 
Batch[8545] - loss: 0.000360 
Batch[8546] - loss: 0.000316 
Batch[8547] - loss: 0.000685 
Batch[8548] - loss: 0.000302 
Batch[8549] - loss: 0.000525 
Batch[8550] - loss: 0.000420 
Batch[8551] - loss: 0.000398 
Batch[8552] - loss: 0.000242 
Batch[8553] - loss: 0.000280 
Batch[8554] - loss: 0.000474 
Batch[8555] - loss: 0.000698 
Batch[8556] - loss: 0.000227 
Batch[8557] - loss: 0.000398 
Batch[8558] - loss: 0.000675 
Batch[8559] - loss: 0.000302 
Batch[8560] - loss: 0.000370 
Batch[8561] - loss: 0.000281 
Batch[8562] - loss: 0.000703 
Batch[8563] - loss: 0.000613 
Batch[8564] - loss: 0.000294 
Batch[8565] - loss: 0.000613 
Batch[8566] - loss: 0.000357 
Batch[8567] - loss: 0.000783 
Batch[8568] - loss: 0.000426 
Batch[8569] - loss: 0.000364 
Batch[8570] - loss: 0.000320 
Batch[8571] - loss: 0.000451 
Batch[8572] - loss: 0.000390 
Batch[8573] - loss: 0.000419 
Batch[8574] - loss: 0.000363 
Batch[8575] - loss: 0.000421 
Batch[8576] - loss: 0.000516 
Batch[8577] - loss: 0.000485 
Batch[8578] - loss: 0.000522 
Batch[8579] - loss: 0.000489 
Batch[8580] - loss: 0.000275 
Batch[8581] - loss: 0.000330 
Batch[8582] - loss: 0.000629 
Batch[8583] - loss: 0.000544 
Batch[8584] - loss: 0.000569 
Batch[8585] - loss: 0.000309 
Batch[8586] - loss: 0.000381 
Batch[8587] - loss: 0.000274 
Batch[8588] - loss: 0.000605 
Batch[8589] - loss: 0.000458 
Batch[8590] - loss: 0.000415 
Batch[8591] - loss: 0.000485 
Batch[8592] - loss: 0.000808 
Batch[8593] - loss: 0.000222 
Batch[8594] - loss: 0.000240 
Batch[8595] - loss: 0.000247 
Batch[8596] - loss: 0.000366 
Batch[8597] - loss: 0.000565 
Batch[8598] - loss: 0.000575 
Batch[8599] - loss: 0.000612 
Batch[8600] - loss: 0.000302 

Evaluation - loss: 0.000070 pearson: 0.5521 

early stop by 1500 steps.
Batch[8601] - loss: 0.000795 
Batch[8602] - loss: 0.000739 
Batch[8603] - loss: 0.000642 
Batch[8604] - loss: 0.000608 
Batch[8605] - loss: 0.000584 
Batch[8606] - loss: 0.000394 
Batch[8607] - loss: 0.000641 
Batch[8608] - loss: 0.000450 
Batch[8609] - loss: 0.000409 
Batch[8610] - loss: 0.000471 
Batch[8611] - loss: 0.000367 
Batch[8612] - loss: 0.000298 
Batch[8613] - loss: 0.000377 
Batch[8614] - loss: 0.000281 
Batch[8615] - loss: 0.000694 
Batch[8616] - loss: 0.000531 
Batch[8617] - loss: 0.000357 
Batch[8618] - loss: 0.000610 
Batch[8619] - loss: 0.000406 
Batch[8620] - loss: 0.000193 
Batch[8621] - loss: 0.000620 
Batch[8622] - loss: 0.000476 
Batch[8623] - loss: 0.000314 
Batch[8624] - loss: 0.000419 
Batch[8625] - loss: 0.000435 
Batch[8626] - loss: 0.000502 
Batch[8627] - loss: 0.000383 
Batch[8628] - loss: 0.000502 
Batch[8629] - loss: 0.000346 
Batch[8630] - loss: 0.000302 
Batch[8631] - loss: 0.000426 
Batch[8632] - loss: 0.000455 
Batch[8633] - loss: 0.000479 
Batch[8634] - loss: 0.000376 
Batch[8635] - loss: 0.000407 
Batch[8636] - loss: 0.000268 
Batch[8637] - loss: 0.000581 
Batch[8638] - loss: 0.000323 
Batch[8639] - loss: 0.000570 
Batch[8640] - loss: 0.000611 
Batch[8641] - loss: 0.000434 
Batch[8642] - loss: 0.000632 
Batch[8643] - loss: 0.000349 
Batch[8644] - loss: 0.000396 
Batch[8645] - loss: 0.000418 
Batch[8646] - loss: 0.000250 
Batch[8647] - loss: 0.000672 
Batch[8648] - loss: 0.000317 
Batch[8649] - loss: 0.000550 
Batch[8650] - loss: 0.000441 
Batch[8651] - loss: 0.000519 
Batch[8652] - loss: 0.000581 
Batch[8653] - loss: 0.000605 
Batch[8654] - loss: 0.000701 
Batch[8655] - loss: 0.000499 
Batch[8656] - loss: 0.000572 
Batch[8657] - loss: 0.000201 
Batch[8658] - loss: 0.000258 
Batch[8659] - loss: 0.000601 
Batch[8660] - loss: 0.000446 
Batch[8661] - loss: 0.000575 
Batch[8662] - loss: 0.000431 
Batch[8663] - loss: 0.000598 
Batch[8664] - loss: 0.000473 
Batch[8665] - loss: 0.000604 
Batch[8666] - loss: 0.000428 
Batch[8667] - loss: 0.000377 
Batch[8668] - loss: 0.000489 
Batch[8669] - loss: 0.000707 
Batch[8670] - loss: 0.000525 
Batch[8671] - loss: 0.000488 
Batch[8672] - loss: 0.000517 
Batch[8673] - loss: 0.000298 
Batch[8674] - loss: 0.000377 
Batch[8675] - loss: 0.000580 
Batch[8676] - loss: 0.000309 
Batch[8677] - loss: 0.000298 
Batch[8678] - loss: 0.000397 
Batch[8679] - loss: 0.000860 
Batch[8680] - loss: 0.000427 
Batch[8681] - loss: 0.000650 
Batch[8682] - loss: 0.000416 
Batch[8683] - loss: 0.000779 
Batch[8684] - loss: 0.000387 
Batch[8685] - loss: 0.000443 
Batch[8686] - loss: 0.000330 
Batch[8687] - loss: 0.000631 
Batch[8688] - loss: 0.000583 
Batch[8689] - loss: 0.000487 
Batch[8690] - loss: 0.000329 
Batch[8691] - loss: 0.000331 
Batch[8692] - loss: 0.000360 
Batch[8693] - loss: 0.000438 
Batch[8694] - loss: 0.000336 
Batch[8695] - loss: 0.000527 
Batch[8696] - loss: 0.000381 
Batch[8697] - loss: 0.000384 
Batch[8698] - loss: 0.000392 
Batch[8699] - loss: 0.000466 
Batch[8700] - loss: 0.000275 

Evaluation - loss: 0.000069 pearson: 0.5549 

early stop by 1500 steps.
Batch[8701] - loss: 0.000333 
Batch[8702] - loss: 0.000222 
Batch[8703] - loss: 0.000381 
Batch[8704] - loss: 0.000301 
Batch[8705] - loss: 0.000439 
Batch[8706] - loss: 0.000665 
Batch[8707] - loss: 0.000223 
Batch[8708] - loss: 0.000566 
Batch[8709] - loss: 0.000344 
Batch[8710] - loss: 0.000629 
Batch[8711] - loss: 0.000678 
Batch[8712] - loss: 0.000622 
Batch[8713] - loss: 0.000492 
Batch[8714] - loss: 0.000461 
Batch[8715] - loss: 0.000469 
Batch[8716] - loss: 0.000579 
Batch[8717] - loss: 0.000617 
Batch[8718] - loss: 0.000902 
Batch[8719] - loss: 0.001347 
Batch[8720] - loss: 0.000261 
Batch[8721] - loss: 0.000346 
Batch[8722] - loss: 0.000461 
Batch[8723] - loss: 0.000745 
Batch[8724] - loss: 0.000488 
Batch[8725] - loss: 0.000383 
Batch[8726] - loss: 0.000599 
Batch[8727] - loss: 0.000451 
Batch[8728] - loss: 0.000899 
Batch[8729] - loss: 0.000779 
Batch[8730] - loss: 0.000517 
Batch[8731] - loss: 0.000503 
Batch[8732] - loss: 0.000517 
Batch[8733] - loss: 0.000402 
Batch[8734] - loss: 0.000397 
Batch[8735] - loss: 0.000315 
Batch[8736] - loss: 0.000261 
Batch[8737] - loss: 0.000573 
Batch[8738] - loss: 0.000760 
Batch[8739] - loss: 0.000267 
Batch[8740] - loss: 0.000344 
Batch[8741] - loss: 0.000520 
Batch[8742] - loss: 0.000310 
Batch[8743] - loss: 0.000354 
Batch[8744] - loss: 0.000550 
Batch[8745] - loss: 0.000610 
Batch[8746] - loss: 0.000335 
Batch[8747] - loss: 0.000488 
Batch[8748] - loss: 0.000753 
Batch[8749] - loss: 0.000470 
Batch[8750] - loss: 0.000305 
Batch[8751] - loss: 0.000394 
Batch[8752] - loss: 0.000376 
Batch[8753] - loss: 0.001013 
Batch[8754] - loss: 0.000309 
Batch[8755] - loss: 0.000547 
Batch[8756] - loss: 0.000546 
Batch[8757] - loss: 0.000371 
Batch[8758] - loss: 0.000626 
Batch[8759] - loss: 0.000507 
Batch[8760] - loss: 0.000371 
Batch[8761] - loss: 0.000378 
Batch[8762] - loss: 0.000225 
Batch[8763] - loss: 0.000399 
Batch[8764] - loss: 0.000151 
Batch[8765] - loss: 0.000650 
Batch[8766] - loss: 0.000358 
Batch[8767] - loss: 0.000407 
Batch[8768] - loss: 0.000336 
Batch[8769] - loss: 0.000323 
Batch[8770] - loss: 0.000349 
Batch[8771] - loss: 0.000494 
Batch[8772] - loss: 0.000741 
Batch[8773] - loss: 0.000315 
Batch[8774] - loss: 0.000278 
Batch[8775] - loss: 0.000393 
Batch[8776] - loss: 0.000447 
Batch[8777] - loss: 0.000428 
Batch[8778] - loss: 0.000591 
Batch[8779] - loss: 0.000408 
Batch[8780] - loss: 0.000437 
Batch[8781] - loss: 0.000597 
Batch[8782] - loss: 0.000697 
Batch[8783] - loss: 0.000518 
Batch[8784] - loss: 0.000711 
Batch[8785] - loss: 0.000527 
Batch[8786] - loss: 0.000671 
Batch[8787] - loss: 0.000628 
Batch[8788] - loss: 0.000299 
Batch[8789] - loss: 0.000508 
Batch[8790] - loss: 0.000470 
Batch[8791] - loss: 0.000489 
Batch[8792] - loss: 0.000586 
Batch[8793] - loss: 0.000211 
Batch[8794] - loss: 0.000417 
Batch[8795] - loss: 0.000958 
Batch[8796] - loss: 0.000409 
Batch[8797] - loss: 0.000542 
Batch[8798] - loss: 0.000327 
Batch[8799] - loss: 0.000527 
Batch[8800] - loss: 0.000337 

Evaluation - loss: 0.000069 pearson: 0.5555 

early stop by 1500 steps.
Batch[8801] - loss: 0.000354 
Batch[8802] - loss: 0.000512 
Batch[8803] - loss: 0.000679 
Batch[8804] - loss: 0.000279 
Batch[8805] - loss: 0.000482 
Batch[8806] - loss: 0.000778 
Batch[8807] - loss: 0.000592 
Batch[8808] - loss: 0.000483 
Batch[8809] - loss: 0.000321 
Batch[8810] - loss: 0.000996 
Batch[8811] - loss: 0.000443 
Batch[8812] - loss: 0.000452 
Batch[8813] - loss: 0.000694 
Batch[8814] - loss: 0.000473 
Batch[8815] - loss: 0.000466 
Batch[8816] - loss: 0.000454 
Batch[8817] - loss: 0.000374 
Batch[8818] - loss: 0.000943 
Batch[8819] - loss: 0.000304 
Batch[8820] - loss: 0.000489 
Batch[8821] - loss: 0.000447 
Batch[8822] - loss: 0.000450 
Batch[8823] - loss: 0.000472 
Batch[8824] - loss: 0.000355 
Batch[8825] - loss: 0.000354 
Batch[8826] - loss: 0.000598 
Batch[8827] - loss: 0.000568 
Batch[8828] - loss: 0.000304 
Batch[8829] - loss: 0.000360 
Batch[8830] - loss: 0.000430 
Batch[8831] - loss: 0.000257 
Batch[8832] - loss: 0.000450 
Batch[8833] - loss: 0.000458 
Batch[8834] - loss: 0.000239 
Batch[8835] - loss: 0.000709 
Batch[8836] - loss: 0.000542 
Batch[8837] - loss: 0.000261 
Batch[8838] - loss: 0.000415 
Batch[8839] - loss: 0.000627 
Batch[8840] - loss: 0.000705 
Batch[8841] - loss: 0.000573 
Batch[8842] - loss: 0.000445 
Batch[8843] - loss: 0.000643 
Batch[8844] - loss: 0.000504 
Batch[8845] - loss: 0.000517 
Batch[8846] - loss: 0.000770 
Batch[8847] - loss: 0.000476 
Batch[8848] - loss: 0.000239 
Batch[8849] - loss: 0.000386 
Batch[8850] - loss: 0.000528 
Batch[8851] - loss: 0.000370 
Batch[8852] - loss: 0.000677 
Batch[8853] - loss: 0.001258 
Batch[8854] - loss: 0.000297 
Batch[8855] - loss: 0.000644 
Batch[8856] - loss: 0.000798 
Batch[8857] - loss: 0.000879 
Batch[8858] - loss: 0.000428 
Batch[8859] - loss: 0.000462 
Batch[8860] - loss: 0.000624 
Batch[8861] - loss: 0.000496 
Batch[8862] - loss: 0.000640 
Batch[8863] - loss: 0.000467 
Batch[8864] - loss: 0.000389 
Batch[8865] - loss: 0.000697 
Batch[8866] - loss: 0.000475 
Batch[8867] - loss: 0.000369 
Batch[8868] - loss: 0.000383 
Batch[8869] - loss: 0.000410 
Batch[8870] - loss: 0.000357 
Batch[8871] - loss: 0.000566 
Batch[8872] - loss: 0.000516 
Batch[8873] - loss: 0.000284 
Batch[8874] - loss: 0.000621 
Batch[8875] - loss: 0.000511 
Batch[8876] - loss: 0.000630 
Batch[8877] - loss: 0.000554 
Batch[8878] - loss: 0.000494 
Batch[8879] - loss: 0.000659 
Batch[8880] - loss: 0.000282 
Batch[8881] - loss: 0.000363 
Batch[8882] - loss: 0.000341 
Batch[8883] - loss: 0.000432 
Batch[8884] - loss: 0.000317 
Batch[8885] - loss: 0.000344 
Batch[8886] - loss: 0.000469 
Batch[8887] - loss: 0.000444 
Batch[8888] - loss: 0.000253 
Batch[8889] - loss: 0.000403 
Batch[8890] - loss: 0.000311 
Batch[8891] - loss: 0.000507 
Batch[8892] - loss: 0.000448 
Batch[8893] - loss: 0.000723 
Batch[8894] - loss: 0.000495 
Batch[8895] - loss: 0.000598 
Batch[8896] - loss: 0.000360 
Batch[8897] - loss: 0.000366 
Batch[8898] - loss: 0.000505 
Batch[8899] - loss: 0.000596 
Batch[8900] - loss: 0.000470 

Evaluation - loss: 0.000069 pearson: 0.5581 

early stop by 1500 steps.
Batch[8901] - loss: 0.000423 
Batch[8902] - loss: 0.000351 
Batch[8903] - loss: 0.000505 
Batch[8904] - loss: 0.000516 
Batch[8905] - loss: 0.000554 
Batch[8906] - loss: 0.000427 
Batch[8907] - loss: 0.000697 
Batch[8908] - loss: 0.000779 
Batch[8909] - loss: 0.000490 
Batch[8910] - loss: 0.000646 
Batch[8911] - loss: 0.000572 
Batch[8912] - loss: 0.000547 
Batch[8913] - loss: 0.000383 
Batch[8914] - loss: 0.000345 
Batch[8915] - loss: 0.000366 
Batch[8916] - loss: 0.000373 
Batch[8917] - loss: 0.000521 
Batch[8918] - loss: 0.000720 
Batch[8919] - loss: 0.000380 
Batch[8920] - loss: 0.000320 
Batch[8921] - loss: 0.000370 
Batch[8922] - loss: 0.000490 
Batch[8923] - loss: 0.000667 
Batch[8924] - loss: 0.000526 
Batch[8925] - loss: 0.000504 
Batch[8926] - loss: 0.001128 
Batch[8927] - loss: 0.000556 
Batch[8928] - loss: 0.000901 
Batch[8929] - loss: 0.000470 
Batch[8930] - loss: 0.000494 
Batch[8931] - loss: 0.000396 
Batch[8932] - loss: 0.000265 
Batch[8933] - loss: 0.000421 
Batch[8934] - loss: 0.000576 
Batch[8935] - loss: 0.000510 
Batch[8936] - loss: 0.000298 
Batch[8937] - loss: 0.000467 
Batch[8938] - loss: 0.000382 
Batch[8939] - loss: 0.000669 
Batch[8940] - loss: 0.000641 
Batch[8941] - loss: 0.000464 
Batch[8942] - loss: 0.000525 
Batch[8943] - loss: 0.000393 
Batch[8944] - loss: 0.000350 
Batch[8945] - loss: 0.000205 
Batch[8946] - loss: 0.000524 
Batch[8947] - loss: 0.000377 
Batch[8948] - loss: 0.000478 
Batch[8949] - loss: 0.000443 
Batch[8950] - loss: 0.000315 
Batch[8951] - loss: 0.000250 
Batch[8952] - loss: 0.000411 
Batch[8953] - loss: 0.000873 
Batch[8954] - loss: 0.000329 
Batch[8955] - loss: 0.000492 
Batch[8956] - loss: 0.000596 
Batch[8957] - loss: 0.000575 
Batch[8958] - loss: 0.000381 
Batch[8959] - loss: 0.000786 
Batch[8960] - loss: 0.000446 
Batch[8961] - loss: 0.000273 
Batch[8962] - loss: 0.000498 
Batch[8963] - loss: 0.000369 
Batch[8964] - loss: 0.000446 
Batch[8965] - loss: 0.000309 
Batch[8966] - loss: 0.000362 
Batch[8967] - loss: 0.000369 
Batch[8968] - loss: 0.000478 
Batch[8969] - loss: 0.000495 
Batch[8970] - loss: 0.000825 
Batch[8971] - loss: 0.000338 
Batch[8972] - loss: 0.000354 
Batch[8973] - loss: 0.000428 
Batch[8974] - loss: 0.000476 
Batch[8975] - loss: 0.000639 
Batch[8976] - loss: 0.000525 
Batch[8977] - loss: 0.000390 
Batch[8978] - loss: 0.000418 
Batch[8979] - loss: 0.000699 
Batch[8980] - loss: 0.000598 
Batch[8981] - loss: 0.000355 
Batch[8982] - loss: 0.000460 
Batch[8983] - loss: 0.000335 
Batch[8984] - loss: 0.000498 
Batch[8985] - loss: 0.000646 
Batch[8986] - loss: 0.000601 
Batch[8987] - loss: 0.000459 
Batch[8988] - loss: 0.000293 
Batch[8989] - loss: 0.000714 
Batch[8990] - loss: 0.000444 
Batch[8991] - loss: 0.000308 
Batch[8992] - loss: 0.000650 
Batch[8993] - loss: 0.000630 
Batch[8994] - loss: 0.000717 
Batch[8995] - loss: 0.000465 
Batch[8996] - loss: 0.000609 
Batch[8997] - loss: 0.000439 
Batch[8998] - loss: 0.000307 
Batch[8999] - loss: 0.000349 
Batch[9000] - loss: 0.000872 

Evaluation - loss: 0.000070 pearson: 0.5554 

early stop by 1500 steps.
Batch[9001] - loss: 0.000415 
Batch[9002] - loss: 0.000502 
Batch[9003] - loss: 0.000589 
Batch[9004] - loss: 0.000615 
Batch[9005] - loss: 0.000319 
Batch[9006] - loss: 0.000490 
Batch[9007] - loss: 0.000456 
Batch[9008] - loss: 0.000513 
Batch[9009] - loss: 0.000382 
Batch[9010] - loss: 0.000746 
Batch[9011] - loss: 0.000378 
Batch[9012] - loss: 0.000470 
Batch[9013] - loss: 0.000903 
Batch[9014] - loss: 0.000335 
Batch[9015] - loss: 0.000304 
Batch[9016] - loss: 0.000427 
Batch[9017] - loss: 0.000608 
Batch[9018] - loss: 0.000450 
Batch[9019] - loss: 0.000526 
Batch[9020] - loss: 0.000412 
Batch[9021] - loss: 0.000400 
Batch[9022] - loss: 0.000309 
Batch[9023] - loss: 0.000417 
Batch[9024] - loss: 0.000432 
Batch[9025] - loss: 0.000760 
Batch[9026] - loss: 0.000384 
Batch[9027] - loss: 0.000325 
Batch[9028] - loss: 0.000577 
Batch[9029] - loss: 0.000375 
Batch[9030] - loss: 0.000247 
Batch[9031] - loss: 0.000384 
Batch[9032] - loss: 0.000249 
Batch[9033] - loss: 0.000581 
Batch[9034] - loss: 0.000313 
Batch[9035] - loss: 0.000354 
Batch[9036] - loss: 0.000573 
Batch[9037] - loss: 0.000244 
Batch[9038] - loss: 0.000421 
Batch[9039] - loss: 0.000494 
Batch[9040] - loss: 0.000219 
Batch[9041] - loss: 0.000358 
Batch[9042] - loss: 0.000313 
Batch[9043] - loss: 0.000325 
Batch[9044] - loss: 0.000320 
Batch[9045] - loss: 0.000336 
Batch[9046] - loss: 0.000408 
Batch[9047] - loss: 0.000676 
Batch[9048] - loss: 0.000429 
Batch[9049] - loss: 0.000325 
Batch[9050] - loss: 0.000349 
Batch[9051] - loss: 0.000424 
Batch[9052] - loss: 0.000652 
Batch[9053] - loss: 0.000442 
Batch[9054] - loss: 0.000783 
Batch[9055] - loss: 0.000451 
Batch[9056] - loss: 0.000299 
Batch[9057] - loss: 0.000672 
Batch[9058] - loss: 0.000584 
Batch[9059] - loss: 0.000409 
Batch[9060] - loss: 0.000495 
Batch[9061] - loss: 0.000581 
Batch[9062] - loss: 0.000747 
Batch[9063] - loss: 0.000314 
Batch[9064] - loss: 0.000435 
Batch[9065] - loss: 0.000379 
Batch[9066] - loss: 0.000607 
Batch[9067] - loss: 0.000311 
Batch[9068] - loss: 0.000527 
Batch[9069] - loss: 0.000756 
Batch[9070] - loss: 0.000498 
Batch[9071] - loss: 0.001081 
Batch[9072] - loss: 0.000317 
Batch[9073] - loss: 0.000252 
Batch[9074] - loss: 0.000478 
Batch[9075] - loss: 0.000409 
Batch[9076] - loss: 0.000389 
Batch[9077] - loss: 0.000633 
Batch[9078] - loss: 0.000471 
Batch[9079] - loss: 0.000411 
Batch[9080] - loss: 0.000511 
Batch[9081] - loss: 0.000466 
Batch[9082] - loss: 0.000484 
Batch[9083] - loss: 0.000565 
Batch[9084] - loss: 0.000391 
Batch[9085] - loss: 0.000316 
Batch[9086] - loss: 0.000563 
Batch[9087] - loss: 0.000447 
Batch[9088] - loss: 0.000338 
Batch[9089] - loss: 0.000450 
Batch[9090] - loss: 0.000607 
Batch[9091] - loss: 0.000279 
Batch[9092] - loss: 0.000480 
Batch[9093] - loss: 0.000391 
Batch[9094] - loss: 0.000450 
Batch[9095] - loss: 0.000305 
Batch[9096] - loss: 0.000300 
Batch[9097] - loss: 0.000331 
Batch[9098] - loss: 0.000372 
Batch[9099] - loss: 0.000638 
Batch[9100] - loss: 0.000526 

Evaluation - loss: 0.000069 pearson: 0.5552 

early stop by 1500 steps.
Batch[9101] - loss: 0.000701 
Batch[9102] - loss: 0.000256 
Batch[9103] - loss: 0.000642 
Batch[9104] - loss: 0.000361 
Batch[9105] - loss: 0.000841 
Batch[9106] - loss: 0.000264 
Batch[9107] - loss: 0.000736 
Batch[9108] - loss: 0.000219 
Batch[9109] - loss: 0.000319 
Batch[9110] - loss: 0.000424 
Batch[9111] - loss: 0.000367 
Batch[9112] - loss: 0.000746 
Batch[9113] - loss: 0.000219 
Batch[9114] - loss: 0.000316 
Batch[9115] - loss: 0.000376 
Batch[9116] - loss: 0.000518 
Batch[9117] - loss: 0.000265 
Batch[9118] - loss: 0.000387 
Batch[9119] - loss: 0.000421 
Batch[9120] - loss: 0.000513 
Batch[9121] - loss: 0.000382 
Batch[9122] - loss: 0.000392 
Batch[9123] - loss: 0.000298 
Batch[9124] - loss: 0.000359 
Batch[9125] - loss: 0.000399 
Batch[9126] - loss: 0.000473 
Batch[9127] - loss: 0.000518 
Batch[9128] - loss: 0.000158 
Batch[9129] - loss: 0.000550 
Batch[9130] - loss: 0.000554 
Batch[9131] - loss: 0.000312 
Batch[9132] - loss: 0.000778 
Batch[9133] - loss: 0.000333 
Batch[9134] - loss: 0.000477 
Batch[9135] - loss: 0.000423 
Batch[9136] - loss: 0.000331 
Batch[9137] - loss: 0.000305 
Batch[9138] - loss: 0.000403 
Batch[9139] - loss: 0.000371 
Batch[9140] - loss: 0.000376 
Batch[9141] - loss: 0.000586 
Batch[9142] - loss: 0.000360 
Batch[9143] - loss: 0.000440 
Batch[9144] - loss: 0.000270 
Batch[9145] - loss: 0.000650 
Batch[9146] - loss: 0.000319 
Batch[9147] - loss: 0.000348 
Batch[9148] - loss: 0.000489 
Batch[9149] - loss: 0.000351 
Batch[9150] - loss: 0.000524 
Batch[9151] - loss: 0.000837 
Batch[9152] - loss: 0.000721 
Batch[9153] - loss: 0.000482 
Batch[9154] - loss: 0.000512 
Batch[9155] - loss: 0.000371 
Batch[9156] - loss: 0.000583 
Batch[9157] - loss: 0.000419 
Batch[9158] - loss: 0.000393 
Batch[9159] - loss: 0.000544 
Batch[9160] - loss: 0.000905 
Batch[9161] - loss: 0.000450 
Batch[9162] - loss: 0.000371 
Batch[9163] - loss: 0.000375 
Batch[9164] - loss: 0.000641 
Batch[9165] - loss: 0.000297 
Batch[9166] - loss: 0.000496 
Batch[9167] - loss: 0.000249 
Batch[9168] - loss: 0.000565 
Batch[9169] - loss: 0.000479 
Batch[9170] - loss: 0.000390 
Batch[9171] - loss: 0.000876 
Batch[9172] - loss: 0.000466 
Batch[9173] - loss: 0.000512 
Batch[9174] - loss: 0.000422 
Batch[9175] - loss: 0.000305 
Batch[9176] - loss: 0.000261 
Batch[9177] - loss: 0.000490 
Batch[9178] - loss: 0.000295 
Batch[9179] - loss: 0.000271 
Batch[9180] - loss: 0.000270 
Batch[9181] - loss: 0.000180 
Batch[9182] - loss: 0.000332 
Batch[9183] - loss: 0.000308 
Batch[9184] - loss: 0.000287 
Batch[9185] - loss: 0.000241 
Batch[9186] - loss: 0.000233 
Batch[9187] - loss: 0.000339 
Batch[9188] - loss: 0.000673 
Batch[9189] - loss: 0.000323 
Batch[9190] - loss: 0.000689 
Batch[9191] - loss: 0.000487 
Batch[9192] - loss: 0.000297 
Batch[9193] - loss: 0.000420 
Batch[9194] - loss: 0.000979 
Batch[9195] - loss: 0.000697 
Batch[9196] - loss: 0.000424 
Batch[9197] - loss: 0.000664 
Batch[9198] - loss: 0.000536 
Batch[9199] - loss: 0.000531 
Batch[9200] - loss: 0.000520 

Evaluation - loss: 0.000069 pearson: 0.5551 

early stop by 1500 steps.
Batch[9201] - loss: 0.000404 
Batch[9202] - loss: 0.000370 
Batch[9203] - loss: 0.000278 
Batch[9204] - loss: 0.000640 
Batch[9205] - loss: 0.000399 
Batch[9206] - loss: 0.000968 
Batch[9207] - loss: 0.000527 
Batch[9208] - loss: 0.000361 
Batch[9209] - loss: 0.000395 
Batch[9210] - loss: 0.000763 
Batch[9211] - loss: 0.000565 
Batch[9212] - loss: 0.000429 
Batch[9213] - loss: 0.000277 
Batch[9214] - loss: 0.000330 
Batch[9215] - loss: 0.000367 
Batch[9216] - loss: 0.000377 
Batch[9217] - loss: 0.000469 
Batch[9218] - loss: 0.000607 
Batch[9219] - loss: 0.000428 
Batch[9220] - loss: 0.000337 
Batch[9221] - loss: 0.000603 
Batch[9222] - loss: 0.000489 
Batch[9223] - loss: 0.000379 
Batch[9224] - loss: 0.000388 
Batch[9225] - loss: 0.000299 
Batch[9226] - loss: 0.000581 
Batch[9227] - loss: 0.000411 
Batch[9228] - loss: 0.000266 
Batch[9229] - loss: 0.000529 
Batch[9230] - loss: 0.000583 
Batch[9231] - loss: 0.000265 
Batch[9232] - loss: 0.000496 
Batch[9233] - loss: 0.000619 
Batch[9234] - loss: 0.000361 
Batch[9235] - loss: 0.000274 
Batch[9236] - loss: 0.000419 
Batch[9237] - loss: 0.000325 
Batch[9238] - loss: 0.000306 
Batch[9239] - loss: 0.000429 
Batch[9240] - loss: 0.000446 
Batch[9241] - loss: 0.000314 
Batch[9242] - loss: 0.000315 
Batch[9243] - loss: 0.000535 
Batch[9244] - loss: 0.000367 
Batch[9245] - loss: 0.000485 
Batch[9246] - loss: 0.000558 
Batch[9247] - loss: 0.000647 
Batch[9248] - loss: 0.000247 
Batch[9249] - loss: 0.000441 
Batch[9250] - loss: 0.000463 
Batch[9251] - loss: 0.000313 
Batch[9252] - loss: 0.000477 
Batch[9253] - loss: 0.000412 
Batch[9254] - loss: 0.000797 
Batch[9255] - loss: 0.000601 
Batch[9256] - loss: 0.000396 
Batch[9257] - loss: 0.000348 
Batch[9258] - loss: 0.000846 
Batch[9259] - loss: 0.000373 
Batch[9260] - loss: 0.000396 
Batch[9261] - loss: 0.000480 
Batch[9262] - loss: 0.000309 
Batch[9263] - loss: 0.000519 
Batch[9264] - loss: 0.000307 
Batch[9265] - loss: 0.000366 
Batch[9266] - loss: 0.000304 
Batch[9267] - loss: 0.000245 
Batch[9268] - loss: 0.000441 
Batch[9269] - loss: 0.000331 
Batch[9270] - loss: 0.000411 
Batch[9271] - loss: 0.000389 
Batch[9272] - loss: 0.000739 
Batch[9273] - loss: 0.000290 
Batch[9274] - loss: 0.000474 
Batch[9275] - loss: 0.000250 
Batch[9276] - loss: 0.000583 
Batch[9277] - loss: 0.000587 
Batch[9278] - loss: 0.000348 
Batch[9279] - loss: 0.000397 
Batch[9280] - loss: 0.000700 
Batch[9281] - loss: 0.000446 
Batch[9282] - loss: 0.000886 
Batch[9283] - loss: 0.000438 
Batch[9284] - loss: 0.000527 
Batch[9285] - loss: 0.000361 
Batch[9286] - loss: 0.000380 
Batch[9287] - loss: 0.000342 
Batch[9288] - loss: 0.000264 
Batch[9289] - loss: 0.000644 
Batch[9290] - loss: 0.000366 
Batch[9291] - loss: 0.000263 
Batch[9292] - loss: 0.000454 
Batch[9293] - loss: 0.000492 
Batch[9294] - loss: 0.000567 
Batch[9295] - loss: 0.000497 
Batch[9296] - loss: 0.000210 
Batch[9297] - loss: 0.000398 
Batch[9298] - loss: 0.000293 
Batch[9299] - loss: 0.000508 
Batch[9300] - loss: 0.000546 

Evaluation - loss: 0.000069 pearson: 0.5573 

early stop by 1500 steps.
Batch[9301] - loss: 0.000483 
Batch[9302] - loss: 0.000611 
Batch[9303] - loss: 0.000301 
Batch[9304] - loss: 0.000440 
Batch[9305] - loss: 0.000276 
Batch[9306] - loss: 0.000386 
Batch[9307] - loss: 0.000350 
Batch[9308] - loss: 0.000383 
Batch[9309] - loss: 0.000465 
Batch[9310] - loss: 0.000552 
Batch[9311] - loss: 0.000631 
Batch[9312] - loss: 0.000330 
Batch[9313] - loss: 0.000406 
Batch[9314] - loss: 0.000367 
Batch[9315] - loss: 0.000768 
Batch[9316] - loss: 0.000275 
Batch[9317] - loss: 0.000396 
Batch[9318] - loss: 0.000632 
Batch[9319] - loss: 0.000273 
Batch[9320] - loss: 0.000409 
Batch[9321] - loss: 0.000391 
Batch[9322] - loss: 0.000666 
Batch[9323] - loss: 0.000275 
Batch[9324] - loss: 0.000346 
Batch[9325] - loss: 0.000415 
Batch[9326] - loss: 0.000380 
Batch[9327] - loss: 0.000272 
Batch[9328] - loss: 0.000465 
Batch[9329] - loss: 0.000208 
Batch[9330] - loss: 0.000286 
Batch[9331] - loss: 0.000320 
Batch[9332] - loss: 0.000436 
Batch[9333] - loss: 0.000395 
Batch[9334] - loss: 0.000353 
Batch[9335] - loss: 0.000967 
Batch[9336] - loss: 0.000310 
Batch[9337] - loss: 0.000532 
Batch[9338] - loss: 0.000450 
Batch[9339] - loss: 0.000635 
Batch[9340] - loss: 0.000407 
Batch[9341] - loss: 0.000408 
Batch[9342] - loss: 0.000676 
Batch[9343] - loss: 0.000748 
Batch[9344] - loss: 0.000765 
Batch[9345] - loss: 0.000571 
Batch[9346] - loss: 0.000210 
Batch[9347] - loss: 0.000232 
Batch[9348] - loss: 0.000594 
Batch[9349] - loss: 0.000196 
Batch[9350] - loss: 0.000325 
Batch[9351] - loss: 0.000420 
Batch[9352] - loss: 0.000639 
Batch[9353] - loss: 0.000387 
Batch[9354] - loss: 0.000507 
Batch[9355] - loss: 0.000563 
Batch[9356] - loss: 0.000715 
Batch[9357] - loss: 0.000397 
Batch[9358] - loss: 0.000521 
Batch[9359] - loss: 0.000416 
Batch[9360] - loss: 0.000380 
Batch[9361] - loss: 0.000570 
Batch[9362] - loss: 0.000460 
Batch[9363] - loss: 0.000376 
Batch[9364] - loss: 0.000628 
Batch[9365] - loss: 0.000551 
Batch[9366] - loss: 0.000402 
Batch[9367] - loss: 0.000445 
Batch[9368] - loss: 0.000293 
Batch[9369] - loss: 0.000340 
Batch[9370] - loss: 0.000280 
Batch[9371] - loss: 0.000372 
Batch[9372] - loss: 0.000446 
Batch[9373] - loss: 0.000840 
Batch[9374] - loss: 0.000216 
Batch[9375] - loss: 0.000369 
Batch[9376] - loss: 0.000371 
Batch[9377] - loss: 0.000299 
Batch[9378] - loss: 0.000329 
Batch[9379] - loss: 0.000399 
Batch[9380] - loss: 0.000668 
Batch[9381] - loss: 0.000318 
Batch[9382] - loss: 0.000402 
Batch[9383] - loss: 0.000343 
Batch[9384] - loss: 0.000539 
Batch[9385] - loss: 0.000523 
Batch[9386] - loss: 0.000370 
Batch[9387] - loss: 0.001222 
Batch[9388] - loss: 0.000720 
Batch[9389] - loss: 0.000262 
Batch[9390] - loss: 0.000674 
Batch[9391] - loss: 0.000367 
Batch[9392] - loss: 0.000688 
Batch[9393] - loss: 0.000247 
Batch[9394] - loss: 0.000735 
Batch[9395] - loss: 0.000791 
Batch[9396] - loss: 0.000404 
Batch[9397] - loss: 0.000271 
Batch[9398] - loss: 0.000290 
Batch[9399] - loss: 0.000408 
Batch[9400] - loss: 0.000400 

Evaluation - loss: 0.000069 pearson: 0.5574 

early stop by 1500 steps.
Batch[9401] - loss: 0.000422 
Batch[9402] - loss: 0.000941 
Batch[9403] - loss: 0.000311 
Batch[9404] - loss: 0.000518 
Batch[9405] - loss: 0.000463 
Batch[9406] - loss: 0.000430 
Batch[9407] - loss: 0.000628 
Batch[9408] - loss: 0.000187 
Batch[9409] - loss: 0.000188 
Batch[9410] - loss: 0.000516 
Batch[9411] - loss: 0.000611 
Batch[9412] - loss: 0.000371 
Batch[9413] - loss: 0.000523 
Batch[9414] - loss: 0.000549 
Batch[9415] - loss: 0.000392 
Batch[9416] - loss: 0.000714 
Batch[9417] - loss: 0.000342 
Batch[9418] - loss: 0.000497 
Batch[9419] - loss: 0.000153 
Batch[9420] - loss: 0.000176 
Batch[9421] - loss: 0.000351 
Batch[9422] - loss: 0.000322 
Batch[9423] - loss: 0.000497 
Batch[9424] - loss: 0.000497 
Batch[9425] - loss: 0.000375 
Batch[9426] - loss: 0.000191 
Batch[9427] - loss: 0.000653 
Batch[9428] - loss: 0.000594 
Batch[9429] - loss: 0.000202 
Batch[9430] - loss: 0.000392 
Batch[9431] - loss: 0.000353 
Batch[9432] - loss: 0.000520 
Batch[9433] - loss: 0.000493 
Batch[9434] - loss: 0.000190 
Batch[9435] - loss: 0.000282 
Batch[9436] - loss: 0.000321 
Batch[9437] - loss: 0.000424 
Batch[9438] - loss: 0.000442 
Batch[9439] - loss: 0.000199 
Batch[9440] - loss: 0.000739 
Batch[9441] - loss: 0.000342 
Batch[9442] - loss: 0.000401 
Batch[9443] - loss: 0.000555 
Batch[9444] - loss: 0.000350 
Batch[9445] - loss: 0.000213 
Batch[9446] - loss: 0.000456 
Batch[9447] - loss: 0.000262 
Batch[9448] - loss: 0.000628 
Batch[9449] - loss: 0.000292 
Batch[9450] - loss: 0.000523 
Batch[9451] - loss: 0.000518 
Batch[9452] - loss: 0.000749 
Batch[9453] - loss: 0.000406 
Batch[9454] - loss: 0.000342 
Batch[9455] - loss: 0.000443 
Batch[9456] - loss: 0.000261 
Batch[9457] - loss: 0.000296 
Batch[9458] - loss: 0.000251 
Batch[9459] - loss: 0.000602 
Batch[9460] - loss: 0.000239 
Batch[9461] - loss: 0.000231 
Batch[9462] - loss: 0.000284 
Batch[9463] - loss: 0.000252 
Batch[9464] - loss: 0.000448 
Batch[9465] - loss: 0.000310 
Batch[9466] - loss: 0.000364 
Batch[9467] - loss: 0.000379 
Batch[9468] - loss: 0.000814 
Batch[9469] - loss: 0.000567 
Batch[9470] - loss: 0.000468 
Batch[9471] - loss: 0.000317 
Batch[9472] - loss: 0.000432 
Batch[9473] - loss: 0.000450 
Batch[9474] - loss: 0.000350 
Batch[9475] - loss: 0.000284 
Batch[9476] - loss: 0.000340 
Batch[9477] - loss: 0.000464 
Batch[9478] - loss: 0.000415 
Batch[9479] - loss: 0.000584 
Batch[9480] - loss: 0.000601 
Batch[9481] - loss: 0.000182 
Batch[9482] - loss: 0.000352 
Batch[9483] - loss: 0.000363 
Batch[9484] - loss: 0.000504 
Batch[9485] - loss: 0.000481 
Batch[9486] - loss: 0.000559 
Batch[9487] - loss: 0.001102 
Batch[9488] - loss: 0.000648 
Batch[9489] - loss: 0.000515 
Batch[9490] - loss: 0.000644 
Batch[9491] - loss: 0.000388 
Batch[9492] - loss: 0.000476 
Batch[9493] - loss: 0.000437 
Batch[9494] - loss: 0.000474 
Batch[9495] - loss: 0.000352 
Batch[9496] - loss: 0.000322 
Batch[9497] - loss: 0.000555 
Batch[9498] - loss: 0.000516 
Batch[9499] - loss: 0.000442 
Batch[9500] - loss: 0.000457 

Evaluation - loss: 0.000070 pearson: 0.5552 

early stop by 1500 steps.
Batch[9501] - loss: 0.000330 
Batch[9502] - loss: 0.000463 
Batch[9503] - loss: 0.000540 
Batch[9504] - loss: 0.000405 
Batch[9505] - loss: 0.000395 
Batch[9506] - loss: 0.001094 
Batch[9507] - loss: 0.000268 
Batch[9508] - loss: 0.000338 
Batch[9509] - loss: 0.000527 
Batch[9510] - loss: 0.000336 
Batch[9511] - loss: 0.000381 
Batch[9512] - loss: 0.000211 
Batch[9513] - loss: 0.000320 
Batch[9514] - loss: 0.000588 
Batch[9515] - loss: 0.000378 
Batch[9516] - loss: 0.000530 
Batch[9517] - loss: 0.000505 
Batch[9518] - loss: 0.000408 
Batch[9519] - loss: 0.000334 
Batch[9520] - loss: 0.000529 
Batch[9521] - loss: 0.000197 
Batch[9522] - loss: 0.000263 
Batch[9523] - loss: 0.000230 
Batch[9524] - loss: 0.000448 
Batch[9525] - loss: 0.000361 
Batch[9526] - loss: 0.000472 
Batch[9527] - loss: 0.000350 
Batch[9528] - loss: 0.000193 
Batch[9529] - loss: 0.000554 
Batch[9530] - loss: 0.000438 
Batch[9531] - loss: 0.000328 
Batch[9532] - loss: 0.000299 
Batch[9533] - loss: 0.000594 
Batch[9534] - loss: 0.000653 
Batch[9535] - loss: 0.000613 
Batch[9536] - loss: 0.000321 
Batch[9537] - loss: 0.000490 
Batch[9538] - loss: 0.000288 
Batch[9539] - loss: 0.000256 
Batch[9540] - loss: 0.000503 
Batch[9541] - loss: 0.000344 
Batch[9542] - loss: 0.000301 
Batch[9543] - loss: 0.000263 
Batch[9544] - loss: 0.000502 
Batch[9545] - loss: 0.000365 
Batch[9546] - loss: 0.000483 
Batch[9547] - loss: 0.000291 
Batch[9548] - loss: 0.000443 
Batch[9549] - loss: 0.000240 
Batch[9550] - loss: 0.000423 
Batch[9551] - loss: 0.000589 
Batch[9552] - loss: 0.000471 
Batch[9553] - loss: 0.000424 
Batch[9554] - loss: 0.000384 
Batch[9555] - loss: 0.000844 
Batch[9556] - loss: 0.000648 
Batch[9557] - loss: 0.000418 
Batch[9558] - loss: 0.000322 
Batch[9559] - loss: 0.000362 
Batch[9560] - loss: 0.000505 
Batch[9561] - loss: 0.000358 
Batch[9562] - loss: 0.000456 
Batch[9563] - loss: 0.000601 
Batch[9564] - loss: 0.000281 
Batch[9565] - loss: 0.000403 
Batch[9566] - loss: 0.000375 
Batch[9567] - loss: 0.000364 
Batch[9568] - loss: 0.000441 
Batch[9569] - loss: 0.000552 
Batch[9570] - loss: 0.000346 
Batch[9571] - loss: 0.000402 
Batch[9572] - loss: 0.000423 
Batch[9573] - loss: 0.000393 
Batch[9574] - loss: 0.000302 
Batch[9575] - loss: 0.000716 
Batch[9576] - loss: 0.000362 
Batch[9577] - loss: 0.000317 
Batch[9578] - loss: 0.000364 
Batch[9579] - loss: 0.000485 
Batch[9580] - loss: 0.000286 
Batch[9581] - loss: 0.000525 
Batch[9582] - loss: 0.000406 
Batch[9583] - loss: 0.000254 
Batch[9584] - loss: 0.000376 
Batch[9585] - loss: 0.000513 
Batch[9586] - loss: 0.000517 
Batch[9587] - loss: 0.000359 
Batch[9588] - loss: 0.000264 
Batch[9589] - loss: 0.000198 
Batch[9590] - loss: 0.000544 
Batch[9591] - loss: 0.000967 
Batch[9592] - loss: 0.000281 
Batch[9593] - loss: 0.000386 
Batch[9594] - loss: 0.000551 
Batch[9595] - loss: 0.000376 
Batch[9596] - loss: 0.000465 
Batch[9597] - loss: 0.000397 
Batch[9598] - loss: 0.000471 
Batch[9599] - loss: 0.000212 
Batch[9600] - loss: 0.000340 

Evaluation - loss: 0.000069 pearson: 0.5559 

early stop by 1500 steps.
Batch[9601] - loss: 0.000464 
Batch[9602] - loss: 0.000200 
Batch[9603] - loss: 0.000369 
Batch[9604] - loss: 0.000548 
Batch[9605] - loss: 0.000882 
Batch[9606] - loss: 0.000489 
Batch[9607] - loss: 0.000307 
Batch[9608] - loss: 0.000425 
Batch[9609] - loss: 0.000472 
Batch[9610] - loss: 0.000549 
Batch[9611] - loss: 0.000581 
Batch[9612] - loss: 0.000363 
Batch[9613] - loss: 0.000434 
Batch[9614] - loss: 0.000640 
Batch[9615] - loss: 0.000364 
Batch[9616] - loss: 0.000604 
Batch[9617] - loss: 0.000500 
Batch[9618] - loss: 0.000282 
Batch[9619] - loss: 0.000315 
Batch[9620] - loss: 0.000374 
Batch[9621] - loss: 0.000300 
Batch[9622] - loss: 0.000277 
Batch[9623] - loss: 0.000408 
Batch[9624] - loss: 0.000481 
Batch[9625] - loss: 0.000469 
Batch[9626] - loss: 0.000638 
Batch[9627] - loss: 0.001154 
Batch[9628] - loss: 0.000658 
Batch[9629] - loss: 0.000286 
Batch[9630] - loss: 0.000449 
Batch[9631] - loss: 0.000391 
Batch[9632] - loss: 0.000420 
Batch[9633] - loss: 0.000318 
Batch[9634] - loss: 0.000245 
Batch[9635] - loss: 0.000420 
Batch[9636] - loss: 0.000994 
Batch[9637] - loss: 0.000329 
Batch[9638] - loss: 0.000389 
Batch[9639] - loss: 0.000321 
Batch[9640] - loss: 0.000237 
Batch[9641] - loss: 0.000307 
Batch[9642] - loss: 0.000378 
Batch[9643] - loss: 0.000262 
Batch[9644] - loss: 0.000485 
Batch[9645] - loss: 0.000442 
Batch[9646] - loss: 0.000500 
Batch[9647] - loss: 0.000391 
Batch[9648] - loss: 0.000587 
Batch[9649] - loss: 0.000349 
Batch[9650] - loss: 0.000400 
Batch[9651] - loss: 0.000506 
Batch[9652] - loss: 0.000229 
Batch[9653] - loss: 0.000422 
Batch[9654] - loss: 0.000461 
Batch[9655] - loss: 0.000354 
Batch[9656] - loss: 0.000298 
Batch[9657] - loss: 0.000438 
Batch[9658] - loss: 0.000400 
Batch[9659] - loss: 0.000334 
Batch[9660] - loss: 0.000393 
Batch[9661] - loss: 0.000363 
Batch[9662] - loss: 0.000301 
Batch[9663] - loss: 0.000228 
Batch[9664] - loss: 0.000414 
Batch[9665] - loss: 0.000198 
Batch[9666] - loss: 0.000522 
Batch[9667] - loss: 0.000218 
Batch[9668] - loss: 0.000683 
Batch[9669] - loss: 0.000277 
Batch[9670] - loss: 0.000279 
Batch[9671] - loss: 0.000533 
Batch[9672] - loss: 0.000530 
Batch[9673] - loss: 0.000401 
Batch[9674] - loss: 0.000306 
Batch[9675] - loss: 0.000646 
Batch[9676] - loss: 0.000421 
Batch[9677] - loss: 0.000298 
Batch[9678] - loss: 0.000474 
Batch[9679] - loss: 0.000831 
Batch[9680] - loss: 0.000471 
Batch[9681] - loss: 0.000345 
Batch[9682] - loss: 0.000373 
Batch[9683] - loss: 0.000622 
Batch[9684] - loss: 0.000416 
Batch[9685] - loss: 0.000438 
Batch[9686] - loss: 0.000316 
Batch[9687] - loss: 0.000227 
Batch[9688] - loss: 0.000452 
Batch[9689] - loss: 0.000302 
Batch[9690] - loss: 0.000576 
Batch[9691] - loss: 0.000838 
Batch[9692] - loss: 0.000403 
Batch[9693] - loss: 0.000419 
Batch[9694] - loss: 0.000519 
Batch[9695] - loss: 0.000369 
Batch[9696] - loss: 0.000510 
Batch[9697] - loss: 0.000399 
Batch[9698] - loss: 0.000448 
Batch[9699] - loss: 0.000218 
Batch[9700] - loss: 0.000634 

Evaluation - loss: 0.000070 pearson: 0.5559 

early stop by 1500 steps.
Batch[9701] - loss: 0.000359 
Batch[9702] - loss: 0.000398 
Batch[9703] - loss: 0.000142 
Batch[9704] - loss: 0.000212 
Batch[9705] - loss: 0.000542 
Batch[9706] - loss: 0.000415 
Batch[9707] - loss: 0.000486 
Batch[9708] - loss: 0.000507 
Batch[9709] - loss: 0.000420 
Batch[9710] - loss: 0.000616 
Batch[9711] - loss: 0.000460 
Batch[9712] - loss: 0.000556 
Batch[9713] - loss: 0.000460 
Batch[9714] - loss: 0.000524 
Batch[9715] - loss: 0.000342 
Batch[9716] - loss: 0.000476 
Batch[9717] - loss: 0.001035 
Batch[9718] - loss: 0.000541 
Batch[9719] - loss: 0.000459 
Batch[9720] - loss: 0.000472 
Batch[9721] - loss: 0.000345 
Batch[9722] - loss: 0.000520 
Batch[9723] - loss: 0.000180 
Batch[9724] - loss: 0.000333 
Batch[9725] - loss: 0.000220 
Batch[9726] - loss: 0.000480 
Batch[9727] - loss: 0.000494 
Batch[9728] - loss: 0.000358 
Batch[9729] - loss: 0.000249 
Batch[9730] - loss: 0.000621 
Batch[9731] - loss: 0.000235 
Batch[9732] - loss: 0.000368 
Batch[9733] - loss: 0.000450 
Batch[9734] - loss: 0.000256 
Batch[9735] - loss: 0.000266 
Batch[9736] - loss: 0.000362 
Batch[9737] - loss: 0.000322 
Batch[9738] - loss: 0.000317 
Batch[9739] - loss: 0.000381 
Batch[9740] - loss: 0.000610 
Batch[9741] - loss: 0.000313 
Batch[9742] - loss: 0.000625 
Batch[9743] - loss: 0.000368 
Batch[9744] - loss: 0.000377 
Batch[9745] - loss: 0.000349 
Batch[9746] - loss: 0.000428 
Batch[9747] - loss: 0.000473 
Batch[9748] - loss: 0.000313 
Batch[9749] - loss: 0.000273 
Batch[9750] - loss: 0.000670 
Batch[9751] - loss: 0.000369 
Batch[9752] - loss: 0.000289 
Batch[9753] - loss: 0.001030 
Batch[9754] - loss: 0.000390 
Batch[9755] - loss: 0.000430 
Batch[9756] - loss: 0.000432 
Batch[9757] - loss: 0.000467 
Batch[9758] - loss: 0.000452 
Batch[9759] - loss: 0.000542 
Batch[9760] - loss: 0.000533 
Batch[9761] - loss: 0.000452 
Batch[9762] - loss: 0.000432 
Batch[9763] - loss: 0.000357 
Batch[9764] - loss: 0.000272 
Batch[9765] - loss: 0.000300 
Batch[9766] - loss: 0.000510 
Batch[9767] - loss: 0.000525 
Batch[9768] - loss: 0.000614 
Batch[9769] - loss: 0.000484 
Batch[9770] - loss: 0.000508 
Batch[9771] - loss: 0.000369 
Batch[9772] - loss: 0.000395 
Batch[9773] - loss: 0.000446 
Batch[9774] - loss: 0.000291 
Batch[9775] - loss: 0.000341 
Batch[9776] - loss: 0.000447 
Batch[9777] - loss: 0.000325 
Batch[9778] - loss: 0.000409 
Batch[9779] - loss: 0.000331 
Batch[9780] - loss: 0.000635 
Batch[9781] - loss: 0.000378 
Batch[9782] - loss: 0.000459 
Batch[9783] - loss: 0.000398 
Batch[9784] - loss: 0.000330 
Batch[9785] - loss: 0.000178 
Batch[9786] - loss: 0.000355 
Batch[9787] - loss: 0.000520 
Batch[9788] - loss: 0.000500 
Batch[9789] - loss: 0.000578 
Batch[9790] - loss: 0.000428 
Batch[9791] - loss: 0.000168 
Batch[9792] - loss: 0.000351 
Batch[9793] - loss: 0.000182 
Batch[9794] - loss: 0.000352 
Batch[9795] - loss: 0.000249 
Batch[9796] - loss: 0.000308 
Batch[9797] - loss: 0.000498 
Batch[9798] - loss: 0.000236 
Batch[9799] - loss: 0.000270 
Batch[9800] - loss: 0.000190 

Evaluation - loss: 0.000070 pearson: 0.5558 

early stop by 1500 steps.
Batch[9801] - loss: 0.000431 
Batch[9802] - loss: 0.000416 
Batch[9803] - loss: 0.000221 
Batch[9804] - loss: 0.000343 
Batch[9805] - loss: 0.000466 
Batch[9806] - loss: 0.000459 
Batch[9807] - loss: 0.000299 
Batch[9808] - loss: 0.000432 
Batch[9809] - loss: 0.000278 
Batch[9810] - loss: 0.000573 
Batch[9811] - loss: 0.000273 
Batch[9812] - loss: 0.000336 
Batch[9813] - loss: 0.000490 
Batch[9814] - loss: 0.000276 
Batch[9815] - loss: 0.000333 
Batch[9816] - loss: 0.000711 
Batch[9817] - loss: 0.000435 
Batch[9818] - loss: 0.000236 
Batch[9819] - loss: 0.000470 
Batch[9820] - loss: 0.000257 
Batch[9821] - loss: 0.000441 
Batch[9822] - loss: 0.000295 
Batch[9823] - loss: 0.000504 
Batch[9824] - loss: 0.000216 
Batch[9825] - loss: 0.000377 
Batch[9826] - loss: 0.000589 
Batch[9827] - loss: 0.000367 
Batch[9828] - loss: 0.000311 
Batch[9829] - loss: 0.000339 
Batch[9830] - loss: 0.000341 
Batch[9831] - loss: 0.000363 
Batch[9832] - loss: 0.000643 
Batch[9833] - loss: 0.000273 
Batch[9834] - loss: 0.000362 
Batch[9835] - loss: 0.000384 
Batch[9836] - loss: 0.000334 
Batch[9837] - loss: 0.000588 
Batch[9838] - loss: 0.000309 
Batch[9839] - loss: 0.000452 
Batch[9840] - loss: 0.000314 
Batch[9841] - loss: 0.000436 
Batch[9842] - loss: 0.000304 
Batch[9843] - loss: 0.000379 
Batch[9844] - loss: 0.000302 
Batch[9845] - loss: 0.000376 
Batch[9846] - loss: 0.000524 
Batch[9847] - loss: 0.000330 
Batch[9848] - loss: 0.000461 
Batch[9849] - loss: 0.000341 
Batch[9850] - loss: 0.000388 
Batch[9851] - loss: 0.000415 
Batch[9852] - loss: 0.000375 
Batch[9853] - loss: 0.000281 
Batch[9854] - loss: 0.000408 
Batch[9855] - loss: 0.000620 
Batch[9856] - loss: 0.000381 
Batch[9857] - loss: 0.000385 
Batch[9858] - loss: 0.000324 
Batch[9859] - loss: 0.000322 
Batch[9860] - loss: 0.000548 
Batch[9861] - loss: 0.000399 
Batch[9862] - loss: 0.000529 
Batch[9863] - loss: 0.000502 
Batch[9864] - loss: 0.000410 
Batch[9865] - loss: 0.000458 
Batch[9866] - loss: 0.000239 
Batch[9867] - loss: 0.000455 
Batch[9868] - loss: 0.000770 
Batch[9869] - loss: 0.000362 
Batch[9870] - loss: 0.000387 
Batch[9871] - loss: 0.000378 
Batch[9872] - loss: 0.000494 
Batch[9873] - loss: 0.000289 
Batch[9874] - loss: 0.000391 
Batch[9875] - loss: 0.000330 
Batch[9876] - loss: 0.000260 
Batch[9877] - loss: 0.000468 
Batch[9878] - loss: 0.000450 
Batch[9879] - loss: 0.000329 
Batch[9880] - loss: 0.000366 
Batch[9881] - loss: 0.000363 
Batch[9882] - loss: 0.000328 
Batch[9883] - loss: 0.000333 
Batch[9884] - loss: 0.000328 
Batch[9885] - loss: 0.000446 
Batch[9886] - loss: 0.000462 
Batch[9887] - loss: 0.000497 
Batch[9888] - loss: 0.000384 
Batch[9889] - loss: 0.000366 
Batch[9890] - loss: 0.000442 
Batch[9891] - loss: 0.000284 
Batch[9892] - loss: 0.000489 
Batch[9893] - loss: 0.000302 
Batch[9894] - loss: 0.000394 
Batch[9895] - loss: 0.000299 
Batch[9896] - loss: 0.000390 
Batch[9897] - loss: 0.000699 
Batch[9898] - loss: 0.000481 
Batch[9899] - loss: 0.000411 
Batch[9900] - loss: 0.000402 

Evaluation - loss: 0.000069 pearson: 0.5592 

early stop by 1500 steps.
Batch[9901] - loss: 0.000267 
Batch[9902] - loss: 0.000217 
Batch[9903] - loss: 0.000960 
Batch[9904] - loss: 0.000206 
Batch[9905] - loss: 0.000226 
Batch[9906] - loss: 0.000315 
Batch[9907] - loss: 0.000348 
Batch[9908] - loss: 0.000378 
Batch[9909] - loss: 0.000343 
Batch[9910] - loss: 0.000340 
Batch[9911] - loss: 0.000395 
Batch[9912] - loss: 0.000397 
Batch[9913] - loss: 0.000705 
Batch[9914] - loss: 0.000386 
Batch[9915] - loss: 0.000395 
Batch[9916] - loss: 0.000273 
Batch[9917] - loss: 0.000427 
Batch[9918] - loss: 0.000483 
Batch[9919] - loss: 0.000307 
Batch[9920] - loss: 0.000502 
Batch[9921] - loss: 0.000263 
Batch[9922] - loss: 0.000429 
Batch[9923] - loss: 0.000635 
Batch[9924] - loss: 0.000232 
Batch[9925] - loss: 0.000331 
Batch[9926] - loss: 0.000151 
Batch[9927] - loss: 0.000420 
Batch[9928] - loss: 0.000294 
Batch[9929] - loss: 0.000410 
Batch[9930] - loss: 0.000473 
Batch[9931] - loss: 0.000295 
Batch[9932] - loss: 0.000323 
Batch[9933] - loss: 0.000865 
Batch[9934] - loss: 0.000387 
Batch[9935] - loss: 0.000253 
Batch[9936] - loss: 0.000281 
Batch[9937] - loss: 0.000423 
Batch[9938] - loss: 0.000648 
Batch[9939] - loss: 0.000285 
Batch[9940] - loss: 0.000492 
Batch[9941] - loss: 0.000703 
Batch[9942] - loss: 0.000173 
Batch[9943] - loss: 0.000334 
Batch[9944] - loss: 0.000367 
Batch[9945] - loss: 0.000245 
Batch[9946] - loss: 0.000348 
Batch[9947] - loss: 0.000511 
Batch[9948] - loss: 0.000139 
Batch[9949] - loss: 0.000489 
Batch[9950] - loss: 0.000575 
Batch[9951] - loss: 0.000652 
Batch[9952] - loss: 0.000325 
Batch[9953] - loss: 0.000473 
Batch[9954] - loss: 0.000409 
Batch[9955] - loss: 0.000333 
Batch[9956] - loss: 0.000510 
Batch[9957] - loss: 0.000388 
Batch[9958] - loss: 0.000274 
Batch[9959] - loss: 0.000352 
Batch[9960] - loss: 0.000268 
Batch[9961] - loss: 0.000509 
Batch[9962] - loss: 0.000427 
Batch[9963] - loss: 0.000424 
Batch[9964] - loss: 0.000322 
Batch[9965] - loss: 0.000375 
Batch[9966] - loss: 0.000693 
Batch[9967] - loss: 0.000570 
Batch[9968] - loss: 0.000356 
Batch[9969] - loss: 0.000437 
Batch[9970] - loss: 0.000227 
Batch[9971] - loss: 0.000255 
Batch[9972] - loss: 0.000299 
Batch[9973] - loss: 0.000262 
Batch[9974] - loss: 0.000296 
Batch[9975] - loss: 0.000369 
Batch[9976] - loss: 0.000245 
Batch[9977] - loss: 0.000897 
Batch[9978] - loss: 0.000477 
Batch[9979] - loss: 0.000546 
Batch[9980] - loss: 0.000637 
Batch[9981] - loss: 0.000271 
Batch[9982] - loss: 0.000248 
Batch[9983] - loss: 0.000424 
Batch[9984] - loss: 0.000258 
Batch[9985] - loss: 0.000223 
Batch[9986] - loss: 0.000189 
Batch[9987] - loss: 0.000389 
Batch[9988] - loss: 0.000412 
Batch[9989] - loss: 0.000312 
Batch[9990] - loss: 0.000306 
Batch[9991] - loss: 0.000432 
Batch[9992] - loss: 0.000348 
Batch[9993] - loss: 0.000416 
Batch[9994] - loss: 0.000304 
Batch[9995] - loss: 0.000211 
Batch[9996] - loss: 0.000292 
Batch[9997] - loss: 0.000204 
Batch[9998] - loss: 0.000389 
Batch[9999] - loss: 0.000506 
Batch[10000] - loss: 0.000351 

Evaluation - loss: 0.000070 pearson: 0.5565 

early stop by 1500 steps.
Batch[10001] - loss: 0.000320 
Batch[10002] - loss: 0.000610 
Batch[10003] - loss: 0.000350 
Batch[10004] - loss: 0.000671 
Batch[10005] - loss: 0.000255 
Batch[10006] - loss: 0.000256 
Batch[10007] - loss: 0.000370 
Batch[10008] - loss: 0.000407 
Batch[10009] - loss: 0.000554 
Batch[10010] - loss: 0.000318 
Batch[10011] - loss: 0.000238 
Batch[10012] - loss: 0.000339 
Batch[10013] - loss: 0.000292 
Batch[10014] - loss: 0.000415 
Batch[10015] - loss: 0.000217 
Batch[10016] - loss: 0.000920 
Batch[10017] - loss: 0.000733 
Batch[10018] - loss: 0.000396 
Batch[10019] - loss: 0.000419 
Batch[10020] - loss: 0.000437 
Batch[10021] - loss: 0.000407 
Batch[10022] - loss: 0.000252 
Batch[10023] - loss: 0.000227 
Batch[10024] - loss: 0.000465 
Batch[10025] - loss: 0.000383 
Batch[10026] - loss: 0.000337 
Batch[10027] - loss: 0.000278 
Batch[10028] - loss: 0.000468 
Batch[10029] - loss: 0.000318 
Batch[10030] - loss: 0.000414 
Batch[10031] - loss: 0.000220 
Batch[10032] - loss: 0.000565 
Batch[10033] - loss: 0.000240 
Batch[10034] - loss: 0.000328 
Batch[10035] - loss: 0.000409 
Batch[10036] - loss: 0.000402 
Batch[10037] - loss: 0.000239 
Batch[10038] - loss: 0.000321 
Batch[10039] - loss: 0.000660 
Batch[10040] - loss: 0.000400 
Batch[10041] - loss: 0.000382 
Batch[10042] - loss: 0.000286 
Batch[10043] - loss: 0.000666 
Batch[10044] - loss: 0.000343 
Batch[10045] - loss: 0.000477 
Batch[10046] - loss: 0.000436 
Batch[10047] - loss: 0.000174 
Batch[10048] - loss: 0.000773 
Batch[10049] - loss: 0.000263 
Batch[10050] - loss: 0.001035 
Batch[10051] - loss: 0.000394 
Batch[10052] - loss: 0.000351 
Batch[10053] - loss: 0.000235 
Batch[10054] - loss: 0.000256 
Batch[10055] - loss: 0.000518 
Batch[10056] - loss: 0.000374 
Batch[10057] - loss: 0.000564 
Batch[10058] - loss: 0.000323 
Batch[10059] - loss: 0.000182 
Batch[10060] - loss: 0.000411 
Batch[10061] - loss: 0.000369 
Batch[10062] - loss: 0.000572 
Batch[10063] - loss: 0.000174 
Batch[10064] - loss: 0.000154 
Batch[10065] - loss: 0.000478 
Batch[10066] - loss: 0.000295 
Batch[10067] - loss: 0.000221 
Batch[10068] - loss: 0.000212 
Batch[10069] - loss: 0.000365 
Batch[10070] - loss: 0.000292 
Batch[10071] - loss: 0.000299 
Batch[10072] - loss: 0.000292 
Batch[10073] - loss: 0.000434 
Batch[10074] - loss: 0.000313 
Batch[10075] - loss: 0.000452 
Batch[10076] - loss: 0.000466 
Batch[10077] - loss: 0.000217 
Batch[10078] - loss: 0.000245 
Batch[10079] - loss: 0.000377 
Batch[10080] - loss: 0.000405 
Batch[10081] - loss: 0.000331 
Batch[10082] - loss: 0.000494 
Batch[10083] - loss: 0.000407 
Batch[10084] - loss: 0.000271 
Batch[10085] - loss: 0.000532 
Batch[10086] - loss: 0.000291 
Batch[10087] - loss: 0.000302 
Batch[10088] - loss: 0.000403 
Batch[10089] - loss: 0.000351 
Batch[10090] - loss: 0.000305 
Batch[10091] - loss: 0.000244 
Batch[10092] - loss: 0.000541 
Batch[10093] - loss: 0.000533 
Batch[10094] - loss: 0.000302 
Batch[10095] - loss: 0.000253 
Batch[10096] - loss: 0.000378 
Batch[10097] - loss: 0.000395 
Batch[10098] - loss: 0.000363 
Batch[10099] - loss: 0.000386 
Batch[10100] - loss: 0.000282 

Evaluation - loss: 0.000070 pearson: 0.5557 

early stop by 1500 steps.
Batch[10101] - loss: 0.000712 
Batch[10102] - loss: 0.000320 
Batch[10103] - loss: 0.000582 
Batch[10104] - loss: 0.000463 
Batch[10105] - loss: 0.000198 
Batch[10106] - loss: 0.000248 
Batch[10107] - loss: 0.000339 
Batch[10108] - loss: 0.000381 
Batch[10109] - loss: 0.000488 
Batch[10110] - loss: 0.000401 
Batch[10111] - loss: 0.000444 
Batch[10112] - loss: 0.000382 
Batch[10113] - loss: 0.000357 
Batch[10114] - loss: 0.000233 
Batch[10115] - loss: 0.000384 
Batch[10116] - loss: 0.000282 
Batch[10117] - loss: 0.000411 
Batch[10118] - loss: 0.000663 
Batch[10119] - loss: 0.000427 
Batch[10120] - loss: 0.000306 
Batch[10121] - loss: 0.000306 
Batch[10122] - loss: 0.000656 
Batch[10123] - loss: 0.000359 
Batch[10124] - loss: 0.000286 
Batch[10125] - loss: 0.000500 
Batch[10126] - loss: 0.000365 
Batch[10127] - loss: 0.000366 
Batch[10128] - loss: 0.000323 
Batch[10129] - loss: 0.000355 
Batch[10130] - loss: 0.000358 
Batch[10131] - loss: 0.000481 
Batch[10132] - loss: 0.000316 
Batch[10133] - loss: 0.000555 
Batch[10134] - loss: 0.000209 
Batch[10135] - loss: 0.000243 
Batch[10136] - loss: 0.000360 
Batch[10137] - loss: 0.000258 
Batch[10138] - loss: 0.000427 
Batch[10139] - loss: 0.000477 
Batch[10140] - loss: 0.000270 
Batch[10141] - loss: 0.000513 
Batch[10142] - loss: 0.000243 
Batch[10143] - loss: 0.000409 
Batch[10144] - loss: 0.000460 
Batch[10145] - loss: 0.000391 
Batch[10146] - loss: 0.000236 
Batch[10147] - loss: 0.000417 
Batch[10148] - loss: 0.000334 
Batch[10149] - loss: 0.000345 
Batch[10150] - loss: 0.000336 
Batch[10151] - loss: 0.000373 
Batch[10152] - loss: 0.000345 
Batch[10153] - loss: 0.000874 
Batch[10154] - loss: 0.000398 
Batch[10155] - loss: 0.000410 
Batch[10156] - loss: 0.000441 
Batch[10157] - loss: 0.000413 
Batch[10158] - loss: 0.000344 
Batch[10159] - loss: 0.000265 
Batch[10160] - loss: 0.000347 
Batch[10161] - loss: 0.000614 
Batch[10162] - loss: 0.000209 
Batch[10163] - loss: 0.000586 
Batch[10164] - loss: 0.000386 
Batch[10165] - loss: 0.000367 
Batch[10166] - loss: 0.000574 
Batch[10167] - loss: 0.000305 
Batch[10168] - loss: 0.000315 
Batch[10169] - loss: 0.000542 
Batch[10170] - loss: 0.000221 
Batch[10171] - loss: 0.000555 
Batch[10172] - loss: 0.000456 
Batch[10173] - loss: 0.000204 
Batch[10174] - loss: 0.000377 
Batch[10175] - loss: 0.000243 
Batch[10176] - loss: 0.001105 
Batch[10177] - loss: 0.000440 
Batch[10178] - loss: 0.000260 
Batch[10179] - loss: 0.000305 
Batch[10180] - loss: 0.000413 
Batch[10181] - loss: 0.000503 
Batch[10182] - loss: 0.000324 
Batch[10183] - loss: 0.000310 
Batch[10184] - loss: 0.000441 
Batch[10185] - loss: 0.000348 
Batch[10186] - loss: 0.000324 
Batch[10187] - loss: 0.000516 
Batch[10188] - loss: 0.000228 
Batch[10189] - loss: 0.000392 
Batch[10190] - loss: 0.000949 
Batch[10191] - loss: 0.000403 
Batch[10192] - loss: 0.000246 
Batch[10193] - loss: 0.000494 
Batch[10194] - loss: 0.000425 
Batch[10195] - loss: 0.000205 
Batch[10196] - loss: 0.000190 
Batch[10197] - loss: 0.000185 
Batch[10198] - loss: 0.000319 
Batch[10199] - loss: 0.000331 
Batch[10200] - loss: 0.000381 

Evaluation - loss: 0.000070 pearson: 0.5549 

early stop by 1500 steps.
Batch[10201] - loss: 0.000349 
Batch[10202] - loss: 0.000360 
Batch[10203] - loss: 0.000699 
Batch[10204] - loss: 0.000359 
Batch[10205] - loss: 0.000669 
Batch[10206] - loss: 0.000510 
Batch[10207] - loss: 0.000491 
Batch[10208] - loss: 0.000345 
Batch[10209] - loss: 0.000422 
Batch[10210] - loss: 0.000198 
Batch[10211] - loss: 0.000250 
Batch[10212] - loss: 0.000217 
Batch[10213] - loss: 0.000173 
Batch[10214] - loss: 0.000281 
Batch[10215] - loss: 0.000327 
Batch[10216] - loss: 0.000352 
Batch[10217] - loss: 0.000176 
Batch[10218] - loss: 0.000360 
Batch[10219] - loss: 0.000497 
Batch[10220] - loss: 0.000582 
Batch[10221] - loss: 0.000234 
Batch[10222] - loss: 0.000167 
Batch[10223] - loss: 0.000383 
Batch[10224] - loss: 0.000382 
Batch[10225] - loss: 0.000216 
Batch[10226] - loss: 0.000380 
Batch[10227] - loss: 0.000292 
Batch[10228] - loss: 0.000291 
Batch[10229] - loss: 0.000288 
Batch[10230] - loss: 0.000671 
Batch[10231] - loss: 0.000526 
Batch[10232] - loss: 0.000460 
Batch[10233] - loss: 0.000551 
Batch[10234] - loss: 0.000376 
Batch[10235] - loss: 0.000403 
Batch[10236] - loss: 0.000452 
Batch[10237] - loss: 0.000344 
Batch[10238] - loss: 0.000779 
Batch[10239] - loss: 0.000352 
Batch[10240] - loss: 0.000309 
Batch[10241] - loss: 0.000709 
Batch[10242] - loss: 0.000434 
Batch[10243] - loss: 0.000697 
Batch[10244] - loss: 0.000371 
Batch[10245] - loss: 0.000451 
Batch[10246] - loss: 0.000436 
Batch[10247] - loss: 0.000369 
Batch[10248] - loss: 0.000483 
Batch[10249] - loss: 0.000192 
Batch[10250] - loss: 0.000457 
Batch[10251] - loss: 0.000397 
Batch[10252] - loss: 0.000322 
Batch[10253] - loss: 0.000394 
Batch[10254] - loss: 0.000383 
Batch[10255] - loss: 0.000219 
Batch[10256] - loss: 0.000441 
Batch[10257] - loss: 0.000390 
Batch[10258] - loss: 0.000514 
Batch[10259] - loss: 0.000533 
Batch[10260] - loss: 0.000437 
Batch[10261] - loss: 0.000352 
Batch[10262] - loss: 0.000480 
Batch[10263] - loss: 0.000552 
Batch[10264] - loss: 0.000429 
Batch[10265] - loss: 0.000471 
Batch[10266] - loss: 0.000290 
Batch[10267] - loss: 0.000318 
Batch[10268] - loss: 0.000329 
Batch[10269] - loss: 0.000398 
Batch[10270] - loss: 0.000366 
Batch[10271] - loss: 0.000264 
Batch[10272] - loss: 0.000449 
Batch[10273] - loss: 0.000259 
Batch[10274] - loss: 0.000161 
Batch[10275] - loss: 0.000334 
Batch[10276] - loss: 0.000333 
Batch[10277] - loss: 0.000241 
Batch[10278] - loss: 0.000578 
Batch[10279] - loss: 0.000324 
Batch[10280] - loss: 0.000457 
Batch[10281] - loss: 0.000165 
Batch[10282] - loss: 0.000339 
Batch[10283] - loss: 0.000309 
Batch[10284] - loss: 0.000354 
Batch[10285] - loss: 0.000263 
Batch[10286] - loss: 0.001109 
Batch[10287] - loss: 0.000256 
Batch[10288] - loss: 0.000388 
Batch[10289] - loss: 0.000234 
Batch[10290] - loss: 0.000247 
Batch[10291] - loss: 0.000335 
Batch[10292] - loss: 0.000550 
Batch[10293] - loss: 0.000346 
Batch[10294] - loss: 0.000590 
Batch[10295] - loss: 0.000431 
Batch[10296] - loss: 0.000434 
Batch[10297] - loss: 0.000306 
Batch[10298] - loss: 0.000302 
Batch[10299] - loss: 0.000312 
Batch[10300] - loss: 0.000651 

Evaluation - loss: 0.000070 pearson: 0.5554 

early stop by 1500 steps.
Batch[10301] - loss: 0.000487 
Batch[10302] - loss: 0.000539 
Batch[10303] - loss: 0.000432 
Batch[10304] - loss: 0.000452 
Batch[10305] - loss: 0.000909 
Batch[10306] - loss: 0.000508 
Batch[10307] - loss: 0.000380 
Batch[10308] - loss: 0.000203 
Batch[10309] - loss: 0.000411 
Batch[10310] - loss: 0.000416 
Batch[10311] - loss: 0.000619 
Batch[10312] - loss: 0.000337 
Batch[10313] - loss: 0.000374 
Batch[10314] - loss: 0.000286 
Batch[10315] - loss: 0.000198 
Batch[10316] - loss: 0.000369 
Batch[10317] - loss: 0.000460 
Batch[10318] - loss: 0.000243 
Batch[10319] - loss: 0.000441 
Batch[10320] - loss: 0.000334 
Batch[10321] - loss: 0.000391 
Batch[10322] - loss: 0.000574 
Batch[10323] - loss: 0.000498 
Batch[10324] - loss: 0.000313 
Batch[10325] - loss: 0.000474 
Batch[10326] - loss: 0.000341 
Batch[10327] - loss: 0.000407 
Batch[10328] - loss: 0.000258 
Batch[10329] - loss: 0.000284 
Batch[10330] - loss: 0.000430 
Batch[10331] - loss: 0.000281 
Batch[10332] - loss: 0.000390 
Batch[10333] - loss: 0.000270 
Batch[10334] - loss: 0.000246 
Batch[10335] - loss: 0.000434 
Batch[10336] - loss: 0.000327 
Batch[10337] - loss: 0.000289 
Batch[10338] - loss: 0.000355 
Batch[10339] - loss: 0.000446 
Batch[10340] - loss: 0.000391 
Batch[10341] - loss: 0.000352 
Batch[10342] - loss: 0.000407 
Batch[10343] - loss: 0.000159 
Batch[10344] - loss: 0.000439 
Batch[10345] - loss: 0.000503 
Batch[10346] - loss: 0.000330 
Batch[10347] - loss: 0.000280 
Batch[10348] - loss: 0.000645 
Batch[10349] - loss: 0.000407 
Batch[10350] - loss: 0.000233 
Batch[10351] - loss: 0.000250 
Batch[10352] - loss: 0.000381 
Batch[10353] - loss: 0.000518 
Batch[10354] - loss: 0.000466 
Batch[10355] - loss: 0.000504 
Batch[10356] - loss: 0.000962 
Batch[10357] - loss: 0.000445 
Batch[10358] - loss: 0.000382 
Batch[10359] - loss: 0.000251 
Batch[10360] - loss: 0.000438 
Batch[10361] - loss: 0.000347 
Batch[10362] - loss: 0.000340 
Batch[10363] - loss: 0.000367 
Batch[10364] - loss: 0.000278 
Batch[10365] - loss: 0.000587 
Batch[10366] - loss: 0.000416 
Batch[10367] - loss: 0.000507 
Batch[10368] - loss: 0.000293 
Batch[10369] - loss: 0.000415 
Batch[10370] - loss: 0.000555 
Batch[10371] - loss: 0.000398 
Batch[10372] - loss: 0.000938 
Batch[10373] - loss: 0.000560 
Batch[10374] - loss: 0.000740 
Batch[10375] - loss: 0.000301 
Batch[10376] - loss: 0.000408 
Batch[10377] - loss: 0.000593 
Batch[10378] - loss: 0.000237 
Batch[10379] - loss: 0.000374 
Batch[10380] - loss: 0.000298 
Batch[10381] - loss: 0.000644 
Batch[10382] - loss: 0.000247 
Batch[10383] - loss: 0.000895 
Batch[10384] - loss: 0.000324 
Batch[10385] - loss: 0.000568 
Batch[10386] - loss: 0.000398 
Batch[10387] - loss: 0.000413 
Batch[10388] - loss: 0.000518 
Batch[10389] - loss: 0.000258 
Batch[10390] - loss: 0.000520 
Batch[10391] - loss: 0.000331 
Batch[10392] - loss: 0.000817 
Batch[10393] - loss: 0.000412 
Batch[10394] - loss: 0.000254 
Batch[10395] - loss: 0.000566 
Batch[10396] - loss: 0.000403 
Batch[10397] - loss: 0.000341 
Batch[10398] - loss: 0.000513 
Batch[10399] - loss: 0.000392 
Batch[10400] - loss: 0.000471 

Evaluation - loss: 0.000070 pearson: 0.5542 

early stop by 1500 steps.
Batch[10401] - loss: 0.000503 
Batch[10402] - loss: 0.000352 
Batch[10403] - loss: 0.000381 
Batch[10404] - loss: 0.000300 
Batch[10405] - loss: 0.000262 
Batch[10406] - loss: 0.000365 
Batch[10407] - loss: 0.000359 
Batch[10408] - loss: 0.000313 
Batch[10409] - loss: 0.000317 
Batch[10410] - loss: 0.000238 
Batch[10411] - loss: 0.000669 
Batch[10412] - loss: 0.000325 
Batch[10413] - loss: 0.000367 
Batch[10414] - loss: 0.000362 
Batch[10415] - loss: 0.000326 
Batch[10416] - loss: 0.000237 
Batch[10417] - loss: 0.000395 
Batch[10418] - loss: 0.000358 
Batch[10419] - loss: 0.000270 
Batch[10420] - loss: 0.000411 
Batch[10421] - loss: 0.000671 
Batch[10422] - loss: 0.000456 
Batch[10423] - loss: 0.000398 
Batch[10424] - loss: 0.000464 
Batch[10425] - loss: 0.000465 
Batch[10426] - loss: 0.000357 
Batch[10427] - loss: 0.000267 
Batch[10428] - loss: 0.000358 
Batch[10429] - loss: 0.000503 
Batch[10430] - loss: 0.000414 
Batch[10431] - loss: 0.000550 
Batch[10432] - loss: 0.000458 
Batch[10433] - loss: 0.000253 
Batch[10434] - loss: 0.000445 
Batch[10435] - loss: 0.000356 
Batch[10436] - loss: 0.000613 
Batch[10437] - loss: 0.000717 
Batch[10438] - loss: 0.000251 
Batch[10439] - loss: 0.000250 
Batch[10440] - loss: 0.000388 
Batch[10441] - loss: 0.000388 
Batch[10442] - loss: 0.000427 
Batch[10443] - loss: 0.000386 
Batch[10444] - loss: 0.000533 
Batch[10445] - loss: 0.000412 
Batch[10446] - loss: 0.000379 
Batch[10447] - loss: 0.000439 
Batch[10448] - loss: 0.000623 
Batch[10449] - loss: 0.000452 
Batch[10450] - loss: 0.000488 
Batch[10451] - loss: 0.000212 
Batch[10452] - loss: 0.000351 
Batch[10453] - loss: 0.000408 
Batch[10454] - loss: 0.000887 
Batch[10455] - loss: 0.000528 
Batch[10456] - loss: 0.000368 
Batch[10457] - loss: 0.000345 
Batch[10458] - loss: 0.000437 
Batch[10459] - loss: 0.000239 
Batch[10460] - loss: 0.000399 
Batch[10461] - loss: 0.000341 
Batch[10462] - loss: 0.000537 
Batch[10463] - loss: 0.000297 
Batch[10464] - loss: 0.000397 
Batch[10465] - loss: 0.000294 
Batch[10466] - loss: 0.000224 
Batch[10467] - loss: 0.000415 
Batch[10468] - loss: 0.000410 
Batch[10469] - loss: 0.000534 
Batch[10470] - loss: 0.000350 
Batch[10471] - loss: 0.000293 
Batch[10472] - loss: 0.000281 
Batch[10473] - loss: 0.000525 
Batch[10474] - loss: 0.000525 
Batch[10475] - loss: 0.000352 
Batch[10476] - loss: 0.000256 
Batch[10477] - loss: 0.000176 
Batch[10478] - loss: 0.000288 
Batch[10479] - loss: 0.000291 
Batch[10480] - loss: 0.000768 
Batch[10481] - loss: 0.000493 
Batch[10482] - loss: 0.000460 
Batch[10483] - loss: 0.000271 
Batch[10484] - loss: 0.000299 
Batch[10485] - loss: 0.000319 
Batch[10486] - loss: 0.000301 
Batch[10487] - loss: 0.000495 
Batch[10488] - loss: 0.000155 
Batch[10489] - loss: 0.000196 
Batch[10490] - loss: 0.000674 
Batch[10491] - loss: 0.000323 
Batch[10492] - loss: 0.000283 
Batch[10493] - loss: 0.000402 
Batch[10494] - loss: 0.000228 
Batch[10495] - loss: 0.000340 
Batch[10496] - loss: 0.000365 
Batch[10497] - loss: 0.000279 
Batch[10498] - loss: 0.000250 
Batch[10499] - loss: 0.000480 
Batch[10500] - loss: 0.000385 

Evaluation - loss: 0.000070 pearson: 0.5542 

early stop by 1500 steps.
Batch[10501] - loss: 0.000442 
Batch[10502] - loss: 0.000444 
Batch[10503] - loss: 0.000499 
Batch[10504] - loss: 0.000286 
Batch[10505] - loss: 0.000282 
Batch[10506] - loss: 0.000301 
Batch[10507] - loss: 0.000211 
Batch[10508] - loss: 0.000363 
Batch[10509] - loss: 0.000467 
Batch[10510] - loss: 0.000279 
Batch[10511] - loss: 0.000502 
Batch[10512] - loss: 0.000266 
Batch[10513] - loss: 0.000424 
Batch[10514] - loss: 0.000513 
Batch[10515] - loss: 0.000393 
Batch[10516] - loss: 0.000264 
Batch[10517] - loss: 0.000870 
Batch[10518] - loss: 0.000517 
Batch[10519] - loss: 0.000391 
Batch[10520] - loss: 0.000449 
Batch[10521] - loss: 0.000295 
Batch[10522] - loss: 0.000293 
Batch[10523] - loss: 0.000287 
Batch[10524] - loss: 0.000189 
Batch[10525] - loss: 0.000451 
Batch[10526] - loss: 0.000337 
Batch[10527] - loss: 0.000470 
Batch[10528] - loss: 0.000310 
Batch[10529] - loss: 0.000309 
Batch[10530] - loss: 0.000416 
Batch[10531] - loss: 0.000452 
Batch[10532] - loss: 0.000308 
Batch[10533] - loss: 0.000373 
Batch[10534] - loss: 0.000483 
Batch[10535] - loss: 0.000427 
Batch[10536] - loss: 0.000421 
Batch[10537] - loss: 0.000376 
Batch[10538] - loss: 0.000325 
Batch[10539] - loss: 0.000643 
Batch[10540] - loss: 0.000538 
Batch[10541] - loss: 0.000489 
Batch[10542] - loss: 0.000485 
Batch[10543] - loss: 0.000272 
Batch[10544] - loss: 0.000268 
Batch[10545] - loss: 0.000388 
Batch[10546] - loss: 0.001185 
Batch[10547] - loss: 0.000366 
Batch[10548] - loss: 0.000752 
Batch[10549] - loss: 0.000258 
Batch[10550] - loss: 0.000298 
Batch[10551] - loss: 0.000505 
Batch[10552] - loss: 0.000300 
Batch[10553] - loss: 0.000272 
Batch[10554] - loss: 0.000187 
Batch[10555] - loss: 0.000669 
Batch[10556] - loss: 0.000487 
Batch[10557] - loss: 0.000368 
Batch[10558] - loss: 0.000124 
Batch[10559] - loss: 0.000334 
Batch[10560] - loss: 0.000539 
Batch[10561] - loss: 0.000202 
Batch[10562] - loss: 0.000248 
Batch[10563] - loss: 0.000300 
Batch[10564] - loss: 0.000364 
Batch[10565] - loss: 0.000372 
Batch[10566] - loss: 0.000307 
Batch[10567] - loss: 0.000247 
Batch[10568] - loss: 0.000517 
Batch[10569] - loss: 0.000486 
Batch[10570] - loss: 0.000401 
Batch[10571] - loss: 0.000205 
Batch[10572] - loss: 0.000471 
Batch[10573] - loss: 0.000293 
Batch[10574] - loss: 0.000349 
Batch[10575] - loss: 0.000306 
Batch[10576] - loss: 0.000472 
Batch[10577] - loss: 0.000577 
Batch[10578] - loss: 0.000578 
Batch[10579] - loss: 0.000430 
Batch[10580] - loss: 0.000432 
Batch[10581] - loss: 0.000620 
Batch[10582] - loss: 0.000248 
Batch[10583] - loss: 0.000281 
Batch[10584] - loss: 0.000584 
Batch[10585] - loss: 0.000523 
Batch[10586] - loss: 0.000318 
Batch[10587] - loss: 0.000344 
Batch[10588] - loss: 0.000276 
Batch[10589] - loss: 0.000243 
Batch[10590] - loss: 0.000365 
Batch[10591] - loss: 0.000273 
Batch[10592] - loss: 0.000418 
Batch[10593] - loss: 0.000302 
Batch[10594] - loss: 0.000417 
Batch[10595] - loss: 0.000557 
Batch[10596] - loss: 0.000336 
Batch[10597] - loss: 0.000482 
Batch[10598] - loss: 0.000311 
Batch[10599] - loss: 0.000387 
Batch[10600] - loss: 0.000508 

Evaluation - loss: 0.000070 pearson: 0.5548 

early stop by 1500 steps.
Batch[10601] - loss: 0.000391 
Batch[10602] - loss: 0.000417 
Batch[10603] - loss: 0.000366 
Batch[10604] - loss: 0.000261 
Batch[10605] - loss: 0.000293 
Batch[10606] - loss: 0.000568 
Batch[10607] - loss: 0.000493 
Batch[10608] - loss: 0.000381 
Batch[10609] - loss: 0.000244 
Batch[10610] - loss: 0.000396 
Batch[10611] - loss: 0.000495 
Batch[10612] - loss: 0.000364 
Batch[10613] - loss: 0.000479 
Batch[10614] - loss: 0.000331 
Batch[10615] - loss: 0.000430 
Batch[10616] - loss: 0.000264 
Batch[10617] - loss: 0.000768 
Batch[10618] - loss: 0.000667 
Batch[10619] - loss: 0.000331 
Batch[10620] - loss: 0.000389 
Batch[10621] - loss: 0.000299 
Batch[10622] - loss: 0.000473 
Batch[10623] - loss: 0.000299 
Batch[10624] - loss: 0.000470 
Batch[10625] - loss: 0.000617 
Batch[10626] - loss: 0.000285 
Batch[10627] - loss: 0.000480 
Batch[10628] - loss: 0.000368 
Batch[10629] - loss: 0.000269 
Batch[10630] - loss: 0.000350 
Batch[10631] - loss: 0.000426 
Batch[10632] - loss: 0.000343 
Batch[10633] - loss: 0.000273 
Batch[10634] - loss: 0.000514 
Batch[10635] - loss: 0.000266 
Batch[10636] - loss: 0.000483 
Batch[10637] - loss: 0.000323 
Batch[10638] - loss: 0.000399 
Batch[10639] - loss: 0.000536 
Batch[10640] - loss: 0.000541 
Batch[10641] - loss: 0.000436 
Batch[10642] - loss: 0.000355 
Batch[10643] - loss: 0.000194 
Batch[10644] - loss: 0.000271 
Batch[10645] - loss: 0.000280 
Batch[10646] - loss: 0.000530 
Batch[10647] - loss: 0.000287 
Batch[10648] - loss: 0.000428 
Batch[10649] - loss: 0.000457 
Batch[10650] - loss: 0.000336 
Batch[10651] - loss: 0.000495 
Batch[10652] - loss: 0.000450 
Batch[10653] - loss: 0.000401 
Batch[10654] - loss: 0.000408 
Batch[10655] - loss: 0.000344 
Batch[10656] - loss: 0.000631 
Batch[10657] - loss: 0.000955 
Batch[10658] - loss: 0.000469 
Batch[10659] - loss: 0.000352 
Batch[10660] - loss: 0.000416 
Batch[10661] - loss: 0.000370 
Batch[10662] - loss: 0.000362 
Batch[10663] - loss: 0.000315 
Batch[10664] - loss: 0.000512 
Batch[10665] - loss: 0.000302 
Batch[10666] - loss: 0.000252 
Batch[10667] - loss: 0.000340 
Batch[10668] - loss: 0.000440 
Batch[10669] - loss: 0.000538 
Batch[10670] - loss: 0.000524 
Batch[10671] - loss: 0.000283 
Batch[10672] - loss: 0.000339 
Batch[10673] - loss: 0.000306 
Batch[10674] - loss: 0.000425 
Batch[10675] - loss: 0.000481 
Batch[10676] - loss: 0.000403 
Batch[10677] - loss: 0.000462 
Batch[10678] - loss: 0.000461 
Batch[10679] - loss: 0.000315 
Batch[10680] - loss: 0.000491 
Batch[10681] - loss: 0.000389 
Batch[10682] - loss: 0.000283 
Batch[10683] - loss: 0.000383 
Batch[10684] - loss: 0.000370 
Batch[10685] - loss: 0.000418 
Batch[10686] - loss: 0.000890 
Batch[10687] - loss: 0.000274 
Batch[10688] - loss: 0.000288 
Batch[10689] - loss: 0.000276 
Batch[10690] - loss: 0.000157 
Batch[10691] - loss: 0.000295 
Batch[10692] - loss: 0.000493 
Batch[10693] - loss: 0.000276 
Batch[10694] - loss: 0.000455 
Batch[10695] - loss: 0.000529 
Batch[10696] - loss: 0.000429 
Batch[10697] - loss: 0.000288 
Batch[10698] - loss: 0.000447 
Batch[10699] - loss: 0.000713 
Batch[10700] - loss: 0.000270 

Evaluation - loss: 0.000070 pearson: 0.5556 

early stop by 1500 steps.
Batch[10701] - loss: 0.000368 
Batch[10702] - loss: 0.000426 
Batch[10703] - loss: 0.000313 
Batch[10704] - loss: 0.000373 
Batch[10705] - loss: 0.000603 
Batch[10706] - loss: 0.000304 
Batch[10707] - loss: 0.000230 
Batch[10708] - loss: 0.000300 
Batch[10709] - loss: 0.000317 
Batch[10710] - loss: 0.000595 
Batch[10711] - loss: 0.000408 
Batch[10712] - loss: 0.000554 
Batch[10713] - loss: 0.000173 
Batch[10714] - loss: 0.000294 
Batch[10715] - loss: 0.000317 
Batch[10716] - loss: 0.000300 
Batch[10717] - loss: 0.000174 
Batch[10718] - loss: 0.000376 
Batch[10719] - loss: 0.000279 
Batch[10720] - loss: 0.000464 
Batch[10721] - loss: 0.000261 
Batch[10722] - loss: 0.000374 
Batch[10723] - loss: 0.000419 
Batch[10724] - loss: 0.000622 
Batch[10725] - loss: 0.000895 
Batch[10726] - loss: 0.000379 
Batch[10727] - loss: 0.000762 
Batch[10728] - loss: 0.000223 
Batch[10729] - loss: 0.000419 
Batch[10730] - loss: 0.000308 
Batch[10731] - loss: 0.000914 
Batch[10732] - loss: 0.000255 
Batch[10733] - loss: 0.000433 
Batch[10734] - loss: 0.000432 
Batch[10735] - loss: 0.000230 
Batch[10736] - loss: 0.000440 
Batch[10737] - loss: 0.000364 
Batch[10738] - loss: 0.000381 
Batch[10739] - loss: 0.000399 
Batch[10740] - loss: 0.000357 
Batch[10741] - loss: 0.000356 
Batch[10742] - loss: 0.000460 
Batch[10743] - loss: 0.000510 
Batch[10744] - loss: 0.000548 
Batch[10745] - loss: 0.000531 
Batch[10746] - loss: 0.000339 
Batch[10747] - loss: 0.000356 
Batch[10748] - loss: 0.000348 
Batch[10749] - loss: 0.000394 
Batch[10750] - loss: 0.000341 
Batch[10751] - loss: 0.000323 
Batch[10752] - loss: 0.000268 
Batch[10753] - loss: 0.000416 
Batch[10754] - loss: 0.000270 
Batch[10755] - loss: 0.000402 
Batch[10756] - loss: 0.000336 
Batch[10757] - loss: 0.000381 
Batch[10758] - loss: 0.000197 
Batch[10759] - loss: 0.000502 
Batch[10760] - loss: 0.000579 
Batch[10761] - loss: 0.000535 
Batch[10762] - loss: 0.000380 
Batch[10763] - loss: 0.000244 
Batch[10764] - loss: 0.000706 
Batch[10765] - loss: 0.000456 
Batch[10766] - loss: 0.000237 
Batch[10767] - loss: 0.000297 
Batch[10768] - loss: 0.000468 
Batch[10769] - loss: 0.000219 
Batch[10770] - loss: 0.000263 
Batch[10771] - loss: 0.000807 
Batch[10772] - loss: 0.000503 
Batch[10773] - loss: 0.000245 
Batch[10774] - loss: 0.000381 
Batch[10775] - loss: 0.000297 
Batch[10776] - loss: 0.000302 
Batch[10777] - loss: 0.000310 
Batch[10778] - loss: 0.000608 
Batch[10779] - loss: 0.000634 
Batch[10780] - loss: 0.000453 
Batch[10781] - loss: 0.000251 
Batch[10782] - loss: 0.000298 
Batch[10783] - loss: 0.000406 
Batch[10784] - loss: 0.000826 
Batch[10785] - loss: 0.000441 
Batch[10786] - loss: 0.000445 
Batch[10787] - loss: 0.000328 
Batch[10788] - loss: 0.000568 
Batch[10789] - loss: 0.000313 
Batch[10790] - loss: 0.000191 
Batch[10791] - loss: 0.000723 
Batch[10792] - loss: 0.000778 
Batch[10793] - loss: 0.000262 
Batch[10794] - loss: 0.000348 
Batch[10795] - loss: 0.000329 
Batch[10796] - loss: 0.000400 
Batch[10797] - loss: 0.000573 
Batch[10798] - loss: 0.000309 
Batch[10799] - loss: 0.000455 
Batch[10800] - loss: 0.000574 

Evaluation - loss: 0.000070 pearson: 0.5554 

early stop by 1500 steps.
Batch[10801] - loss: 0.000595 
Batch[10802] - loss: 0.000525 
Batch[10803] - loss: 0.000273 
Batch[10804] - loss: 0.000421 
Batch[10805] - loss: 0.000468 
Batch[10806] - loss: 0.000226 
Batch[10807] - loss: 0.000554 
Batch[10808] - loss: 0.000557 
Batch[10809] - loss: 0.000222 
Batch[10810] - loss: 0.000445 
Batch[10811] - loss: 0.000263 
Batch[10812] - loss: 0.000420 
Batch[10813] - loss: 0.000338 
Batch[10814] - loss: 0.000462 
Batch[10815] - loss: 0.000269 
Batch[10816] - loss: 0.000373 
Batch[10817] - loss: 0.000510 
Batch[10818] - loss: 0.000886 
Batch[10819] - loss: 0.000425 
Batch[10820] - loss: 0.000461 
Batch[10821] - loss: 0.000407 
Batch[10822] - loss: 0.000192 
Batch[10823] - loss: 0.000346 
Batch[10824] - loss: 0.000351 
Batch[10825] - loss: 0.000449 
Batch[10826] - loss: 0.000264 
Batch[10827] - loss: 0.000507 
Batch[10828] - loss: 0.000250 
Batch[10829] - loss: 0.000280 
Batch[10830] - loss: 0.000338 
Batch[10831] - loss: 0.000541 
Batch[10832] - loss: 0.000665 
Batch[10833] - loss: 0.000198 
Batch[10834] - loss: 0.000273 
Batch[10835] - loss: 0.000355 
Batch[10836] - loss: 0.000214 
Batch[10837] - loss: 0.000386 
Batch[10838] - loss: 0.000439 
Batch[10839] - loss: 0.000424 
Batch[10840] - loss: 0.000341 
Batch[10841] - loss: 0.000282 
Batch[10842] - loss: 0.000357 
Batch[10843] - loss: 0.000805 
Batch[10844] - loss: 0.000411 
Batch[10845] - loss: 0.000425 
Batch[10846] - loss: 0.000539 
Batch[10847] - loss: 0.000611 
Batch[10848] - loss: 0.000382 
Batch[10849] - loss: 0.000305 
Batch[10850] - loss: 0.000373 
Batch[10851] - loss: 0.000478 
Batch[10852] - loss: 0.000386 
Batch[10853] - loss: 0.000323 
Batch[10854] - loss: 0.000317 
Batch[10855] - loss: 0.000703 
Batch[10856] - loss: 0.000673 
Batch[10857] - loss: 0.000649 
Batch[10858] - loss: 0.000500 
Batch[10859] - loss: 0.000332 
Batch[10860] - loss: 0.000378 
Batch[10861] - loss: 0.000325 
Batch[10862] - loss: 0.000348 
Batch[10863] - loss: 0.000452 
Batch[10864] - loss: 0.000309 
Batch[10865] - loss: 0.000403 
Batch[10866] - loss: 0.000569 
Batch[10867] - loss: 0.000396 
Batch[10868] - loss: 0.000449 
Batch[10869] - loss: 0.000395 
Batch[10870] - loss: 0.000303 
Batch[10871] - loss: 0.000367 
Batch[10872] - loss: 0.000378 
Batch[10873] - loss: 0.000487 
Batch[10874] - loss: 0.000315 
Batch[10875] - loss: 0.000309 
Batch[10876] - loss: 0.000597 
Batch[10877] - loss: 0.000449 
Batch[10878] - loss: 0.000420 
Batch[10879] - loss: 0.000230 
Batch[10880] - loss: 0.000253 
Batch[10881] - loss: 0.000548 
Batch[10882] - loss: 0.000362 
Batch[10883] - loss: 0.000317 
Batch[10884] - loss: 0.000622 
Batch[10885] - loss: 0.000537 
Batch[10886] - loss: 0.000299 
Batch[10887] - loss: 0.000522 
Batch[10888] - loss: 0.000599 
Batch[10889] - loss: 0.000444 
Batch[10890] - loss: 0.000510 
Batch[10891] - loss: 0.000248 
Batch[10892] - loss: 0.000239 
Batch[10893] - loss: 0.000460 
Batch[10894] - loss: 0.000492 
Batch[10895] - loss: 0.000291 
Batch[10896] - loss: 0.000232 
Batch[10897] - loss: 0.000358 
Batch[10898] - loss: 0.000371 
Batch[10899] - loss: 0.000453 
Batch[10900] - loss: 0.000224 

Evaluation - loss: 0.000070 pearson: 0.5534 

early stop by 1500 steps.
Batch[10901] - loss: 0.000483 
Batch[10902] - loss: 0.000212 
Batch[10903] - loss: 0.000467 
Batch[10904] - loss: 0.000473 
Batch[10905] - loss: 0.000345 
Batch[10906] - loss: 0.000246 
Batch[10907] - loss: 0.000404 
Batch[10908] - loss: 0.000220 
Batch[10909] - loss: 0.000229 
Batch[10910] - loss: 0.000434 
Batch[10911] - loss: 0.000549 
Batch[10912] - loss: 0.000598 
Batch[10913] - loss: 0.000282 
Batch[10914] - loss: 0.000428 
Batch[10915] - loss: 0.000332 
Batch[10916] - loss: 0.000451 
Batch[10917] - loss: 0.000355 
Batch[10918] - loss: 0.000230 
Batch[10919] - loss: 0.000192 
Batch[10920] - loss: 0.000398 
Batch[10921] - loss: 0.000660 
Batch[10922] - loss: 0.000520 
Batch[10923] - loss: 0.000412 
Batch[10924] - loss: 0.000254 
Batch[10925] - loss: 0.000322 
Batch[10926] - loss: 0.000643 
Batch[10927] - loss: 0.000372 
Batch[10928] - loss: 0.000553 
Batch[10929] - loss: 0.000473 
Batch[10930] - loss: 0.000571 
Batch[10931] - loss: 0.000460 
Batch[10932] - loss: 0.000340 
Batch[10933] - loss: 0.000215 
Batch[10934] - loss: 0.000262 
Batch[10935] - loss: 0.000440 
Batch[10936] - loss: 0.000644 
Batch[10937] - loss: 0.000714 
Batch[10938] - loss: 0.000332 
Batch[10939] - loss: 0.000390 
Batch[10940] - loss: 0.000300 
Batch[10941] - loss: 0.000421 
Batch[10942] - loss: 0.000609 
Batch[10943] - loss: 0.000395 
Batch[10944] - loss: 0.000385 
Batch[10945] - loss: 0.000274 
Batch[10946] - loss: 0.000237 
Batch[10947] - loss: 0.000386 
Batch[10948] - loss: 0.000376 
Batch[10949] - loss: 0.000609 
Batch[10950] - loss: 0.000384 
Batch[10951] - loss: 0.000404 
Batch[10952] - loss: 0.000295 
Batch[10953] - loss: 0.000254 
Batch[10954] - loss: 0.000232 
Batch[10955] - loss: 0.000287 
Batch[10956] - loss: 0.000551 
Batch[10957] - loss: 0.000338 
Batch[10958] - loss: 0.000294 
Batch[10959] - loss: 0.000301 
Batch[10960] - loss: 0.000245 
Batch[10961] - loss: 0.000473 
Batch[10962] - loss: 0.000343 
Batch[10963] - loss: 0.000175 
Batch[10964] - loss: 0.000227 
Batch[10965] - loss: 0.000650 
Batch[10966] - loss: 0.000228 
Batch[10967] - loss: 0.000327 
Batch[10968] - loss: 0.000606 
Batch[10969] - loss: 0.000577 
Batch[10970] - loss: 0.000285 
Batch[10971] - loss: 0.000614 
Batch[10972] - loss: 0.000178 
Batch[10973] - loss: 0.000462 
Batch[10974] - loss: 0.000492 
Batch[10975] - loss: 0.000513 
Batch[10976] - loss: 0.000698 
Batch[10977] - loss: 0.000365 
Batch[10978] - loss: 0.000383 
Batch[10979] - loss: 0.000511 
Batch[10980] - loss: 0.000229 
Batch[10981] - loss: 0.000420 
Batch[10982] - loss: 0.000303 
Batch[10983] - loss: 0.000435 
Batch[10984] - loss: 0.000304 
Batch[10985] - loss: 0.000421 
Batch[10986] - loss: 0.000263 
Batch[10987] - loss: 0.000395 
Batch[10988] - loss: 0.000371 
Batch[10989] - loss: 0.000262 
Batch[10990] - loss: 0.000368 
Batch[10991] - loss: 0.000251 
Batch[10992] - loss: 0.000628 
Batch[10993] - loss: 0.000342 
Batch[10994] - loss: 0.000204 
Batch[10995] - loss: 0.000399 
Batch[10996] - loss: 0.000311 
Batch[10997] - loss: 0.000405 
Batch[10998] - loss: 0.000256 
Batch[10999] - loss: 0.000245 
Batch[11000] - loss: 0.000406 

Evaluation - loss: 0.000070 pearson: 0.5528 

early stop by 1500 steps.
Batch[11001] - loss: 0.000332 
Batch[11002] - loss: 0.000377 
Batch[11003] - loss: 0.000441 
Batch[11004] - loss: 0.000503 
Batch[11005] - loss: 0.000318 
Batch[11006] - loss: 0.000536 
Batch[11007] - loss: 0.000460 
Batch[11008] - loss: 0.000577 
Batch[11009] - loss: 0.000485 
Batch[11010] - loss: 0.001101 
Batch[11011] - loss: 0.000383 
Batch[11012] - loss: 0.000387 
Batch[11013] - loss: 0.000276 
Batch[11014] - loss: 0.000338 
Batch[11015] - loss: 0.000398 
Batch[11016] - loss: 0.000481 
Batch[11017] - loss: 0.000319 
Batch[11018] - loss: 0.000402 
Batch[11019] - loss: 0.000352 
Batch[11020] - loss: 0.000322 
Batch[11021] - loss: 0.000352 
Batch[11022] - loss: 0.000290 
Batch[11023] - loss: 0.000361 
Batch[11024] - loss: 0.000290 
Batch[11025] - loss: 0.000494 
Batch[11026] - loss: 0.000206 
Batch[11027] - loss: 0.000559 
Batch[11028] - loss: 0.000314 
Batch[11029] - loss: 0.000322 
Batch[11030] - loss: 0.000399 
Batch[11031] - loss: 0.000244 
Batch[11032] - loss: 0.000242 
Batch[11033] - loss: 0.000338 
Batch[11034] - loss: 0.000314 
Batch[11035] - loss: 0.000377 
Batch[11036] - loss: 0.000542 
Batch[11037] - loss: 0.000424 
Batch[11038] - loss: 0.000701 
Batch[11039] - loss: 0.000177 
Batch[11040] - loss: 0.000569 
Batch[11041] - loss: 0.000341 
Batch[11042] - loss: 0.000346 
Batch[11043] - loss: 0.000168 
Batch[11044] - loss: 0.000314 
Batch[11045] - loss: 0.000200 
Batch[11046] - loss: 0.000281 
Batch[11047] - loss: 0.000212 
Batch[11048] - loss: 0.000222 
Batch[11049] - loss: 0.000767 
Batch[11050] - loss: 0.000529 
Batch[11051] - loss: 0.000222 
Batch[11052] - loss: 0.000231 
Batch[11053] - loss: 0.000456 
Batch[11054] - loss: 0.000268 
Batch[11055] - loss: 0.000380 
Batch[11056] - loss: 0.000359 
Batch[11057] - loss: 0.000315 
Batch[11058] - loss: 0.000355 
Batch[11059] - loss: 0.000469 
Batch[11060] - loss: 0.000314 
Batch[11061] - loss: 0.000403 
Batch[11062] - loss: 0.000509 
Batch[11063] - loss: 0.000343 
Batch[11064] - loss: 0.000325 
Batch[11065] - loss: 0.000368 
Batch[11066] - loss: 0.000438 
Batch[11067] - loss: 0.000282 
Batch[11068] - loss: 0.000581 
Batch[11069] - loss: 0.000262 
Batch[11070] - loss: 0.000444 
Batch[11071] - loss: 0.000365 
Batch[11072] - loss: 0.000660 
Batch[11073] - loss: 0.000633 
Batch[11074] - loss: 0.000373 
Batch[11075] - loss: 0.000612 
Batch[11076] - loss: 0.000726 
Batch[11077] - loss: 0.000268 
Batch[11078] - loss: 0.000189 
Batch[11079] - loss: 0.000860 
Batch[11080] - loss: 0.000188 
Batch[11081] - loss: 0.000240 
Batch[11082] - loss: 0.000291 
Batch[11083] - loss: 0.000181 
Batch[11084] - loss: 0.000367 
Batch[11085] - loss: 0.000422 
Batch[11086] - loss: 0.000573 
Batch[11087] - loss: 0.000351 
Batch[11088] - loss: 0.000568 
Batch[11089] - loss: 0.000231 
Batch[11090] - loss: 0.000455 
Batch[11091] - loss: 0.000440 
Batch[11092] - loss: 0.000358 
Batch[11093] - loss: 0.000423 
Batch[11094] - loss: 0.000314 
Batch[11095] - loss: 0.000276 
Batch[11096] - loss: 0.000323 
Batch[11097] - loss: 0.000337 
Batch[11098] - loss: 0.000302 
Batch[11099] - loss: 0.000359 
Batch[11100] - loss: 0.000245 

Evaluation - loss: 0.000070 pearson: 0.5544 

early stop by 1500 steps.
Batch[11101] - loss: 0.000517 
Batch[11102] - loss: 0.000621 
Batch[11103] - loss: 0.000328 
Batch[11104] - loss: 0.000454 
Batch[11105] - loss: 0.000262 
Batch[11106] - loss: 0.000305 
Batch[11107] - loss: 0.000482 
Batch[11108] - loss: 0.000271 
Batch[11109] - loss: 0.000500 
Batch[11110] - loss: 0.000222 
Batch[11111] - loss: 0.000171 
Batch[11112] - loss: 0.000356 
Batch[11113] - loss: 0.000304 
Batch[11114] - loss: 0.000316 
Batch[11115] - loss: 0.000245 
Batch[11116] - loss: 0.000349 
Batch[11117] - loss: 0.000244 
Batch[11118] - loss: 0.000329 
Batch[11119] - loss: 0.000234 
Batch[11120] - loss: 0.000202 
Batch[11121] - loss: 0.000378 
Batch[11122] - loss: 0.000500 
Batch[11123] - loss: 0.000395 
Batch[11124] - loss: 0.000460 
Batch[11125] - loss: 0.000431 
Batch[11126] - loss: 0.000345 
Batch[11127] - loss: 0.000364 
Batch[11128] - loss: 0.000345 
Batch[11129] - loss: 0.000332 
Batch[11130] - loss: 0.000426 
Batch[11131] - loss: 0.000318 
Batch[11132] - loss: 0.000477 
Batch[11133] - loss: 0.000351 
Batch[11134] - loss: 0.000471 
Batch[11135] - loss: 0.000403 
Batch[11136] - loss: 0.000318 
Batch[11137] - loss: 0.000499 
Batch[11138] - loss: 0.000392 
Batch[11139] - loss: 0.000394 
Batch[11140] - loss: 0.000251 
Batch[11141] - loss: 0.000276 
Batch[11142] - loss: 0.000386 
Batch[11143] - loss: 0.000439 
Batch[11144] - loss: 0.000293 
Batch[11145] - loss: 0.000371 
Batch[11146] - loss: 0.000717 
Batch[11147] - loss: 0.000404 
Batch[11148] - loss: 0.000323 
Batch[11149] - loss: 0.000330 
Batch[11150] - loss: 0.000900 
Batch[11151] - loss: 0.000491 
Batch[11152] - loss: 0.000286 
Batch[11153] - loss: 0.000382 
Batch[11154] - loss: 0.000405 
Batch[11155] - loss: 0.000172 
Batch[11156] - loss: 0.000410 
Batch[11157] - loss: 0.000286 
Batch[11158] - loss: 0.000350 
Batch[11159] - loss: 0.000351 
Batch[11160] - loss: 0.000349 
Batch[11161] - loss: 0.000480 
Batch[11162] - loss: 0.000326 
Batch[11163] - loss: 0.000658 
Batch[11164] - loss: 0.000487 
Batch[11165] - loss: 0.000238 
Batch[11166] - loss: 0.000355 
Batch[11167] - loss: 0.000352 
Batch[11168] - loss: 0.000241 
Batch[11169] - loss: 0.000413 
Batch[11170] - loss: 0.000268 
Batch[11171] - loss: 0.000252 
Batch[11172] - loss: 0.000328 
Batch[11173] - loss: 0.000382 
Batch[11174] - loss: 0.000356 
Batch[11175] - loss: 0.000283 
Batch[11176] - loss: 0.000298 
Batch[11177] - loss: 0.000183 
Batch[11178] - loss: 0.001397 
Batch[11179] - loss: 0.000246 
Batch[11180] - loss: 0.000299 
Batch[11181] - loss: 0.000510 
Batch[11182] - loss: 0.000268 
Batch[11183] - loss: 0.000513 
Batch[11184] - loss: 0.000416 
Batch[11185] - loss: 0.000285 
Batch[11186] - loss: 0.000251 
Batch[11187] - loss: 0.000452 
Batch[11188] - loss: 0.000422 
Batch[11189] - loss: 0.000292 
Batch[11190] - loss: 0.000328 
Batch[11191] - loss: 0.000246 
Batch[11192] - loss: 0.000210 
Batch[11193] - loss: 0.000166 
Batch[11194] - loss: 0.000258 
Batch[11195] - loss: 0.000475 
Batch[11196] - loss: 0.000169 
Batch[11197] - loss: 0.000401 
Batch[11198] - loss: 0.000267 
Batch[11199] - loss: 0.000343 
Batch[11200] - loss: 0.000305 

Evaluation - loss: 0.000070 pearson: 0.5549 

early stop by 1500 steps.
Batch[11201] - loss: 0.000335 
Batch[11202] - loss: 0.000338 
Batch[11203] - loss: 0.000325 
Batch[11204] - loss: 0.000658 
Batch[11205] - loss: 0.000364 
Batch[11206] - loss: 0.000367 
Batch[11207] - loss: 0.000446 
Batch[11208] - loss: 0.000265 
Batch[11209] - loss: 0.000172 
Batch[11210] - loss: 0.000562 
Batch[11211] - loss: 0.000253 
Batch[11212] - loss: 0.000417 
Batch[11213] - loss: 0.000305 
Batch[11214] - loss: 0.000179 
Batch[11215] - loss: 0.000491 
Batch[11216] - loss: 0.000724 
Batch[11217] - loss: 0.000509 
Batch[11218] - loss: 0.000491 
Batch[11219] - loss: 0.000194 
Batch[11220] - loss: 0.000506 
Batch[11221] - loss: 0.000261 
Batch[11222] - loss: 0.000347 
Batch[11223] - loss: 0.000305 
Batch[11224] - loss: 0.000369 
Batch[11225] - loss: 0.000181 
Batch[11226] - loss: 0.000321 
Batch[11227] - loss: 0.000485 
Batch[11228] - loss: 0.000328 
Batch[11229] - loss: 0.000226 
Batch[11230] - loss: 0.000371 
Batch[11231] - loss: 0.000299 
Batch[11232] - loss: 0.000231 
Batch[11233] - loss: 0.000382 
Batch[11234] - loss: 0.000164 
Batch[11235] - loss: 0.000878 
Batch[11236] - loss: 0.000454 
Batch[11237] - loss: 0.000291 
Batch[11238] - loss: 0.000437 
Batch[11239] - loss: 0.000218 
Batch[11240] - loss: 0.000264 
Batch[11241] - loss: 0.000370 
Batch[11242] - loss: 0.000281 
Batch[11243] - loss: 0.000362 
Batch[11244] - loss: 0.000867 
Batch[11245] - loss: 0.000335 
Batch[11246] - loss: 0.000309 
Batch[11247] - loss: 0.000363 
Batch[11248] - loss: 0.000299 
Batch[11249] - loss: 0.000360 
Batch[11250] - loss: 0.000271 
Batch[11251] - loss: 0.000380 
Batch[11252] - loss: 0.000213 
Batch[11253] - loss: 0.000158 
Batch[11254] - loss: 0.000318 
Batch[11255] - loss: 0.000385 
Batch[11256] - loss: 0.000223 
Batch[11257] - loss: 0.000393 
Batch[11258] - loss: 0.000215 
Batch[11259] - loss: 0.000691 
Batch[11260] - loss: 0.000219 
Batch[11261] - loss: 0.000390 
Batch[11262] - loss: 0.000488 
Batch[11263] - loss: 0.000510 
Batch[11264] - loss: 0.000212 
Batch[11265] - loss: 0.000287 
Batch[11266] - loss: 0.000359 
Batch[11267] - loss: 0.000270 
Batch[11268] - loss: 0.000365 
Batch[11269] - loss: 0.000259 
Batch[11270] - loss: 0.000454 
Batch[11271] - loss: 0.000298 
Batch[11272] - loss: 0.000235 
Batch[11273] - loss: 0.000271 
Batch[11274] - loss: 0.000186 
Batch[11275] - loss: 0.000240 
Batch[11276] - loss: 0.000417 
Batch[11277] - loss: 0.000422 
Batch[11278] - loss: 0.000202 
Batch[11279] - loss: 0.000418 
Batch[11280] - loss: 0.000559 
Batch[11281] - loss: 0.000196 
Batch[11282] - loss: 0.000213 
Batch[11283] - loss: 0.000274 
Batch[11284] - loss: 0.000198 
Batch[11285] - loss: 0.000286 
Batch[11286] - loss: 0.000326 
Batch[11287] - loss: 0.000559 
Batch[11288] - loss: 0.000451 
Batch[11289] - loss: 0.000281 
Batch[11290] - loss: 0.000435 
Batch[11291] - loss: 0.000404 
Batch[11292] - loss: 0.000284 
Batch[11293] - loss: 0.000331 
Batch[11294] - loss: 0.000356 
Batch[11295] - loss: 0.000813 
Batch[11296] - loss: 0.000228 
Batch[11297] - loss: 0.000287 
Batch[11298] - loss: 0.000443 
Batch[11299] - loss: 0.000428 
Batch[11300] - loss: 0.000237 

Evaluation - loss: 0.000070 pearson: 0.5554 

early stop by 1500 steps.
Batch[11301] - loss: 0.000566 
Batch[11302] - loss: 0.000416 
Batch[11303] - loss: 0.000267 
Batch[11304] - loss: 0.000204 
Batch[11305] - loss: 0.000400 
Batch[11306] - loss: 0.000446 
Batch[11307] - loss: 0.000339 
Batch[11308] - loss: 0.000291 
Batch[11309] - loss: 0.000479 
Batch[11310] - loss: 0.000205 
Batch[11311] - loss: 0.000353 
Batch[11312] - loss: 0.000261 
Batch[11313] - loss: 0.000190 
Batch[11314] - loss: 0.000244 
Batch[11315] - loss: 0.000445 
Batch[11316] - loss: 0.000269 
Batch[11317] - loss: 0.000320 
Batch[11318] - loss: 0.000360 
Batch[11319] - loss: 0.000438 
Batch[11320] - loss: 0.000243 
Batch[11321] - loss: 0.000276 
Batch[11322] - loss: 0.000412 
Batch[11323] - loss: 0.000203 
Batch[11324] - loss: 0.000342 
Batch[11325] - loss: 0.000195 
Batch[11326] - loss: 0.000227 
Batch[11327] - loss: 0.000344 
Batch[11328] - loss: 0.000305 
Batch[11329] - loss: 0.000511 
Batch[11330] - loss: 0.000444 
Batch[11331] - loss: 0.000275 
Batch[11332] - loss: 0.000297 
Batch[11333] - loss: 0.000307 
Batch[11334] - loss: 0.000372 
Batch[11335] - loss: 0.000435 
Batch[11336] - loss: 0.000416 
Batch[11337] - loss: 0.000398 
Batch[11338] - loss: 0.000251 
Batch[11339] - loss: 0.000337 
Batch[11340] - loss: 0.000471 
Batch[11341] - loss: 0.000302 
Batch[11342] - loss: 0.000349 
Batch[11343] - loss: 0.000249 
Batch[11344] - loss: 0.000565 
Batch[11345] - loss: 0.000331 
Batch[11346] - loss: 0.000469 
Batch[11347] - loss: 0.000532 
Batch[11348] - loss: 0.000649 
Batch[11349] - loss: 0.000844 
Batch[11350] - loss: 0.000559 
Batch[11351] - loss: 0.000309 
Batch[11352] - loss: 0.000232 
Batch[11353] - loss: 0.000391 
Batch[11354] - loss: 0.000246 
Batch[11355] - loss: 0.000229 
Batch[11356] - loss: 0.000245 
Batch[11357] - loss: 0.000253 
Batch[11358] - loss: 0.000373 
Batch[11359] - loss: 0.000266 
Batch[11360] - loss: 0.000198 
Batch[11361] - loss: 0.000316 
Batch[11362] - loss: 0.000325 
Batch[11363] - loss: 0.000294 
Batch[11364] - loss: 0.000271 
Batch[11365] - loss: 0.000243 
Batch[11366] - loss: 0.000222 
Batch[11367] - loss: 0.000338 
Batch[11368] - loss: 0.000619 
Batch[11369] - loss: 0.000188 
Batch[11370] - loss: 0.000358 
Batch[11371] - loss: 0.000625 
Batch[11372] - loss: 0.000225 
Batch[11373] - loss: 0.000265 
Batch[11374] - loss: 0.000264 
Batch[11375] - loss: 0.000217 
Batch[11376] - loss: 0.000225 
Batch[11377] - loss: 0.000525 
Batch[11378] - loss: 0.000270 
Batch[11379] - loss: 0.000260 
Batch[11380] - loss: 0.000471 
Batch[11381] - loss: 0.000396 
Batch[11382] - loss: 0.000223 
Batch[11383] - loss: 0.000371 
Batch[11384] - loss: 0.000288 
Batch[11385] - loss: 0.000269 
Batch[11386] - loss: 0.000495 
Batch[11387] - loss: 0.000417 
Batch[11388] - loss: 0.000289 
Batch[11389] - loss: 0.000220 
Batch[11390] - loss: 0.000384 
Batch[11391] - loss: 0.000180 
Batch[11392] - loss: 0.000219 
Batch[11393] - loss: 0.000524 
Batch[11394] - loss: 0.000564 
Batch[11395] - loss: 0.000376 
Batch[11396] - loss: 0.000274 
Batch[11397] - loss: 0.000476 
Batch[11398] - loss: 0.000311 
Batch[11399] - loss: 0.000318 
Batch[11400] - loss: 0.000404 

Evaluation - loss: 0.000070 pearson: 0.5547 

early stop by 1500 steps.
Batch[11401] - loss: 0.000199 
Batch[11402] - loss: 0.000232 
Batch[11403] - loss: 0.000540 
Batch[11404] - loss: 0.000220 
Batch[11405] - loss: 0.000575 
Batch[11406] - loss: 0.000585 
Batch[11407] - loss: 0.000328 
Batch[11408] - loss: 0.000201 
Batch[11409] - loss: 0.000225 
Batch[11410] - loss: 0.000254 
Batch[11411] - loss: 0.000544 
Batch[11412] - loss: 0.000247 
Batch[11413] - loss: 0.000246 
Batch[11414] - loss: 0.000222 
Batch[11415] - loss: 0.000365 
Batch[11416] - loss: 0.000320 
Batch[11417] - loss: 0.000404 
Batch[11418] - loss: 0.000326 
Batch[11419] - loss: 0.000313 
Batch[11420] - loss: 0.000374 
Batch[11421] - loss: 0.000291 
Batch[11422] - loss: 0.000325 
Batch[11423] - loss: 0.000201 
Batch[11424] - loss: 0.000393 
Batch[11425] - loss: 0.000334 
Batch[11426] - loss: 0.000310 
Batch[11427] - loss: 0.000417 
Batch[11428] - loss: 0.000395 
Batch[11429] - loss: 0.000362 
Batch[11430] - loss: 0.000231 
Batch[11431] - loss: 0.000465 
Batch[11432] - loss: 0.000374 
Batch[11433] - loss: 0.000306 
Batch[11434] - loss: 0.000260 
Batch[11435] - loss: 0.000477 
Batch[11436] - loss: 0.000329 
Batch[11437] - loss: 0.000347 
Batch[11438] - loss: 0.000394 
Batch[11439] - loss: 0.000379 
Batch[11440] - loss: 0.000218 
Batch[11441] - loss: 0.000407 
Batch[11442] - loss: 0.000449 
Batch[11443] - loss: 0.000437 
Batch[11444] - loss: 0.000242 
Batch[11445] - loss: 0.000251 
Batch[11446] - loss: 0.000398 
Batch[11447] - loss: 0.000294 
Batch[11448] - loss: 0.000353 
Batch[11449] - loss: 0.000307 
Batch[11450] - loss: 0.000297 
Batch[11451] - loss: 0.000236 
Batch[11452] - loss: 0.000623 
Batch[11453] - loss: 0.000223 
Batch[11454] - loss: 0.000283 
Batch[11455] - loss: 0.000183 
Batch[11456] - loss: 0.000176 
Batch[11457] - loss: 0.000450 
Batch[11458] - loss: 0.000253 
Batch[11459] - loss: 0.000287 
Batch[11460] - loss: 0.000230 
Batch[11461] - loss: 0.000257 
Batch[11462] - loss: 0.000281 
Batch[11463] - loss: 0.000349 
Batch[11464] - loss: 0.000353 
Batch[11465] - loss: 0.000293 
Batch[11466] - loss: 0.000264 
Batch[11467] - loss: 0.000328 
Batch[11468] - loss: 0.000358 
Batch[11469] - loss: 0.000508 
Batch[11470] - loss: 0.000259 
Batch[11471] - loss: 0.000327 
Batch[11472] - loss: 0.000345 
Batch[11473] - loss: 0.000433 
Batch[11474] - loss: 0.000217 
Batch[11475] - loss: 0.000455 
Batch[11476] - loss: 0.000261 
Batch[11477] - loss: 0.000257 
Batch[11478] - loss: 0.000260 
Batch[11479] - loss: 0.000200 
Batch[11480] - loss: 0.000374 
Batch[11481] - loss: 0.000430 
Batch[11482] - loss: 0.000272 
Batch[11483] - loss: 0.000238 
Batch[11484] - loss: 0.000576 
Batch[11485] - loss: 0.000294 
Batch[11486] - loss: 0.000373 
Batch[11487] - loss: 0.000314 
Batch[11488] - loss: 0.000220 
Batch[11489] - loss: 0.000382 
Batch[11490] - loss: 0.000489 
Batch[11491] - loss: 0.000232 
Batch[11492] - loss: 0.000230 
Batch[11493] - loss: 0.000427 
Batch[11494] - loss: 0.000248 
Batch[11495] - loss: 0.000375 
Batch[11496] - loss: 0.000250 
Batch[11497] - loss: 0.000219 
Batch[11498] - loss: 0.000557 
Batch[11499] - loss: 0.000276 
Batch[11500] - loss: 0.000327 

Evaluation - loss: 0.000071 pearson: 0.5529 

early stop by 1500 steps.
Batch[11501] - loss: 0.000253 
Batch[11502] - loss: 0.000268 
Batch[11503] - loss: 0.000373 
Batch[11504] - loss: 0.000505 
Batch[11505] - loss: 0.000220 
Batch[11506] - loss: 0.000422 
Batch[11507] - loss: 0.000272 
Batch[11508] - loss: 0.000411 
Batch[11509] - loss: 0.000446 
Batch[11510] - loss: 0.000166 
Batch[11511] - loss: 0.000123 
Batch[11512] - loss: 0.000224 
Batch[11513] - loss: 0.000311 
Batch[11514] - loss: 0.000433 
Batch[11515] - loss: 0.000404 
Batch[11516] - loss: 0.000430 
Batch[11517] - loss: 0.000418 
Batch[11518] - loss: 0.000452 
Batch[11519] - loss: 0.000301 
Batch[11520] - loss: 0.000298 
Batch[11521] - loss: 0.000266 
Batch[11522] - loss: 0.000297 
Batch[11523] - loss: 0.000902 
Batch[11524] - loss: 0.000366 
Batch[11525] - loss: 0.000359 
Batch[11526] - loss: 0.000440 
Batch[11527] - loss: 0.000281 
Batch[11528] - loss: 0.000239 
Batch[11529] - loss: 0.000236 
Batch[11530] - loss: 0.000231 
Batch[11531] - loss: 0.000372 
Batch[11532] - loss: 0.000188 
Batch[11533] - loss: 0.000372 
Batch[11534] - loss: 0.000500 
Batch[11535] - loss: 0.000251 
Batch[11536] - loss: 0.000329 
Batch[11537] - loss: 0.000274 
Batch[11538] - loss: 0.000292 
Batch[11539] - loss: 0.000271 
Batch[11540] - loss: 0.000462 
Batch[11541] - loss: 0.000336 
Batch[11542] - loss: 0.000174 
Batch[11543] - loss: 0.000362 
Batch[11544] - loss: 0.000193 
Batch[11545] - loss: 0.000329 
Batch[11546] - loss: 0.000319 
Batch[11547] - loss: 0.000353 
Batch[11548] - loss: 0.000189 
Batch[11549] - loss: 0.000175 
Batch[11550] - loss: 0.000367 
Batch[11551] - loss: 0.000468 
Batch[11552] - loss: 0.000255 
Batch[11553] - loss: 0.000178 
Batch[11554] - loss: 0.000234 
Batch[11555] - loss: 0.000466 
Batch[11556] - loss: 0.000284 
Batch[11557] - loss: 0.000405 
Batch[11558] - loss: 0.000211 
Batch[11559] - loss: 0.000392 
Batch[11560] - loss: 0.000919 
Batch[11561] - loss: 0.000473 
Batch[11562] - loss: 0.000419 
Batch[11563] - loss: 0.000250 
Batch[11564] - loss: 0.000335 
Batch[11565] - loss: 0.000228 
Batch[11566] - loss: 0.000225 
Batch[11567] - loss: 0.000369 
Batch[11568] - loss: 0.000284 
Batch[11569] - loss: 0.000229 
Batch[11570] - loss: 0.000372 
Batch[11571] - loss: 0.000283 
Batch[11572] - loss: 0.000342 
Batch[11573] - loss: 0.000455 
Batch[11574] - loss: 0.000255 
Batch[11575] - loss: 0.000249 
Batch[11576] - loss: 0.000434 
Batch[11577] - loss: 0.000418 
Batch[11578] - loss: 0.000449 
Batch[11579] - loss: 0.000222 
Batch[11580] - loss: 0.000240 
Batch[11581] - loss: 0.000169 
Batch[11582] - loss: 0.000291 
Batch[11583] - loss: 0.000205 
Batch[11584] - loss: 0.000343 
Batch[11585] - loss: 0.000271 
Batch[11586] - loss: 0.000325 
Batch[11587] - loss: 0.000385 
Batch[11588] - loss: 0.000370 
Batch[11589] - loss: 0.000169 
Batch[11590] - loss: 0.000248 
Batch[11591] - loss: 0.000167 
Batch[11592] - loss: 0.000104 
Batch[11593] - loss: 0.000233 
Batch[11594] - loss: 0.000281 
Batch[11595] - loss: 0.000187 
Batch[11596] - loss: 0.000235 
Batch[11597] - loss: 0.000649 
Batch[11598] - loss: 0.000402 
Batch[11599] - loss: 0.000197 
Batch[11600] - loss: 0.000268 

Evaluation - loss: 0.000071 pearson: 0.5524 

early stop by 1500 steps.
Batch[11601] - loss: 0.000474 
Batch[11602] - loss: 0.000392 
Batch[11603] - loss: 0.000237 
Batch[11604] - loss: 0.000312 
Batch[11605] - loss: 0.000257 
Batch[11606] - loss: 0.000421 
Batch[11607] - loss: 0.000346 
Batch[11608] - loss: 0.000225 
Batch[11609] - loss: 0.000214 
Batch[11610] - loss: 0.000290 
Batch[11611] - loss: 0.000241 
Batch[11612] - loss: 0.000333 
Batch[11613] - loss: 0.000501 
Batch[11614] - loss: 0.000297 
Batch[11615] - loss: 0.000278 
Batch[11616] - loss: 0.000598 
Batch[11617] - loss: 0.000274 
Batch[11618] - loss: 0.000330 
Batch[11619] - loss: 0.000231 
Batch[11620] - loss: 0.000279 
Batch[11621] - loss: 0.000219 
Batch[11622] - loss: 0.000311 
Batch[11623] - loss: 0.000323 
Batch[11624] - loss: 0.000252 
Batch[11625] - loss: 0.000317 
Batch[11626] - loss: 0.000485 
Batch[11627] - loss: 0.000417 
Batch[11628] - loss: 0.000241 
Batch[11629] - loss: 0.000416 
Batch[11630] - loss: 0.000395 
Batch[11631] - loss: 0.000294 
Batch[11632] - loss: 0.000358 
Batch[11633] - loss: 0.000268 
Batch[11634] - loss: 0.000257 
Batch[11635] - loss: 0.000370 
Batch[11636] - loss: 0.000220 
Batch[11637] - loss: 0.000670 
Batch[11638] - loss: 0.000231 
Batch[11639] - loss: 0.000367 
Batch[11640] - loss: 0.000223 
Batch[11641] - loss: 0.000224 
Batch[11642] - loss: 0.000149 
Batch[11643] - loss: 0.000517 
Batch[11644] - loss: 0.000240 
Batch[11645] - loss: 0.000435 
Batch[11646] - loss: 0.000222 
Batch[11647] - loss: 0.000355 
Batch[11648] - loss: 0.000446 
Batch[11649] - loss: 0.000243 
Batch[11650] - loss: 0.000312 
Batch[11651] - loss: 0.000211 
Batch[11652] - loss: 0.000218 
Batch[11653] - loss: 0.000261 
Batch[11654] - loss: 0.000367 
Batch[11655] - loss: 0.000452 
Batch[11656] - loss: 0.000289 
Batch[11657] - loss: 0.000292 
Batch[11658] - loss: 0.000139 
Batch[11659] - loss: 0.000200 
Batch[11660] - loss: 0.000233 
Batch[11661] - loss: 0.000319 
Batch[11662] - loss: 0.000498 
Batch[11663] - loss: 0.000269 
Batch[11664] - loss: 0.000478 
Batch[11665] - loss: 0.000235 
Batch[11666] - loss: 0.000314 
Batch[11667] - loss: 0.000264 
Batch[11668] - loss: 0.000203 
Batch[11669] - loss: 0.000366 
Batch[11670] - loss: 0.000193 
Batch[11671] - loss: 0.000211 
Batch[11672] - loss: 0.000176 
Batch[11673] - loss: 0.000336 
Batch[11674] - loss: 0.000237 
Batch[11675] - loss: 0.000258 
Batch[11676] - loss: 0.000346 
Batch[11677] - loss: 0.000268 
Batch[11678] - loss: 0.000511 
Batch[11679] - loss: 0.000215 
Batch[11680] - loss: 0.000465 
Batch[11681] - loss: 0.000545 
Batch[11682] - loss: 0.000306 
Batch[11683] - loss: 0.000292 
Batch[11684] - loss: 0.000242 
Batch[11685] - loss: 0.000203 
Batch[11686] - loss: 0.000306 
Batch[11687] - loss: 0.000434 
Batch[11688] - loss: 0.000405 
Batch[11689] - loss: 0.000410 
Batch[11690] - loss: 0.000296 
Batch[11691] - loss: 0.000375 
Batch[11692] - loss: 0.000341 
Batch[11693] - loss: 0.000450 
Batch[11694] - loss: 0.000240 
Batch[11695] - loss: 0.000321 
Batch[11696] - loss: 0.000459 
Batch[11697] - loss: 0.000340 
Batch[11698] - loss: 0.000793 
Batch[11699] - loss: 0.000391 
Batch[11700] - loss: 0.000457 

Evaluation - loss: 0.000071 pearson: 0.5523 

early stop by 1500 steps.
Batch[11701] - loss: 0.000248 
Batch[11702] - loss: 0.000311 
Batch[11703] - loss: 0.000370 
Batch[11704] - loss: 0.000318 
Batch[11705] - loss: 0.000281 
Batch[11706] - loss: 0.000252 
Batch[11707] - loss: 0.000299 
Batch[11708] - loss: 0.000244 
Batch[11709] - loss: 0.000259 
Batch[11710] - loss: 0.000435 
Batch[11711] - loss: 0.000660 
Batch[11712] - loss: 0.000356 
Batch[11713] - loss: 0.000241 
Batch[11714] - loss: 0.000290 
Batch[11715] - loss: 0.000221 
Batch[11716] - loss: 0.000349 
Batch[11717] - loss: 0.000286 
Batch[11718] - loss: 0.000390 
Batch[11719] - loss: 0.000162 
Batch[11720] - loss: 0.000344 
Batch[11721] - loss: 0.000315 
Batch[11722] - loss: 0.000341 
Batch[11723] - loss: 0.000349 
Batch[11724] - loss: 0.000368 
Batch[11725] - loss: 0.000222 
Batch[11726] - loss: 0.000254 
Batch[11727] - loss: 0.000258 
Batch[11728] - loss: 0.000317 
Batch[11729] - loss: 0.000342 
Batch[11730] - loss: 0.000384 
Batch[11731] - loss: 0.000117 
Batch[11732] - loss: 0.000232 
Batch[11733] - loss: 0.000273 
Batch[11734] - loss: 0.000292 
Batch[11735] - loss: 0.000259 
Batch[11736] - loss: 0.000228 
Batch[11737] - loss: 0.000209 
Batch[11738] - loss: 0.000315 
Batch[11739] - loss: 0.000366 
Batch[11740] - loss: 0.000229 
Batch[11741] - loss: 0.000619 
Batch[11742] - loss: 0.000278 
Batch[11743] - loss: 0.000332 
Batch[11744] - loss: 0.000455 
Batch[11745] - loss: 0.000381 
Batch[11746] - loss: 0.000293 
Batch[11747] - loss: 0.000807 
Batch[11748] - loss: 0.000337 
Batch[11749] - loss: 0.000479 
Batch[11750] - loss: 0.000320 
Batch[11751] - loss: 0.000273 
Batch[11752] - loss: 0.000443 
Batch[11753] - loss: 0.000480 
Batch[11754] - loss: 0.000427 
Batch[11755] - loss: 0.000297 
Batch[11756] - loss: 0.000169 
Batch[11757] - loss: 0.000381 
Batch[11758] - loss: 0.000529 
Batch[11759] - loss: 0.000423 
Batch[11760] - loss: 0.000318 
Batch[11761] - loss: 0.000370 
Batch[11762] - loss: 0.000332 
Batch[11763] - loss: 0.000267 
Batch[11764] - loss: 0.000393 
Batch[11765] - loss: 0.000537 
Batch[11766] - loss: 0.000171 
Batch[11767] - loss: 0.000380 
Batch[11768] - loss: 0.000334 
Batch[11769] - loss: 0.000591 
Batch[11770] - loss: 0.000246 
Batch[11771] - loss: 0.000290 
Batch[11772] - loss: 0.000428 
Batch[11773] - loss: 0.000363 
Batch[11774] - loss: 0.000157 
Batch[11775] - loss: 0.000175 
Batch[11776] - loss: 0.000359 
Batch[11777] - loss: 0.000242 
Batch[11778] - loss: 0.000223 
Batch[11779] - loss: 0.000307 
Batch[11780] - loss: 0.000330 
Batch[11781] - loss: 0.000436 
Batch[11782] - loss: 0.000595 
Batch[11783] - loss: 0.000468 
Batch[11784] - loss: 0.000281 
Batch[11785] - loss: 0.000407 
Batch[11786] - loss: 0.000552 
Batch[11787] - loss: 0.000274 
Batch[11788] - loss: 0.000163 
Batch[11789] - loss: 0.000235 
Batch[11790] - loss: 0.000205 
Batch[11791] - loss: 0.000239 
Batch[11792] - loss: 0.000272 
Batch[11793] - loss: 0.000259 
Batch[11794] - loss: 0.000279 
Batch[11795] - loss: 0.000296 
Batch[11796] - loss: 0.000251 
Batch[11797] - loss: 0.000219 
Batch[11798] - loss: 0.000263 
Batch[11799] - loss: 0.000375 
Batch[11800] - loss: 0.000297 

Evaluation - loss: 0.000070 pearson: 0.5555 

early stop by 1500 steps.
Batch[11801] - loss: 0.000218 
Batch[11802] - loss: 0.000456 
Batch[11803] - loss: 0.000209 
Batch[11804] - loss: 0.000317 
Batch[11805] - loss: 0.000190 
Batch[11806] - loss: 0.000459 
Batch[11807] - loss: 0.000185 
Batch[11808] - loss: 0.000560 
Batch[11809] - loss: 0.000390 
Batch[11810] - loss: 0.000293 
Batch[11811] - loss: 0.000418 
Batch[11812] - loss: 0.000352 
Batch[11813] - loss: 0.000328 
Batch[11814] - loss: 0.000282 
Batch[11815] - loss: 0.000408 
Batch[11816] - loss: 0.000225 
Batch[11817] - loss: 0.000310 
Batch[11818] - loss: 0.000225 
Batch[11819] - loss: 0.000312 
Batch[11820] - loss: 0.000193 
Batch[11821] - loss: 0.000290 
Batch[11822] - loss: 0.000238 
Batch[11823] - loss: 0.000362 
Batch[11824] - loss: 0.000423 
Batch[11825] - loss: 0.000304 
Batch[11826] - loss: 0.000203 
Batch[11827] - loss: 0.000358 
Batch[11828] - loss: 0.000242 
Batch[11829] - loss: 0.001260 
Batch[11830] - loss: 0.000418 
Batch[11831] - loss: 0.000421 
Batch[11832] - loss: 0.000304 
Batch[11833] - loss: 0.000395 
Batch[11834] - loss: 0.000402 
Batch[11835] - loss: 0.000363 
Batch[11836] - loss: 0.000200 
Batch[11837] - loss: 0.000330 
Batch[11838] - loss: 0.000289 
Batch[11839] - loss: 0.000250 
Batch[11840] - loss: 0.000592 
Batch[11841] - loss: 0.000370 
Batch[11842] - loss: 0.000422 
Batch[11843] - loss: 0.000272 
Batch[11844] - loss: 0.000297 
Batch[11845] - loss: 0.000346 
Batch[11846] - loss: 0.000338 
Batch[11847] - loss: 0.000224 
Batch[11848] - loss: 0.000705 
Batch[11849] - loss: 0.000313 
Batch[11850] - loss: 0.000351 
Batch[11851] - loss: 0.000259 
Batch[11852] - loss: 0.000365 
Batch[11853] - loss: 0.000237 
Batch[11854] - loss: 0.000259 
Batch[11855] - loss: 0.000304 
Batch[11856] - loss: 0.000324 
Batch[11857] - loss: 0.000362 
Batch[11858] - loss: 0.000426 
Batch[11859] - loss: 0.000682 
Batch[11860] - loss: 0.000471 
Batch[11861] - loss: 0.000527 
Batch[11862] - loss: 0.000320 
Batch[11863] - loss: 0.000453 
Batch[11864] - loss: 0.000315 
Batch[11865] - loss: 0.000397 
Batch[11866] - loss: 0.000195 
Batch[11867] - loss: 0.000371 
Batch[11868] - loss: 0.000197 
Batch[11869] - loss: 0.000346 
Batch[11870] - loss: 0.000332 
Batch[11871] - loss: 0.000352 
Batch[11872] - loss: 0.000250 
Batch[11873] - loss: 0.000493 
Batch[11874] - loss: 0.000452 
Batch[11875] - loss: 0.000212 
Batch[11876] - loss: 0.000539 
Batch[11877] - loss: 0.000277 
Batch[11878] - loss: 0.000465 
Batch[11879] - loss: 0.000254 
Batch[11880] - loss: 0.000290 
Batch[11881] - loss: 0.001198 
Batch[11882] - loss: 0.000369 
Batch[11883] - loss: 0.000510 
Batch[11884] - loss: 0.000197 
Batch[11885] - loss: 0.000314 
Batch[11886] - loss: 0.000487 
Batch[11887] - loss: 0.000187 
Batch[11888] - loss: 0.000262 
Batch[11889] - loss: 0.000417 
Batch[11890] - loss: 0.000459 
Batch[11891] - loss: 0.000445 
Batch[11892] - loss: 0.000387 
Batch[11893] - loss: 0.000334 
Batch[11894] - loss: 0.000378 
Batch[11895] - loss: 0.000270 
Batch[11896] - loss: 0.000480 
Batch[11897] - loss: 0.000217 
Batch[11898] - loss: 0.000433 
Batch[11899] - loss: 0.000200 
Batch[11900] - loss: 0.000265 

Evaluation - loss: 0.000071 pearson: 0.5523 

early stop by 1500 steps.
Batch[11901] - loss: 0.000249 
Batch[11902] - loss: 0.000323 
Batch[11903] - loss: 0.000413 
Batch[11904] - loss: 0.000584 
Batch[11905] - loss: 0.000757 
Batch[11906] - loss: 0.000693 
Batch[11907] - loss: 0.000223 
Batch[11908] - loss: 0.000233 
Batch[11909] - loss: 0.000376 
Batch[11910] - loss: 0.000230 
Batch[11911] - loss: 0.000193 
Batch[11912] - loss: 0.000319 
Batch[11913] - loss: 0.000184 
Batch[11914] - loss: 0.000268 
Batch[11915] - loss: 0.000324 
Batch[11916] - loss: 0.000329 
Batch[11917] - loss: 0.000256 
Batch[11918] - loss: 0.000378 
Batch[11919] - loss: 0.000479 
Batch[11920] - loss: 0.000431 
Batch[11921] - loss: 0.000515 
Batch[11922] - loss: 0.000207 
Batch[11923] - loss: 0.000537 
Batch[11924] - loss: 0.000496 
Batch[11925] - loss: 0.000265 
Batch[11926] - loss: 0.000477 
Batch[11927] - loss: 0.000355 
Batch[11928] - loss: 0.000340 
Batch[11929] - loss: 0.000395 
Batch[11930] - loss: 0.000302 
Batch[11931] - loss: 0.000203 
Batch[11932] - loss: 0.000332 
Batch[11933] - loss: 0.000222 
Batch[11934] - loss: 0.000591 
Batch[11935] - loss: 0.000233 
Batch[11936] - loss: 0.000263 
Batch[11937] - loss: 0.000209 
Batch[11938] - loss: 0.000265 
Batch[11939] - loss: 0.000337 
Batch[11940] - loss: 0.000291 
Batch[11941] - loss: 0.000396 
Batch[11942] - loss: 0.000241 
Batch[11943] - loss: 0.000207 
Batch[11944] - loss: 0.000211 
Batch[11945] - loss: 0.000323 
Batch[11946] - loss: 0.000445 
Batch[11947] - loss: 0.000160 
Batch[11948] - loss: 0.000413 
Batch[11949] - loss: 0.000308 
Batch[11950] - loss: 0.000267 
Batch[11951] - loss: 0.000309 
Batch[11952] - loss: 0.000286 
Batch[11953] - loss: 0.000589 
Batch[11954] - loss: 0.000350 
Batch[11955] - loss: 0.000541 
Batch[11956] - loss: 0.000473 
Batch[11957] - loss: 0.000327 
Batch[11958] - loss: 0.000440 
Batch[11959] - loss: 0.000232 
Batch[11960] - loss: 0.000518 
Batch[11961] - loss: 0.000319 
Batch[11962] - loss: 0.000441 
Batch[11963] - loss: 0.000356 
Batch[11964] - loss: 0.000592 
Batch[11965] - loss: 0.000407 
Batch[11966] - loss: 0.000268 
Batch[11967] - loss: 0.000323 
Batch[11968] - loss: 0.000259 
Batch[11969] - loss: 0.000371 
Batch[11970] - loss: 0.000338 
Batch[11971] - loss: 0.000371 
Batch[11972] - loss: 0.000431 
Batch[11973] - loss: 0.000472 
Batch[11974] - loss: 0.000397 
Batch[11975] - loss: 0.000228 
Batch[11976] - loss: 0.000295 
Batch[11977] - loss: 0.000250 
Batch[11978] - loss: 0.000423 
Batch[11979] - loss: 0.000426 
Batch[11980] - loss: 0.000463 
Batch[11981] - loss: 0.000495 
Batch[11982] - loss: 0.000301 
Batch[11983] - loss: 0.000317 
Batch[11984] - loss: 0.000530 
Batch[11985] - loss: 0.000275 
Batch[11986] - loss: 0.000190 
Batch[11987] - loss: 0.000354 
Batch[11988] - loss: 0.000279 
Batch[11989] - loss: 0.000275 
Batch[11990] - loss: 0.000418 
Batch[11991] - loss: 0.000267 
Batch[11992] - loss: 0.000202 
Batch[11993] - loss: 0.000172 
Batch[11994] - loss: 0.000198 
Batch[11995] - loss: 0.000449 
Batch[11996] - loss: 0.000433 
Batch[11997] - loss: 0.000240 
Batch[11998] - loss: 0.000241 
Batch[11999] - loss: 0.000247 
Batch[12000] - loss: 0.000561 

Evaluation - loss: 0.000071 pearson: 0.5541 

early stop by 1500 steps.
Batch[12001] - loss: 0.000199 
Batch[12002] - loss: 0.000190 
Batch[12003] - loss: 0.000295 
Batch[12004] - loss: 0.000255 
Batch[12005] - loss: 0.000265 
Batch[12006] - loss: 0.000111 
Batch[12007] - loss: 0.000147 
Batch[12008] - loss: 0.000164 
Batch[12009] - loss: 0.000318 
Batch[12010] - loss: 0.000272 
Batch[12011] - loss: 0.000260 
Batch[12012] - loss: 0.000210 
Batch[12013] - loss: 0.000269 
Batch[12014] - loss: 0.000207 
Batch[12015] - loss: 0.000310 
Batch[12016] - loss: 0.000367 
Batch[12017] - loss: 0.000339 
Batch[12018] - loss: 0.000413 
Batch[12019] - loss: 0.000540 
Batch[12020] - loss: 0.000465 
Batch[12021] - loss: 0.000398 
Batch[12022] - loss: 0.000345 
Batch[12023] - loss: 0.000238 
Batch[12024] - loss: 0.000275 
Batch[12025] - loss: 0.000614 
Batch[12026] - loss: 0.000337 
Batch[12027] - loss: 0.000603 
Batch[12028] - loss: 0.000293 
Batch[12029] - loss: 0.000433 
Batch[12030] - loss: 0.000294 
Batch[12031] - loss: 0.000534 
Batch[12032] - loss: 0.000311 
Batch[12033] - loss: 0.000316 
Batch[12034] - loss: 0.000202 
Batch[12035] - loss: 0.000391 
Batch[12036] - loss: 0.000253 
Batch[12037] - loss: 0.000367 
Batch[12038] - loss: 0.000553 
Batch[12039] - loss: 0.000458 
Batch[12040] - loss: 0.000260 
Batch[12041] - loss: 0.000440 
Batch[12042] - loss: 0.000253 
Batch[12043] - loss: 0.000287 
Batch[12044] - loss: 0.000362 
Batch[12045] - loss: 0.000418 
Batch[12046] - loss: 0.000353 
Batch[12047] - loss: 0.000305 
Batch[12048] - loss: 0.000517 
Batch[12049] - loss: 0.000306 
Batch[12050] - loss: 0.000155 
Batch[12051] - loss: 0.000450 
Batch[12052] - loss: 0.000279 
Batch[12053] - loss: 0.000208 
Batch[12054] - loss: 0.000285 
Batch[12055] - loss: 0.000225 
Batch[12056] - loss: 0.000294 
Batch[12057] - loss: 0.000509 
Batch[12058] - loss: 0.000213 
Batch[12059] - loss: 0.000252 
Batch[12060] - loss: 0.000260 
Batch[12061] - loss: 0.000290 
Batch[12062] - loss: 0.000212 
Batch[12063] - loss: 0.000221 
Batch[12064] - loss: 0.000160 
Batch[12065] - loss: 0.000221 
Batch[12066] - loss: 0.000381 
Batch[12067] - loss: 0.000426 
Batch[12068] - loss: 0.000269 
Batch[12069] - loss: 0.000301 
Batch[12070] - loss: 0.000194 
Batch[12071] - loss: 0.000179 
Batch[12072] - loss: 0.000175 
Batch[12073] - loss: 0.000222 
Batch[12074] - loss: 0.000361 
Batch[12075] - loss: 0.000444 
Batch[12076] - loss: 0.000167 
Batch[12077] - loss: 0.000314 
Batch[12078] - loss: 0.000259 
Batch[12079] - loss: 0.000288 
Batch[12080] - loss: 0.000167 
Batch[12081] - loss: 0.000372 
Batch[12082] - loss: 0.000343 
Batch[12083] - loss: 0.000269 
Batch[12084] - loss: 0.000314 
Batch[12085] - loss: 0.000182 
Batch[12086] - loss: 0.000377 
Batch[12087] - loss: 0.000456 
Batch[12088] - loss: 0.000196 
Batch[12089] - loss: 0.000255 
Batch[12090] - loss: 0.000308 
Batch[12091] - loss: 0.000607 
Batch[12092] - loss: 0.000358 
Batch[12093] - loss: 0.000285 
Batch[12094] - loss: 0.000361 
Batch[12095] - loss: 0.000260 
Batch[12096] - loss: 0.000422 
Batch[12097] - loss: 0.000300 
Batch[12098] - loss: 0.000506 
Batch[12099] - loss: 0.000259 
Batch[12100] - loss: 0.000411 

Evaluation - loss: 0.000071 pearson: 0.5536 

early stop by 1500 steps.
Batch[12101] - loss: 0.000226 
Batch[12102] - loss: 0.000454 
Batch[12103] - loss: 0.000263 
Batch[12104] - loss: 0.000498 
Batch[12105] - loss: 0.000396 
Batch[12106] - loss: 0.000309 
Batch[12107] - loss: 0.000380 
Batch[12108] - loss: 0.000343 
Batch[12109] - loss: 0.000311 
Batch[12110] - loss: 0.000501 
Batch[12111] - loss: 0.000412 
Batch[12112] - loss: 0.000301 
Batch[12113] - loss: 0.000526 
Batch[12114] - loss: 0.000407 
Batch[12115] - loss: 0.000418 
Batch[12116] - loss: 0.000152 
Batch[12117] - loss: 0.000271 
Batch[12118] - loss: 0.000499 
Batch[12119] - loss: 0.000329 
Batch[12120] - loss: 0.000372 
Batch[12121] - loss: 0.000248 
Batch[12122] - loss: 0.000185 
Batch[12123] - loss: 0.000384 
Batch[12124] - loss: 0.000184 
Batch[12125] - loss: 0.000427 
Batch[12126] - loss: 0.000618 
Batch[12127] - loss: 0.000195 
Batch[12128] - loss: 0.000202 
Batch[12129] - loss: 0.000292 
Batch[12130] - loss: 0.000253 
Batch[12131] - loss: 0.000255 
Batch[12132] - loss: 0.000393 
Batch[12133] - loss: 0.000137 
Batch[12134] - loss: 0.000242 
Batch[12135] - loss: 0.000197 
Batch[12136] - loss: 0.000197 
Batch[12137] - loss: 0.000376 
Batch[12138] - loss: 0.000337 
Batch[12139] - loss: 0.000177 
Batch[12140] - loss: 0.000421 
Batch[12141] - loss: 0.000263 
Batch[12142] - loss: 0.000211 
Batch[12143] - loss: 0.000227 
Batch[12144] - loss: 0.000389 
Batch[12145] - loss: 0.000176 
Batch[12146] - loss: 0.000171 
Batch[12147] - loss: 0.000191 
Batch[12148] - loss: 0.000413 
Batch[12149] - loss: 0.000271 
Batch[12150] - loss: 0.000146 
Batch[12151] - loss: 0.000271 
Batch[12152] - loss: 0.000589 
Batch[12153] - loss: 0.000516 
Batch[12154] - loss: 0.000413 
Batch[12155] - loss: 0.000423 
Batch[12156] - loss: 0.000365 
Batch[12157] - loss: 0.000567 
Batch[12158] - loss: 0.000222 
Batch[12159] - loss: 0.000207 
Batch[12160] - loss: 0.000509 
Batch[12161] - loss: 0.000386 
Batch[12162] - loss: 0.000220 
Batch[12163] - loss: 0.000283 
Batch[12164] - loss: 0.000401 
Batch[12165] - loss: 0.000338 
Batch[12166] - loss: 0.000374 
Batch[12167] - loss: 0.000827 
Batch[12168] - loss: 0.000295 
Batch[12169] - loss: 0.000190 
Batch[12170] - loss: 0.000241 
Batch[12171] - loss: 0.000286 
Batch[12172] - loss: 0.000291 
Batch[12173] - loss: 0.000266 
Batch[12174] - loss: 0.000274 
Batch[12175] - loss: 0.000237 
Batch[12176] - loss: 0.000674 
Batch[12177] - loss: 0.000366 
Batch[12178] - loss: 0.000515 
Batch[12179] - loss: 0.000137 
Batch[12180] - loss: 0.000577 
Batch[12181] - loss: 0.000320 
Batch[12182] - loss: 0.000443 
Batch[12183] - loss: 0.000314 
Batch[12184] - loss: 0.000249 
Batch[12185] - loss: 0.000393 
Batch[12186] - loss: 0.000279 
Batch[12187] - loss: 0.000264 
Batch[12188] - loss: 0.000372 
Batch[12189] - loss: 0.000387 
Batch[12190] - loss: 0.000262 
Batch[12191] - loss: 0.000321 
Batch[12192] - loss: 0.000268 
Batch[12193] - loss: 0.000403 
Batch[12194] - loss: 0.000349 
Batch[12195] - loss: 0.000341 
Batch[12196] - loss: 0.000340 
Batch[12197] - loss: 0.000452 
Batch[12198] - loss: 0.000255 
Batch[12199] - loss: 0.000264 
Batch[12200] - loss: 0.000313 

Evaluation - loss: 0.000071 pearson: 0.5515 

early stop by 1500 steps.
Batch[12201] - loss: 0.000219 
Batch[12202] - loss: 0.000211 
Batch[12203] - loss: 0.000192 
Batch[12204] - loss: 0.000216 
Batch[12205] - loss: 0.000414 
Batch[12206] - loss: 0.000308 
Batch[12207] - loss: 0.000446 
Batch[12208] - loss: 0.000464 
Batch[12209] - loss: 0.000347 
Batch[12210] - loss: 0.000349 
Batch[12211] - loss: 0.000197 
Batch[12212] - loss: 0.000405 
Batch[12213] - loss: 0.000396 
Batch[12214] - loss: 0.000181 
Batch[12215] - loss: 0.000392 
Batch[12216] - loss: 0.000241 
Batch[12217] - loss: 0.000257 
Batch[12218] - loss: 0.000327 
Batch[12219] - loss: 0.000360 
Batch[12220] - loss: 0.000254 
Batch[12221] - loss: 0.000313 
Batch[12222] - loss: 0.000261 
Batch[12223] - loss: 0.000373 
Batch[12224] - loss: 0.000268 
Batch[12225] - loss: 0.000257 
Batch[12226] - loss: 0.000194 
Batch[12227] - loss: 0.000482 
Batch[12228] - loss: 0.000275 
Batch[12229] - loss: 0.000186 
Batch[12230] - loss: 0.000971 
Batch[12231] - loss: 0.000433 
Batch[12232] - loss: 0.000439 
Batch[12233] - loss: 0.000377 
Batch[12234] - loss: 0.000203 
Batch[12235] - loss: 0.000357 
Batch[12236] - loss: 0.000241 
Batch[12237] - loss: 0.000254 
Batch[12238] - loss: 0.000217 
Batch[12239] - loss: 0.000301 
Batch[12240] - loss: 0.000212 
Batch[12241] - loss: 0.000267 
Batch[12242] - loss: 0.000372 
Batch[12243] - loss: 0.000477 
Batch[12244] - loss: 0.000326 
Batch[12245] - loss: 0.000292 
Batch[12246] - loss: 0.000538 
Batch[12247] - loss: 0.000466 
Batch[12248] - loss: 0.000487 
Batch[12249] - loss: 0.000371 
Batch[12250] - loss: 0.000285 
Batch[12251] - loss: 0.000315 
Batch[12252] - loss: 0.000360 
Batch[12253] - loss: 0.000342 
Batch[12254] - loss: 0.000270 
Batch[12255] - loss: 0.000231 
Batch[12256] - loss: 0.000286 
Batch[12257] - loss: 0.000238 
Batch[12258] - loss: 0.000426 
Batch[12259] - loss: 0.000246 
Batch[12260] - loss: 0.000251 
Batch[12261] - loss: 0.000363 
Batch[12262] - loss: 0.000334 
Batch[12263] - loss: 0.000514 
Batch[12264] - loss: 0.000261 
Batch[12265] - loss: 0.000299 
Batch[12266] - loss: 0.000347 
Batch[12267] - loss: 0.000199 
Batch[12268] - loss: 0.000196 
Batch[12269] - loss: 0.000184 
Batch[12270] - loss: 0.000449 
Batch[12271] - loss: 0.000405 
Batch[12272] - loss: 0.000548 
Batch[12273] - loss: 0.000264 
Batch[12274] - loss: 0.000267 
Batch[12275] - loss: 0.000278 
Batch[12276] - loss: 0.000253 
Batch[12277] - loss: 0.000318 
Batch[12278] - loss: 0.000413 
Batch[12279] - loss: 0.000432 
Batch[12280] - loss: 0.000291 
Batch[12281] - loss: 0.000308 
Batch[12282] - loss: 0.000351 
Batch[12283] - loss: 0.000304 
Batch[12284] - loss: 0.000151 
Batch[12285] - loss: 0.000644 
Batch[12286] - loss: 0.000187 
Batch[12287] - loss: 0.000426 
Batch[12288] - loss: 0.000478 
Batch[12289] - loss: 0.000276 
Batch[12290] - loss: 0.000186 
Batch[12291] - loss: 0.000438 
Batch[12292] - loss: 0.000301 
Batch[12293] - loss: 0.000382 
Batch[12294] - loss: 0.000226 
Batch[12295] - loss: 0.000359 
Batch[12296] - loss: 0.000270 
Batch[12297] - loss: 0.000331 
Batch[12298] - loss: 0.000353 
Batch[12299] - loss: 0.000433 
Batch[12300] - loss: 0.000237 

Evaluation - loss: 0.000071 pearson: 0.5545 

early stop by 1500 steps.
Batch[12301] - loss: 0.000636 
Batch[12302] - loss: 0.000284 
Batch[12303] - loss: 0.000382 
Batch[12304] - loss: 0.000165 
Batch[12305] - loss: 0.000221 
Batch[12306] - loss: 0.000240 
Batch[12307] - loss: 0.000218 
Batch[12308] - loss: 0.000396 
Batch[12309] - loss: 0.000414 
Batch[12310] - loss: 0.000480 
Batch[12311] - loss: 0.000332 
Batch[12312] - loss: 0.000277 
Batch[12313] - loss: 0.000648 
Batch[12314] - loss: 0.000377 
Batch[12315] - loss: 0.000731 
Batch[12316] - loss: 0.000315 
Batch[12317] - loss: 0.000113 
Batch[12318] - loss: 0.000276 
Batch[12319] - loss: 0.000295 
Batch[12320] - loss: 0.000387 
Batch[12321] - loss: 0.000242 
Batch[12322] - loss: 0.000267 
Batch[12323] - loss: 0.000334 
Batch[12324] - loss: 0.000367 
Batch[12325] - loss: 0.000390 
Batch[12326] - loss: 0.000434 
Batch[12327] - loss: 0.000238 
Batch[12328] - loss: 0.000366 
Batch[12329] - loss: 0.000293 
Batch[12330] - loss: 0.000614 
Batch[12331] - loss: 0.000329 
Batch[12332] - loss: 0.000506 
Batch[12333] - loss: 0.000323 
Batch[12334] - loss: 0.000109 
Batch[12335] - loss: 0.000566 
Batch[12336] - loss: 0.000181 
Batch[12337] - loss: 0.000337 
Batch[12338] - loss: 0.000181 
Batch[12339] - loss: 0.000250 
Batch[12340] - loss: 0.000360 
Batch[12341] - loss: 0.000356 
Batch[12342] - loss: 0.000181 
Batch[12343] - loss: 0.000363 
Batch[12344] - loss: 0.000254 
Batch[12345] - loss: 0.000248 
Batch[12346] - loss: 0.000283 
Batch[12347] - loss: 0.000378 
Batch[12348] - loss: 0.000382 
Batch[12349] - loss: 0.000281 
Batch[12350] - loss: 0.000244 
Batch[12351] - loss: 0.000374 
Batch[12352] - loss: 0.000284 
Batch[12353] - loss: 0.000337 
Batch[12354] - loss: 0.000185 
Batch[12355] - loss: 0.000272 
Batch[12356] - loss: 0.000227 
Batch[12357] - loss: 0.000214 
Batch[12358] - loss: 0.000277 
Batch[12359] - loss: 0.000240 
Batch[12360] - loss: 0.000357 
Batch[12361] - loss: 0.000374 
Batch[12362] - loss: 0.000221 
Batch[12363] - loss: 0.000321 
Batch[12364] - loss: 0.000289 
Batch[12365] - loss: 0.000383 
Batch[12366] - loss: 0.000440 
Batch[12367] - loss: 0.000346 
Batch[12368] - loss: 0.000327 
Batch[12369] - loss: 0.000636 
Batch[12370] - loss: 0.000723 
Batch[12371] - loss: 0.000234 
Batch[12372] - loss: 0.000448 
Batch[12373] - loss: 0.000389 
Batch[12374] - loss: 0.000300 
Batch[12375] - loss: 0.000244 
Batch[12376] - loss: 0.000341 
Batch[12377] - loss: 0.001058 
Batch[12378] - loss: 0.000257 
Batch[12379] - loss: 0.000410 
Batch[12380] - loss: 0.000313 
Batch[12381] - loss: 0.000297 
Batch[12382] - loss: 0.000545 
Batch[12383] - loss: 0.000342 
Batch[12384] - loss: 0.000603 
Batch[12385] - loss: 0.000342 
Batch[12386] - loss: 0.000203 
Batch[12387] - loss: 0.000423 
Batch[12388] - loss: 0.000142 
Batch[12389] - loss: 0.000306 
Batch[12390] - loss: 0.000657 
Batch[12391] - loss: 0.000277 
Batch[12392] - loss: 0.000349 
Batch[12393] - loss: 0.000441 
Batch[12394] - loss: 0.000249 
Batch[12395] - loss: 0.000396 
Batch[12396] - loss: 0.000393 
Batch[12397] - loss: 0.000196 
Batch[12398] - loss: 0.000455 
Batch[12399] - loss: 0.000193 
Batch[12400] - loss: 0.000244 

Evaluation - loss: 0.000071 pearson: 0.5555 

early stop by 1500 steps.
Batch[12401] - loss: 0.000342 
Batch[12402] - loss: 0.000324 
Batch[12403] - loss: 0.000546 
Batch[12404] - loss: 0.000472 
Batch[12405] - loss: 0.000283 
Batch[12406] - loss: 0.000331 
Batch[12407] - loss: 0.000334 
Batch[12408] - loss: 0.000180 
Batch[12409] - loss: 0.000397 
Batch[12410] - loss: 0.000257 
Batch[12411] - loss: 0.000334 
Batch[12412] - loss: 0.000450 
Batch[12413] - loss: 0.000215 
Batch[12414] - loss: 0.000246 
Batch[12415] - loss: 0.000284 
Batch[12416] - loss: 0.000347 
Batch[12417] - loss: 0.000299 
Batch[12418] - loss: 0.000299 
Batch[12419] - loss: 0.000239 
Batch[12420] - loss: 0.000256 
Batch[12421] - loss: 0.000491 
Batch[12422] - loss: 0.000470 
Batch[12423] - loss: 0.000357 
Batch[12424] - loss: 0.000250 
Batch[12425] - loss: 0.000216 
Batch[12426] - loss: 0.000249 
Batch[12427] - loss: 0.000481 
Batch[12428] - loss: 0.000288 
Batch[12429] - loss: 0.000541 
Batch[12430] - loss: 0.000270 
Batch[12431] - loss: 0.000315 
Batch[12432] - loss: 0.000214 
Batch[12433] - loss: 0.000411 
Batch[12434] - loss: 0.000276 
Batch[12435] - loss: 0.000513 
Batch[12436] - loss: 0.000570 
Batch[12437] - loss: 0.000357 
Batch[12438] - loss: 0.000259 
Batch[12439] - loss: 0.000368 
Batch[12440] - loss: 0.000487 
Batch[12441] - loss: 0.000270 
Batch[12442] - loss: 0.000220 
Batch[12443] - loss: 0.000344 
Batch[12444] - loss: 0.000211 
Batch[12445] - loss: 0.000354 
Batch[12446] - loss: 0.000512 
Batch[12447] - loss: 0.000202 
Batch[12448] - loss: 0.000411 
Batch[12449] - loss: 0.000249 
Batch[12450] - loss: 0.000268 
Batch[12451] - loss: 0.000313 
Batch[12452] - loss: 0.000585 
Batch[12453] - loss: 0.000436 
Batch[12454] - loss: 0.000476 
Batch[12455] - loss: 0.000505 
Batch[12456] - loss: 0.000218 
Batch[12457] - loss: 0.000325 
Batch[12458] - loss: 0.000480 
Batch[12459] - loss: 0.000295 
Batch[12460] - loss: 0.000421 
Batch[12461] - loss: 0.000424 
Batch[12462] - loss: 0.000438 
Batch[12463] - loss: 0.000394 
Batch[12464] - loss: 0.000460 
Batch[12465] - loss: 0.000356 
Batch[12466] - loss: 0.000448 
Batch[12467] - loss: 0.000270 
Batch[12468] - loss: 0.000595 
Batch[12469] - loss: 0.000381 
Batch[12470] - loss: 0.000446 
Batch[12471] - loss: 0.000481 
Batch[12472] - loss: 0.000462 
Batch[12473] - loss: 0.000309 
Batch[12474] - loss: 0.000450 
Batch[12475] - loss: 0.000460 
Batch[12476] - loss: 0.000221 
Batch[12477] - loss: 0.000268 
Batch[12478] - loss: 0.000294 
Batch[12479] - loss: 0.000325 
Batch[12480] - loss: 0.000234 
Batch[12481] - loss: 0.000255 
Batch[12482] - loss: 0.000146 
Batch[12483] - loss: 0.000451 
Batch[12484] - loss: 0.000436 
Batch[12485] - loss: 0.000204 
Batch[12486] - loss: 0.000241 
Batch[12487] - loss: 0.000274 
Batch[12488] - loss: 0.000392 
Batch[12489] - loss: 0.000129 
Batch[12490] - loss: 0.000449 
Batch[12491] - loss: 0.000320 
Batch[12492] - loss: 0.000177 
Batch[12493] - loss: 0.000306 
Batch[12494] - loss: 0.000330 
Batch[12495] - loss: 0.000185 
Batch[12496] - loss: 0.000320 
Batch[12497] - loss: 0.000265 
Batch[12498] - loss: 0.000241 
Batch[12499] - loss: 0.000493 
Batch[12500] - loss: 0.000336 

Evaluation - loss: 0.000071 pearson: 0.5562 

early stop by 1500 steps.
Batch[12501] - loss: 0.000348 
Batch[12502] - loss: 0.000239 
Batch[12503] - loss: 0.000412 
Batch[12504] - loss: 0.000411 
Batch[12505] - loss: 0.000239 
Batch[12506] - loss: 0.000470 
Batch[12507] - loss: 0.000287 
Batch[12508] - loss: 0.000136 
Batch[12509] - loss: 0.000327 
Batch[12510] - loss: 0.000432 
Batch[12511] - loss: 0.000480 
Batch[12512] - loss: 0.000530 
Batch[12513] - loss: 0.000276 
Batch[12514] - loss: 0.000476 
Batch[12515] - loss: 0.000233 
Batch[12516] - loss: 0.000512 
Batch[12517] - loss: 0.000408 
Batch[12518] - loss: 0.000267 
Batch[12519] - loss: 0.000332 
Batch[12520] - loss: 0.000274 
Batch[12521] - loss: 0.000292 
Batch[12522] - loss: 0.000388 
Batch[12523] - loss: 0.000425 
Batch[12524] - loss: 0.000522 
Batch[12525] - loss: 0.000282 
Batch[12526] - loss: 0.000458 
Batch[12527] - loss: 0.000462 
Batch[12528] - loss: 0.000492 
Batch[12529] - loss: 0.000323 
Batch[12530] - loss: 0.000290 
Batch[12531] - loss: 0.000370 
Batch[12532] - loss: 0.000625 
Batch[12533] - loss: 0.000481 
Batch[12534] - loss: 0.000175 
Batch[12535] - loss: 0.000268 
Batch[12536] - loss: 0.000403 
Batch[12537] - loss: 0.000300 
Batch[12538] - loss: 0.000316 
Batch[12539] - loss: 0.000263 
Batch[12540] - loss: 0.000418 
Batch[12541] - loss: 0.000270 
Batch[12542] - loss: 0.000351 
Batch[12543] - loss: 0.000356 
Batch[12544] - loss: 0.000281 
Batch[12545] - loss: 0.000242 
Batch[12546] - loss: 0.000225 
Batch[12547] - loss: 0.000309 
Batch[12548] - loss: 0.000232 
Batch[12549] - loss: 0.000269 
Batch[12550] - loss: 0.000434 
Batch[12551] - loss: 0.000177 
Batch[12552] - loss: 0.000426 
Batch[12553] - loss: 0.000143 
Batch[12554] - loss: 0.000544 
Batch[12555] - loss: 0.000298 
Batch[12556] - loss: 0.000255 
Batch[12557] - loss: 0.000322 
Batch[12558] - loss: 0.000286 
Batch[12559] - loss: 0.000284 
Batch[12560] - loss: 0.000340 
Batch[12561] - loss: 0.000430 
Batch[12562] - loss: 0.000324 
Batch[12563] - loss: 0.000373 
Batch[12564] - loss: 0.000191 
Batch[12565] - loss: 0.000272 
Batch[12566] - loss: 0.000259 
Batch[12567] - loss: 0.000222 
Batch[12568] - loss: 0.000209 
Batch[12569] - loss: 0.000262 
Batch[12570] - loss: 0.000255 
Batch[12571] - loss: 0.000476 
Batch[12572] - loss: 0.000338 
Batch[12573] - loss: 0.000435 
Batch[12574] - loss: 0.000358 
Batch[12575] - loss: 0.000797 
Batch[12576] - loss: 0.000406 
Batch[12577] - loss: 0.000252 
Batch[12578] - loss: 0.000455 
Batch[12579] - loss: 0.000683 
Batch[12580] - loss: 0.000475 
Batch[12581] - loss: 0.000350 
Batch[12582] - loss: 0.000375 
Batch[12583] - loss: 0.000381 
Batch[12584] - loss: 0.000303 
Batch[12585] - loss: 0.000382 
Batch[12586] - loss: 0.000367 
Batch[12587] - loss: 0.000302 
Batch[12588] - loss: 0.000251 
Batch[12589] - loss: 0.000230 
Batch[12590] - loss: 0.000282 
Batch[12591] - loss: 0.000157 
Batch[12592] - loss: 0.000324 
Batch[12593] - loss: 0.000382 
Batch[12594] - loss: 0.000258 
Batch[12595] - loss: 0.000259 
Batch[12596] - loss: 0.000201 
Batch[12597] - loss: 0.000425 
Batch[12598] - loss: 0.000530 
Batch[12599] - loss: 0.000427 
Batch[12600] - loss: 0.000154 

Evaluation - loss: 0.000071 pearson: 0.5536 

early stop by 1500 steps.
Batch[12601] - loss: 0.000370 
Batch[12602] - loss: 0.000314 
Batch[12603] - loss: 0.000227 
Batch[12604] - loss: 0.000457 
Batch[12605] - loss: 0.000334 
Batch[12606] - loss: 0.000335 
Batch[12607] - loss: 0.000405 
Batch[12608] - loss: 0.000336 
Batch[12609] - loss: 0.000276 
Batch[12610] - loss: 0.000431 
Batch[12611] - loss: 0.000276 
Batch[12612] - loss: 0.000220 
Batch[12613] - loss: 0.000273 
Batch[12614] - loss: 0.000374 
Batch[12615] - loss: 0.000242 
Batch[12616] - loss: 0.000175 
Batch[12617] - loss: 0.000430 
Batch[12618] - loss: 0.000389 
Batch[12619] - loss: 0.000144 
Batch[12620] - loss: 0.000335 
Batch[12621] - loss: 0.000450 
Batch[12622] - loss: 0.000350 
Batch[12623] - loss: 0.000696 
Batch[12624] - loss: 0.000482 
Batch[12625] - loss: 0.000254 
Batch[12626] - loss: 0.000232 
Batch[12627] - loss: 0.000257 
Batch[12628] - loss: 0.000291 
Batch[12629] - loss: 0.000212 
Batch[12630] - loss: 0.000300 
Batch[12631] - loss: 0.000258 
Batch[12632] - loss: 0.000448 
Batch[12633] - loss: 0.000274 
Batch[12634] - loss: 0.000275 
Batch[12635] - loss: 0.000233 
Batch[12636] - loss: 0.000363 
Batch[12637] - loss: 0.000196 
Batch[12638] - loss: 0.000471 
Batch[12639] - loss: 0.000233 
Batch[12640] - loss: 0.000513 
Batch[12641] - loss: 0.000225 
Batch[12642] - loss: 0.000287 
Batch[12643] - loss: 0.000280 
Batch[12644] - loss: 0.000317 
Batch[12645] - loss: 0.000339 
Batch[12646] - loss: 0.000241 
Batch[12647] - loss: 0.000341 
Batch[12648] - loss: 0.000332 
Batch[12649] - loss: 0.000179 
Batch[12650] - loss: 0.000704 
Batch[12651] - loss: 0.000308 
Batch[12652] - loss: 0.000397 
Batch[12653] - loss: 0.000473 
Batch[12654] - loss: 0.000248 
Batch[12655] - loss: 0.000287 
Batch[12656] - loss: 0.000342 
Batch[12657] - loss: 0.000373 
Batch[12658] - loss: 0.000197 
Batch[12659] - loss: 0.000365 
Batch[12660] - loss: 0.000269 
Batch[12661] - loss: 0.000286 
Batch[12662] - loss: 0.000322 
Batch[12663] - loss: 0.000297 
Batch[12664] - loss: 0.000167 
Batch[12665] - loss: 0.000503 
Batch[12666] - loss: 0.000379 
Batch[12667] - loss: 0.000423 
Batch[12668] - loss: 0.000308 
Batch[12669] - loss: 0.000236 
Batch[12670] - loss: 0.000458 
Batch[12671] - loss: 0.000196 
Batch[12672] - loss: 0.000446 
Batch[12673] - loss: 0.000225 
Batch[12674] - loss: 0.000276 
Batch[12675] - loss: 0.000329 
Batch[12676] - loss: 0.000383 
Batch[12677] - loss: 0.000188 
Batch[12678] - loss: 0.000263 
Batch[12679] - loss: 0.000461 
Batch[12680] - loss: 0.000261 
Batch[12681] - loss: 0.000173 
Batch[12682] - loss: 0.000256 
Batch[12683] - loss: 0.000424 
Batch[12684] - loss: 0.000355 
Batch[12685] - loss: 0.000242 
Batch[12686] - loss: 0.000273 
Batch[12687] - loss: 0.000345 
Batch[12688] - loss: 0.000409 
Batch[12689] - loss: 0.000308 
Batch[12690] - loss: 0.000323 
Batch[12691] - loss: 0.000257 
Batch[12692] - loss: 0.000434 
Batch[12693] - loss: 0.000400 
Batch[12694] - loss: 0.000312 
Batch[12695] - loss: 0.000331 
Batch[12696] - loss: 0.000417 
Batch[12697] - loss: 0.000144 
Batch[12698] - loss: 0.000171 
Batch[12699] - loss: 0.000285 
Batch[12700] - loss: 0.000248 

Evaluation - loss: 0.000071 pearson: 0.5519 

early stop by 1500 steps.
Batch[12701] - loss: 0.000245 
Batch[12702] - loss: 0.000370 
Batch[12703] - loss: 0.000187 
Batch[12704] - loss: 0.000289 
Batch[12705] - loss: 0.000175 
Batch[12706] - loss: 0.000233 
Batch[12707] - loss: 0.000274 
Batch[12708] - loss: 0.000397 
Batch[12709] - loss: 0.000245 
Batch[12710] - loss: 0.000249 
Batch[12711] - loss: 0.000390 
Batch[12712] - loss: 0.000513 
Batch[12713] - loss: 0.000416 
Batch[12714] - loss: 0.000378 
Batch[12715] - loss: 0.000411 
Batch[12716] - loss: 0.000357 
Batch[12717] - loss: 0.000376 
Batch[12718] - loss: 0.000374 
Batch[12719] - loss: 0.000291 
Batch[12720] - loss: 0.000271 
Batch[12721] - loss: 0.000370 
Batch[12722] - loss: 0.000242 
Batch[12723] - loss: 0.000469 
Batch[12724] - loss: 0.000225 
Batch[12725] - loss: 0.001246 
Batch[12726] - loss: 0.000360 
Batch[12727] - loss: 0.000260 
Batch[12728] - loss: 0.000403 
Batch[12729] - loss: 0.000261 
Batch[12730] - loss: 0.000279 
Batch[12731] - loss: 0.000324 
Batch[12732] - loss: 0.000315 
Batch[12733] - loss: 0.000250 
Batch[12734] - loss: 0.000399 
Batch[12735] - loss: 0.000381 
Batch[12736] - loss: 0.000378 
Batch[12737] - loss: 0.000237 
Batch[12738] - loss: 0.000280 
Batch[12739] - loss: 0.000528 
Batch[12740] - loss: 0.000253 
Batch[12741] - loss: 0.000214 
Batch[12742] - loss: 0.000424 
Batch[12743] - loss: 0.000382 
Batch[12744] - loss: 0.000355 
Batch[12745] - loss: 0.000210 
Batch[12746] - loss: 0.000282 
Batch[12747] - loss: 0.000311 
Batch[12748] - loss: 0.000379 
Batch[12749] - loss: 0.000246 
Batch[12750] - loss: 0.000208 
Batch[12751] - loss: 0.000398 
Batch[12752] - loss: 0.000201 
Batch[12753] - loss: 0.000323 
Batch[12754] - loss: 0.000285 
Batch[12755] - loss: 0.000444 
Batch[12756] - loss: 0.000338 
Batch[12757] - loss: 0.000205 
Batch[12758] - loss: 0.000254 
Batch[12759] - loss: 0.000382 
Batch[12760] - loss: 0.000374 
Batch[12761] - loss: 0.000254 
Batch[12762] - loss: 0.000427 
Batch[12763] - loss: 0.000382 
Batch[12764] - loss: 0.000237 
Batch[12765] - loss: 0.000192 
Batch[12766] - loss: 0.000214 
Batch[12767] - loss: 0.000253 
Batch[12768] - loss: 0.000494 
Batch[12769] - loss: 0.000294 
Batch[12770] - loss: 0.000215 
Batch[12771] - loss: 0.000238 
Batch[12772] - loss: 0.000265 
Batch[12773] - loss: 0.000488 
Batch[12774] - loss: 0.000287 
Batch[12775] - loss: 0.000295 
Batch[12776] - loss: 0.000326 
Batch[12777] - loss: 0.000199 
Batch[12778] - loss: 0.000376 
Batch[12779] - loss: 0.000366 
Batch[12780] - loss: 0.000617 
Batch[12781] - loss: 0.000304 
Batch[12782] - loss: 0.000496 
Batch[12783] - loss: 0.000165 
Batch[12784] - loss: 0.000434 
Batch[12785] - loss: 0.000516 
Batch[12786] - loss: 0.000250 
Batch[12787] - loss: 0.000333 
Batch[12788] - loss: 0.000307 
Batch[12789] - loss: 0.000399 
Batch[12790] - loss: 0.000222 
Batch[12791] - loss: 0.000342 
Batch[12792] - loss: 0.000420 
Batch[12793] - loss: 0.000453 
Batch[12794] - loss: 0.000192 
Batch[12795] - loss: 0.000238 
Batch[12796] - loss: 0.000225 
Batch[12797] - loss: 0.000268 
Batch[12798] - loss: 0.000372 
Batch[12799] - loss: 0.000302 
Batch[12800] - loss: 0.000309 

Evaluation - loss: 0.000071 pearson: 0.5535 

early stop by 1500 steps.
Batch[12801] - loss: 0.000272 
Batch[12802] - loss: 0.000210 
Batch[12803] - loss: 0.000566 
Batch[12804] - loss: 0.000153 
Batch[12805] - loss: 0.000292 
Batch[12806] - loss: 0.000485 
Batch[12807] - loss: 0.000315 
Batch[12808] - loss: 0.000288 
Batch[12809] - loss: 0.000240 
Batch[12810] - loss: 0.000222 
Batch[12811] - loss: 0.000339 
Batch[12812] - loss: 0.000247 
Batch[12813] - loss: 0.000422 
Batch[12814] - loss: 0.000160 
Batch[12815] - loss: 0.000226 
Batch[12816] - loss: 0.000465 
Batch[12817] - loss: 0.000307 
Batch[12818] - loss: 0.000346 
Batch[12819] - loss: 0.000243 
Batch[12820] - loss: 0.000321 
Batch[12821] - loss: 0.000211 
Batch[12822] - loss: 0.000234 
Batch[12823] - loss: 0.000506 
Batch[12824] - loss: 0.000187 
Batch[12825] - loss: 0.000150 
Batch[12826] - loss: 0.000406 
Batch[12827] - loss: 0.000392 
Batch[12828] - loss: 0.000464 
Batch[12829] - loss: 0.000225 
Batch[12830] - loss: 0.000341 
Batch[12831] - loss: 0.000316 
Batch[12832] - loss: 0.000197 
Batch[12833] - loss: 0.000186 
Batch[12834] - loss: 0.000212 
Batch[12835] - loss: 0.000137 
Batch[12836] - loss: 0.000354 
Batch[12837] - loss: 0.000439 
Batch[12838] - loss: 0.000177 
Batch[12839] - loss: 0.000228 
Batch[12840] - loss: 0.000172 
Batch[12841] - loss: 0.000236 
Batch[12842] - loss: 0.000245 
Batch[12843] - loss: 0.000217 
Batch[12844] - loss: 0.000297 
Batch[12845] - loss: 0.000458 
Batch[12846] - loss: 0.000194 
Batch[12847] - loss: 0.000354 
Batch[12848] - loss: 0.000497 
Batch[12849] - loss: 0.000434 
Batch[12850] - loss: 0.000257 
Batch[12851] - loss: 0.000347 
Batch[12852] - loss: 0.000222 
Batch[12853] - loss: 0.000283 
Batch[12854] - loss: 0.000213 
Batch[12855] - loss: 0.000255 
Batch[12856] - loss: 0.000421 
Batch[12857] - loss: 0.000191 
Batch[12858] - loss: 0.000187 
Batch[12859] - loss: 0.000408 
Batch[12860] - loss: 0.000189 
Batch[12861] - loss: 0.000308 
Batch[12862] - loss: 0.000332 
Batch[12863] - loss: 0.000418 
Batch[12864] - loss: 0.000502 
Batch[12865] - loss: 0.000172 
Batch[12866] - loss: 0.000448 
Batch[12867] - loss: 0.000291 
Batch[12868] - loss: 0.000406 
Batch[12869] - loss: 0.000324 
Batch[12870] - loss: 0.000450 
Batch[12871] - loss: 0.000330 
Batch[12872] - loss: 0.000203 
Batch[12873] - loss: 0.000324 
Batch[12874] - loss: 0.000314 
Batch[12875] - loss: 0.000326 
Batch[12876] - loss: 0.000424 
Batch[12877] - loss: 0.000152 
Batch[12878] - loss: 0.000304 
Batch[12879] - loss: 0.000270 
Batch[12880] - loss: 0.000306 
Batch[12881] - loss: 0.000280 
Batch[12882] - loss: 0.000236 
Batch[12883] - loss: 0.000258 
Batch[12884] - loss: 0.000422 
Batch[12885] - loss: 0.000282 
Batch[12886] - loss: 0.000195 
Batch[12887] - loss: 0.000450 
Batch[12888] - loss: 0.000299 
Batch[12889] - loss: 0.000143 
Batch[12890] - loss: 0.000282 
Batch[12891] - loss: 0.000392 
Batch[12892] - loss: 0.000375 
Batch[12893] - loss: 0.000187 
Batch[12894] - loss: 0.000277 
Batch[12895] - loss: 0.000554 
Batch[12896] - loss: 0.000172 
Batch[12897] - loss: 0.000246 
Batch[12898] - loss: 0.000296 
Batch[12899] - loss: 0.000085 
Batch[12900] - loss: 0.000168 

Evaluation - loss: 0.000071 pearson: 0.5554 

early stop by 1500 steps.
Batch[12901] - loss: 0.000370 
Batch[12902] - loss: 0.000355 
Batch[12903] - loss: 0.000225 
Batch[12904] - loss: 0.000293 
Batch[12905] - loss: 0.000490 
Batch[12906] - loss: 0.000371 
Batch[12907] - loss: 0.000210 
Batch[12908] - loss: 0.000237 
Batch[12909] - loss: 0.000228 
Batch[12910] - loss: 0.000244 
Batch[12911] - loss: 0.000266 
Batch[12912] - loss: 0.000210 
Batch[12913] - loss: 0.000326 
Batch[12914] - loss: 0.000192 
Batch[12915] - loss: 0.000221 
Batch[12916] - loss: 0.000261 
Batch[12917] - loss: 0.000271 
Batch[12918] - loss: 0.000330 
Batch[12919] - loss: 0.000231 
Batch[12920] - loss: 0.000395 
Batch[12921] - loss: 0.000256 
Batch[12922] - loss: 0.000254 
Batch[12923] - loss: 0.000160 
Batch[12924] - loss: 0.000209 
Batch[12925] - loss: 0.000211 
Batch[12926] - loss: 0.000164 
Batch[12927] - loss: 0.000516 
Batch[12928] - loss: 0.000178 
Batch[12929] - loss: 0.001019 
Batch[12930] - loss: 0.000218 
Batch[12931] - loss: 0.000234 
Batch[12932] - loss: 0.000250 
Batch[12933] - loss: 0.000348 
Batch[12934] - loss: 0.000395 
Batch[12935] - loss: 0.000249 
Batch[12936] - loss: 0.000461 
Batch[12937] - loss: 0.000262 
Batch[12938] - loss: 0.000204 
Batch[12939] - loss: 0.000251 
Batch[12940] - loss: 0.000223 
Batch[12941] - loss: 0.000241 
Batch[12942] - loss: 0.000261 
Batch[12943] - loss: 0.000171 
Batch[12944] - loss: 0.000209 
Batch[12945] - loss: 0.000245 
Batch[12946] - loss: 0.000363 
Batch[12947] - loss: 0.000195 
Batch[12948] - loss: 0.000256 
Batch[12949] - loss: 0.000173 
Batch[12950] - loss: 0.000219 
Batch[12951] - loss: 0.000256 
Batch[12952] - loss: 0.000271 
Batch[12953] - loss: 0.000151 
Batch[12954] - loss: 0.000621 
Batch[12955] - loss: 0.000203 
Batch[12956] - loss: 0.000280 
Batch[12957] - loss: 0.000176 
Batch[12958] - loss: 0.000291 
Batch[12959] - loss: 0.000271 
Batch[12960] - loss: 0.000210 
Batch[12961] - loss: 0.000324 
Batch[12962] - loss: 0.000323 
Batch[12963] - loss: 0.000449 
Batch[12964] - loss: 0.000228 
Batch[12965] - loss: 0.000310 
Batch[12966] - loss: 0.000332 
Batch[12967] - loss: 0.000145 
Batch[12968] - loss: 0.000314 
Batch[12969] - loss: 0.000198 
Batch[12970] - loss: 0.000204 
Batch[12971] - loss: 0.000188 
Batch[12972] - loss: 0.000202 
Batch[12973] - loss: 0.000316 
Batch[12974] - loss: 0.000211 
Batch[12975] - loss: 0.000166 
Batch[12976] - loss: 0.000162 
Batch[12977] - loss: 0.000300 
Batch[12978] - loss: 0.000368 
Batch[12979] - loss: 0.000130 
Batch[12980] - loss: 0.000188 
Batch[12981] - loss: 0.000281 
Batch[12982] - loss: 0.000277 
Batch[12983] - loss: 0.000156 
Batch[12984] - loss: 0.000248 
Batch[12985] - loss: 0.000204 
Batch[12986] - loss: 0.000336 
Batch[12987] - loss: 0.000149 
Batch[12988] - loss: 0.000224 
Batch[12989] - loss: 0.000269 
Batch[12990] - loss: 0.000301 
Batch[12991] - loss: 0.000339 
Batch[12992] - loss: 0.000413 
Batch[12993] - loss: 0.000255 
Batch[12994] - loss: 0.000205 
Batch[12995] - loss: 0.000549 
Batch[12996] - loss: 0.000262 
Batch[12997] - loss: 0.000342 
Batch[12998] - loss: 0.000278 
Batch[12999] - loss: 0.000301 
Batch[13000] - loss: 0.000197 

Evaluation - loss: 0.000071 pearson: 0.5553 

early stop by 1500 steps.
Batch[13001] - loss: 0.000482 
Batch[13002] - loss: 0.000197 
Batch[13003] - loss: 0.000187 
Batch[13004] - loss: 0.000273 
Batch[13005] - loss: 0.000205 
Batch[13006] - loss: 0.000156 
Batch[13007] - loss: 0.000204 
Batch[13008] - loss: 0.000480 
Batch[13009] - loss: 0.000336 
Batch[13010] - loss: 0.000265 
Batch[13011] - loss: 0.000151 
Batch[13012] - loss: 0.000235 
Batch[13013] - loss: 0.000188 
Batch[13014] - loss: 0.000318 
Batch[13015] - loss: 0.000374 
Batch[13016] - loss: 0.000348 
Batch[13017] - loss: 0.000171 
Batch[13018] - loss: 0.000400 
Batch[13019] - loss: 0.000227 
Batch[13020] - loss: 0.000231 
Batch[13021] - loss: 0.000360 
Batch[13022] - loss: 0.000246 
Batch[13023] - loss: 0.000218 
Batch[13024] - loss: 0.000481 
Batch[13025] - loss: 0.000373 
Batch[13026] - loss: 0.000273 
Batch[13027] - loss: 0.000309 
Batch[13028] - loss: 0.000330 
Batch[13029] - loss: 0.000199 
Batch[13030] - loss: 0.000215 
Batch[13031] - loss: 0.000333 
Batch[13032] - loss: 0.000459 
Batch[13033] - loss: 0.000154 
Batch[13034] - loss: 0.000178 
Batch[13035] - loss: 0.000332 
Batch[13036] - loss: 0.000173 
Batch[13037] - loss: 0.000136 
Batch[13038] - loss: 0.000285 
Batch[13039] - loss: 0.000197 
Batch[13040] - loss: 0.000396 
Batch[13041] - loss: 0.000230 
Batch[13042] - loss: 0.000307 
Batch[13043] - loss: 0.000333 
Batch[13044] - loss: 0.000195 
Batch[13045] - loss: 0.000280 
Batch[13046] - loss: 0.000162 
Batch[13047] - loss: 0.000342 
Batch[13048] - loss: 0.000227 
Batch[13049] - loss: 0.000196 
Batch[13050] - loss: 0.000295 
Batch[13051] - loss: 0.000277 
Batch[13052] - loss: 0.000214 
Batch[13053] - loss: 0.000314 
Batch[13054] - loss: 0.000399 
Batch[13055] - loss: 0.000428 
Batch[13056] - loss: 0.000116 
Batch[13057] - loss: 0.000324 
Batch[13058] - loss: 0.000262 
Batch[13059] - loss: 0.000220 
Batch[13060] - loss: 0.000280 
Batch[13061] - loss: 0.000261 
Batch[13062] - loss: 0.000150 
Batch[13063] - loss: 0.000438 
Batch[13064] - loss: 0.000554 
Batch[13065] - loss: 0.000324 
Batch[13066] - loss: 0.000281 
Batch[13067] - loss: 0.000203 
Batch[13068] - loss: 0.000187 
Batch[13069] - loss: 0.000364 
Batch[13070] - loss: 0.000311 
Batch[13071] - loss: 0.000404 
Batch[13072] - loss: 0.000283 
Batch[13073] - loss: 0.000252 
Batch[13074] - loss: 0.000298 
Batch[13075] - loss: 0.000192 
Batch[13076] - loss: 0.000215 
Batch[13077] - loss: 0.000285 
Batch[13078] - loss: 0.000185 
Batch[13079] - loss: 0.000277 
Batch[13080] - loss: 0.000224 
Batch[13081] - loss: 0.000195 
Batch[13082] - loss: 0.000441 
Batch[13083] - loss: 0.000189 
Batch[13084] - loss: 0.000295 
Batch[13085] - loss: 0.000247 
Batch[13086] - loss: 0.000144 
Batch[13087] - loss: 0.000172 
Batch[13088] - loss: 0.000168 
Batch[13089] - loss: 0.000188 
Batch[13090] - loss: 0.000202 
Batch[13091] - loss: 0.000288 
Batch[13092] - loss: 0.000218 
Batch[13093] - loss: 0.000176 
Batch[13094] - loss: 0.000263 
Batch[13095] - loss: 0.000268 
Batch[13096] - loss: 0.000157 
Batch[13097] - loss: 0.000338 
Batch[13098] - loss: 0.000251 
Batch[13099] - loss: 0.000257 
Batch[13100] - loss: 0.000141 

Evaluation - loss: 0.000071 pearson: 0.5542 

early stop by 1500 steps.
Batch[13101] - loss: 0.000206 
Batch[13102] - loss: 0.000168 
Batch[13103] - loss: 0.000284 
Batch[13104] - loss: 0.000183 
Batch[13105] - loss: 0.000165 
Batch[13106] - loss: 0.000319 
Batch[13107] - loss: 0.000195 
Batch[13108] - loss: 0.000204 
Batch[13109] - loss: 0.000700 
Batch[13110] - loss: 0.000404 
Batch[13111] - loss: 0.000215 
Batch[13112] - loss: 0.000289 
Batch[13113] - loss: 0.000211 
Batch[13114] - loss: 0.000228 
Batch[13115] - loss: 0.000184 
Batch[13116] - loss: 0.000205 
Batch[13117] - loss: 0.000198 
Batch[13118] - loss: 0.000160 
Batch[13119] - loss: 0.000427 
Batch[13120] - loss: 0.000336 
Batch[13121] - loss: 0.000191 
Batch[13122] - loss: 0.000218 
Batch[13123] - loss: 0.000239 
Batch[13124] - loss: 0.000350 
Batch[13125] - loss: 0.000431 
Batch[13126] - loss: 0.000440 
Batch[13127] - loss: 0.000401 
Batch[13128] - loss: 0.000366 
Batch[13129] - loss: 0.000219 
Batch[13130] - loss: 0.000292 
Batch[13131] - loss: 0.000197 
Batch[13132] - loss: 0.000133 
Batch[13133] - loss: 0.000232 
Batch[13134] - loss: 0.000388 
Batch[13135] - loss: 0.000155 
Batch[13136] - loss: 0.000243 
Batch[13137] - loss: 0.000379 
Batch[13138] - loss: 0.000246 
Batch[13139] - loss: 0.000375 
Batch[13140] - loss: 0.000470 
Batch[13141] - loss: 0.000330 
Batch[13142] - loss: 0.000206 
Batch[13143] - loss: 0.000590 
Batch[13144] - loss: 0.000271 
Batch[13145] - loss: 0.000238 
Batch[13146] - loss: 0.000227 
Batch[13147] - loss: 0.000228 
Batch[13148] - loss: 0.000239 
Batch[13149] - loss: 0.000146 
Batch[13150] - loss: 0.000176 
Batch[13151] - loss: 0.000339 
Batch[13152] - loss: 0.000147 
Batch[13153] - loss: 0.000193 
Batch[13154] - loss: 0.000214 
Batch[13155] - loss: 0.000342 
Batch[13156] - loss: 0.000206 
Batch[13157] - loss: 0.000164 
Batch[13158] - loss: 0.000284 
Batch[13159] - loss: 0.000389 
Batch[13160] - loss: 0.000275 
Batch[13161] - loss: 0.000385 
Batch[13162] - loss: 0.000236 
Batch[13163] - loss: 0.000234 
Batch[13164] - loss: 0.000115 
Batch[13165] - loss: 0.000237 
Batch[13166] - loss: 0.000171 
Batch[13167] - loss: 0.000197 
Batch[13168] - loss: 0.000250 
Batch[13169] - loss: 0.000181 
Batch[13170] - loss: 0.000286 
Batch[13171] - loss: 0.000483 
Batch[13172] - loss: 0.000412 
Batch[13173] - loss: 0.000144 
Batch[13174] - loss: 0.000281 
Batch[13175] - loss: 0.000252 
Batch[13176] - loss: 0.000183 
Batch[13177] - loss: 0.000187 
Batch[13178] - loss: 0.000181 
Batch[13179] - loss: 0.000375 
Batch[13180] - loss: 0.000174 
Batch[13181] - loss: 0.000224 
Batch[13182] - loss: 0.000170 
Batch[13183] - loss: 0.000141 
Batch[13184] - loss: 0.000248 
Batch[13185] - loss: 0.000574 
Batch[13186] - loss: 0.000321 
Batch[13187] - loss: 0.000272 
Batch[13188] - loss: 0.000394 
Batch[13189] - loss: 0.000231 
Batch[13190] - loss: 0.000567 
Batch[13191] - loss: 0.000208 
Batch[13192] - loss: 0.000185 
Batch[13193] - loss: 0.000374 
Batch[13194] - loss: 0.000243 
Batch[13195] - loss: 0.000214 
Batch[13196] - loss: 0.000279 
Batch[13197] - loss: 0.000269 
Batch[13198] - loss: 0.000172 
Batch[13199] - loss: 0.000323 
Batch[13200] - loss: 0.000227 

Evaluation - loss: 0.000071 pearson: 0.5524 

early stop by 1500 steps.
Batch[13201] - loss: 0.000314 
Batch[13202] - loss: 0.000815 
Batch[13203] - loss: 0.000493 
Batch[13204] - loss: 0.000491 
Batch[13205] - loss: 0.000303 
Batch[13206] - loss: 0.000182 
Batch[13207] - loss: 0.000263 
Batch[13208] - loss: 0.000291 
Batch[13209] - loss: 0.000203 
Batch[13210] - loss: 0.000235 
Batch[13211] - loss: 0.000237 
Batch[13212] - loss: 0.000419 
Batch[13213] - loss: 0.000247 
Batch[13214] - loss: 0.000363 
Batch[13215] - loss: 0.000262 
Batch[13216] - loss: 0.000272 
Batch[13217] - loss: 0.000252 
Batch[13218] - loss: 0.000293 
Batch[13219] - loss: 0.000227 
Batch[13220] - loss: 0.000389 
Batch[13221] - loss: 0.000203 
Batch[13222] - loss: 0.000334 
Batch[13223] - loss: 0.000373 
Batch[13224] - loss: 0.000332 
Batch[13225] - loss: 0.000280 
Batch[13226] - loss: 0.000172 
Batch[13227] - loss: 0.000446 
Batch[13228] - loss: 0.000225 
Batch[13229] - loss: 0.000463 
Batch[13230] - loss: 0.000285 
Batch[13231] - loss: 0.000248 
Batch[13232] - loss: 0.000220 
Batch[13233] - loss: 0.000283 
Batch[13234] - loss: 0.000241 
Batch[13235] - loss: 0.000209 
Batch[13236] - loss: 0.000293 
Batch[13237] - loss: 0.000169 
Batch[13238] - loss: 0.000255 
Batch[13239] - loss: 0.000156 
Batch[13240] - loss: 0.000224 
Batch[13241] - loss: 0.000207 
Batch[13242] - loss: 0.000177 
Batch[13243] - loss: 0.000207 
Batch[13244] - loss: 0.000170 
Batch[13245] - loss: 0.000154 
Batch[13246] - loss: 0.000421 
Batch[13247] - loss: 0.000190 
Batch[13248] - loss: 0.000126 
Batch[13249] - loss: 0.000195 
Batch[13250] - loss: 0.000099 
Batch[13251] - loss: 0.000238 
Batch[13252] - loss: 0.000397 
Batch[13253] - loss: 0.000393 
Batch[13254] - loss: 0.000218 
Batch[13255] - loss: 0.000289 
Batch[13256] - loss: 0.000262 
Batch[13257] - loss: 0.000196 
Batch[13258] - loss: 0.000181 
Batch[13259] - loss: 0.000212 
Batch[13260] - loss: 0.000276 
Batch[13261] - loss: 0.000364 
Batch[13262] - loss: 0.000283 
Batch[13263] - loss: 0.000212 
Batch[13264] - loss: 0.000218 
Batch[13265] - loss: 0.000195 
Batch[13266] - loss: 0.000371 
Batch[13267] - loss: 0.000430 
Batch[13268] - loss: 0.000294 
Batch[13269] - loss: 0.000245 
Batch[13270] - loss: 0.000226 
Batch[13271] - loss: 0.000269 
Batch[13272] - loss: 0.000224 
Batch[13273] - loss: 0.000266 
Batch[13274] - loss: 0.000303 
Batch[13275] - loss: 0.000316 
Batch[13276] - loss: 0.000241 
Batch[13277] - loss: 0.000438 
Batch[13278] - loss: 0.000358 
Batch[13279] - loss: 0.000344 
Batch[13280] - loss: 0.000302 
Batch[13281] - loss: 0.000251 
Batch[13282] - loss: 0.000249 
Batch[13283] - loss: 0.000225 
Batch[13284] - loss: 0.000258 
Batch[13285] - loss: 0.000237 
Batch[13286] - loss: 0.000229 
Batch[13287] - loss: 0.000582 
Batch[13288] - loss: 0.000232 
Batch[13289] - loss: 0.000609 
Batch[13290] - loss: 0.000569 
Batch[13291] - loss: 0.000131 
Batch[13292] - loss: 0.000227 
Batch[13293] - loss: 0.000278 
Batch[13294] - loss: 0.000243 
Batch[13295] - loss: 0.000277 
Batch[13296] - loss: 0.000280 
Batch[13297] - loss: 0.000358 
Batch[13298] - loss: 0.000365 
Batch[13299] - loss: 0.000258 
Batch[13300] - loss: 0.000502 

Evaluation - loss: 0.000071 pearson: 0.5521 

early stop by 1500 steps.
Batch[13301] - loss: 0.000295 
Batch[13302] - loss: 0.000351 
Batch[13303] - loss: 0.000283 
Batch[13304] - loss: 0.000196 
Batch[13305] - loss: 0.000289 
Batch[13306] - loss: 0.000263 
Batch[13307] - loss: 0.000239 
Batch[13308] - loss: 0.000250 
Batch[13309] - loss: 0.000230 
Batch[13310] - loss: 0.000228 
Batch[13311] - loss: 0.000347 
Batch[13312] - loss: 0.000226 
Batch[13313] - loss: 0.000307 
Batch[13314] - loss: 0.000212 
Batch[13315] - loss: 0.000271 
Batch[13316] - loss: 0.000223 
Batch[13317] - loss: 0.000500 
Batch[13318] - loss: 0.000478 
Batch[13319] - loss: 0.000233 
Batch[13320] - loss: 0.000159 
Batch[13321] - loss: 0.000326 
Batch[13322] - loss: 0.000234 
Batch[13323] - loss: 0.000288 
Batch[13324] - loss: 0.000209 
Batch[13325] - loss: 0.000278 
Batch[13326] - loss: 0.000292 
Batch[13327] - loss: 0.000234 
Batch[13328] - loss: 0.000272 
Batch[13329] - loss: 0.000320 
Batch[13330] - loss: 0.000373 
Batch[13331] - loss: 0.000272 
Batch[13332] - loss: 0.000336 
Batch[13333] - loss: 0.000599 
Batch[13334] - loss: 0.000267 
Batch[13335] - loss: 0.000281 
Batch[13336] - loss: 0.000366 
Batch[13337] - loss: 0.000391 
Batch[13338] - loss: 0.000229 
Batch[13339] - loss: 0.000270 
Batch[13340] - loss: 0.000199 
Batch[13341] - loss: 0.000282 
Batch[13342] - loss: 0.000301 
Batch[13343] - loss: 0.000212 
Batch[13344] - loss: 0.000216 
Batch[13345] - loss: 0.000183 
Batch[13346] - loss: 0.000578 
Batch[13347] - loss: 0.000320 
Batch[13348] - loss: 0.000288 
Batch[13349] - loss: 0.000385 
Batch[13350] - loss: 0.000289 
Batch[13351] - loss: 0.000261 
Batch[13352] - loss: 0.000310 
Batch[13353] - loss: 0.000257 
Batch[13354] - loss: 0.000300 
Batch[13355] - loss: 0.000259 
Batch[13356] - loss: 0.000463 
Batch[13357] - loss: 0.000175 
Batch[13358] - loss: 0.000138 
Batch[13359] - loss: 0.000229 
Batch[13360] - loss: 0.000232 
Batch[13361] - loss: 0.000423 
Batch[13362] - loss: 0.000334 
Batch[13363] - loss: 0.000217 
Batch[13364] - loss: 0.000337 
Batch[13365] - loss: 0.000227 
Batch[13366] - loss: 0.000448 
Batch[13367] - loss: 0.000158 
Batch[13368] - loss: 0.000226 
Batch[13369] - loss: 0.000301 
Batch[13370] - loss: 0.000271 
Batch[13371] - loss: 0.000318 
Batch[13372] - loss: 0.000329 
Batch[13373] - loss: 0.000503 
Batch[13374] - loss: 0.000319 
Batch[13375] - loss: 0.000260 
Batch[13376] - loss: 0.000466 
Batch[13377] - loss: 0.000314 
Batch[13378] - loss: 0.000204 
Batch[13379] - loss: 0.000293 
Batch[13380] - loss: 0.000327 
Batch[13381] - loss: 0.000216 
Batch[13382] - loss: 0.000315 
Batch[13383] - loss: 0.000419 
Batch[13384] - loss: 0.000278 
Batch[13385] - loss: 0.000221 
Batch[13386] - loss: 0.000276 
Batch[13387] - loss: 0.000166 
Batch[13388] - loss: 0.000283 
Batch[13389] - loss: 0.000587 
Batch[13390] - loss: 0.000359 
Batch[13391] - loss: 0.000238 
Batch[13392] - loss: 0.000167 
Batch[13393] - loss: 0.000268 
Batch[13394] - loss: 0.000204 
Batch[13395] - loss: 0.000239 
Batch[13396] - loss: 0.000473 
Batch[13397] - loss: 0.000412 
Batch[13398] - loss: 0.000272 
Batch[13399] - loss: 0.000244 
Batch[13400] - loss: 0.000396 

Evaluation - loss: 0.000071 pearson: 0.5525 

early stop by 1500 steps.
Batch[13401] - loss: 0.000300 
Batch[13402] - loss: 0.000258 
Batch[13403] - loss: 0.000218 
Batch[13404] - loss: 0.000331 
Batch[13405] - loss: 0.000395 
Batch[13406] - loss: 0.000209 
Batch[13407] - loss: 0.000271 
Batch[13408] - loss: 0.000269 
Batch[13409] - loss: 0.000182 
Batch[13410] - loss: 0.000185 
Batch[13411] - loss: 0.000303 
Batch[13412] - loss: 0.000362 
Batch[13413] - loss: 0.000598 
Batch[13414] - loss: 0.000254 
Batch[13415] - loss: 0.000255 
Batch[13416] - loss: 0.000453 
Batch[13417] - loss: 0.000461 
Batch[13418] - loss: 0.000293 
Batch[13419] - loss: 0.000272 
Batch[13420] - loss: 0.000339 
Batch[13421] - loss: 0.000285 
Batch[13422] - loss: 0.000307 
Batch[13423] - loss: 0.000363 
Batch[13424] - loss: 0.000683 
Batch[13425] - loss: 0.000251 
Batch[13426] - loss: 0.000225 
Batch[13427] - loss: 0.000482 
Batch[13428] - loss: 0.000727 
Batch[13429] - loss: 0.000974 
Batch[13430] - loss: 0.000323 
Batch[13431] - loss: 0.000383 
Batch[13432] - loss: 0.000256 
Batch[13433] - loss: 0.000223 
Batch[13434] - loss: 0.000142 
Batch[13435] - loss: 0.000185 
Batch[13436] - loss: 0.000339 
Batch[13437] - loss: 0.000415 
Batch[13438] - loss: 0.000322 
Batch[13439] - loss: 0.000269 
Batch[13440] - loss: 0.000345 
Batch[13441] - loss: 0.000430 
Batch[13442] - loss: 0.000177 
Batch[13443] - loss: 0.000442 
Batch[13444] - loss: 0.000316 
Batch[13445] - loss: 0.000355 
Batch[13446] - loss: 0.000385 
Batch[13447] - loss: 0.000155 
Batch[13448] - loss: 0.000145 
Batch[13449] - loss: 0.000347 
Batch[13450] - loss: 0.000190 
Batch[13451] - loss: 0.000289 
Batch[13452] - loss: 0.000337 
Batch[13453] - loss: 0.000249 
Batch[13454] - loss: 0.000251 
Batch[13455] - loss: 0.000360 
Batch[13456] - loss: 0.000291 
Batch[13457] - loss: 0.000227 
Batch[13458] - loss: 0.000180 
Batch[13459] - loss: 0.000217 
Batch[13460] - loss: 0.000358 
Batch[13461] - loss: 0.000345 
Batch[13462] - loss: 0.000153 
Batch[13463] - loss: 0.000178 
Batch[13464] - loss: 0.000323 
Batch[13465] - loss: 0.000194 
Batch[13466] - loss: 0.000258 
Batch[13467] - loss: 0.000439 
Batch[13468] - loss: 0.000345 
Batch[13469] - loss: 0.000307 
Batch[13470] - loss: 0.000212 
Batch[13471] - loss: 0.000324 
Batch[13472] - loss: 0.000190 
Batch[13473] - loss: 0.000237 
Batch[13474] - loss: 0.000268 
Batch[13475] - loss: 0.000472 
Batch[13476] - loss: 0.000480 
Batch[13477] - loss: 0.000321 
Batch[13478] - loss: 0.000167 
Batch[13479] - loss: 0.000259 
Batch[13480] - loss: 0.000291 
Batch[13481] - loss: 0.000274 
Batch[13482] - loss: 0.000175 
Batch[13483] - loss: 0.000351 
Batch[13484] - loss: 0.000467 
Batch[13485] - loss: 0.000170 
Batch[13486] - loss: 0.000235 
Batch[13487] - loss: 0.000285 
Batch[13488] - loss: 0.000152 
Batch[13489] - loss: 0.000354 
Batch[13490] - loss: 0.000572 
Batch[13491] - loss: 0.000285 
Batch[13492] - loss: 0.000276 
Batch[13493] - loss: 0.000664 
Batch[13494] - loss: 0.000846 
Batch[13495] - loss: 0.000219 
Batch[13496] - loss: 0.000467 
Batch[13497] - loss: 0.000379 
Batch[13498] - loss: 0.000330 
Batch[13499] - loss: 0.000229 
Batch[13500] - loss: 0.000236 

Evaluation - loss: 0.000071 pearson: 0.5533 

early stop by 1500 steps.
Batch[13501] - loss: 0.000290 
Batch[13502] - loss: 0.000488 
Batch[13503] - loss: 0.000334 
Batch[13504] - loss: 0.000326 
Batch[13505] - loss: 0.000212 
Batch[13506] - loss: 0.000209 
Batch[13507] - loss: 0.000178 
Batch[13508] - loss: 0.000222 
Batch[13509] - loss: 0.000522 
Batch[13510] - loss: 0.000400 
Batch[13511] - loss: 0.000337 
Batch[13512] - loss: 0.000386 
Batch[13513] - loss: 0.000387 
Batch[13514] - loss: 0.000232 
Batch[13515] - loss: 0.000532 
Batch[13516] - loss: 0.000339 
Batch[13517] - loss: 0.000373 
Batch[13518] - loss: 0.000264 
Batch[13519] - loss: 0.000390 
Batch[13520] - loss: 0.000296 
Batch[13521] - loss: 0.000206 
Batch[13522] - loss: 0.000392 
Batch[13523] - loss: 0.000202 
Batch[13524] - loss: 0.000381 
Batch[13525] - loss: 0.000196 
Batch[13526] - loss: 0.000179 
Batch[13527] - loss: 0.000393 
Batch[13528] - loss: 0.000368 
Batch[13529] - loss: 0.000415 
Batch[13530] - loss: 0.000214 
Batch[13531] - loss: 0.000320 
Batch[13532] - loss: 0.000340 
Batch[13533] - loss: 0.000320 
Batch[13534] - loss: 0.000156 
Batch[13535] - loss: 0.000454 
Batch[13536] - loss: 0.000431 
Batch[13537] - loss: 0.000246 
Batch[13538] - loss: 0.000368 
Batch[13539] - loss: 0.000348 
Batch[13540] - loss: 0.000223 
Batch[13541] - loss: 0.000298 
Batch[13542] - loss: 0.000334 
Batch[13543] - loss: 0.000328 
Batch[13544] - loss: 0.000357 
Batch[13545] - loss: 0.000357 
Batch[13546] - loss: 0.000312 
Batch[13547] - loss: 0.000274 
Batch[13548] - loss: 0.000322 
Batch[13549] - loss: 0.000155 
Batch[13550] - loss: 0.000297 
Batch[13551] - loss: 0.000322 
Batch[13552] - loss: 0.000713 
Batch[13553] - loss: 0.000302 
Batch[13554] - loss: 0.000338 
Batch[13555] - loss: 0.000293 
Batch[13556] - loss: 0.000325 
Batch[13557] - loss: 0.000256 
Batch[13558] - loss: 0.000260 
Batch[13559] - loss: 0.000458 
Batch[13560] - loss: 0.000104 
Batch[13561] - loss: 0.000263 
Batch[13562] - loss: 0.000266 
Batch[13563] - loss: 0.000395 
Batch[13564] - loss: 0.000309 
Batch[13565] - loss: 0.000589 
Batch[13566] - loss: 0.000432 
Batch[13567] - loss: 0.000259 
Batch[13568] - loss: 0.000499 
Batch[13569] - loss: 0.000268 
Batch[13570] - loss: 0.000329 
Batch[13571] - loss: 0.000607 
Batch[13572] - loss: 0.000479 
Batch[13573] - loss: 0.000211 
Batch[13574] - loss: 0.000310 
Batch[13575] - loss: 0.000289 
Batch[13576] - loss: 0.000421 
Batch[13577] - loss: 0.000385 
Batch[13578] - loss: 0.000393 
Batch[13579] - loss: 0.000214 
Batch[13580] - loss: 0.000289 
Batch[13581] - loss: 0.000473 
Batch[13582] - loss: 0.000208 
Batch[13583] - loss: 0.000214 
Batch[13584] - loss: 0.000301 
Batch[13585] - loss: 0.000167 
Batch[13586] - loss: 0.000359 
Batch[13587] - loss: 0.000327 
Batch[13588] - loss: 0.000549 
Batch[13589] - loss: 0.000162 
Batch[13590] - loss: 0.000143 
Batch[13591] - loss: 0.000301 
Batch[13592] - loss: 0.000498 
Batch[13593] - loss: 0.000107 
Batch[13594] - loss: 0.000229 
Batch[13595] - loss: 0.000468 
Batch[13596] - loss: 0.000304 
Batch[13597] - loss: 0.000262 
Batch[13598] - loss: 0.000290 
Batch[13599] - loss: 0.000187 
Batch[13600] - loss: 0.000340 

Evaluation - loss: 0.000071 pearson: 0.5508 

early stop by 1500 steps.
Batch[13601] - loss: 0.000360 
Batch[13602] - loss: 0.000314 
Batch[13603] - loss: 0.000290 
Batch[13604] - loss: 0.000340 
Batch[13605] - loss: 0.000299 
Batch[13606] - loss: 0.000201 
Batch[13607] - loss: 0.000263 
Batch[13608] - loss: 0.000332 
Batch[13609] - loss: 0.000181 
Batch[13610] - loss: 0.000145 
Batch[13611] - loss: 0.000317 
Batch[13612] - loss: 0.000361 
Batch[13613] - loss: 0.000297 
Batch[13614] - loss: 0.000667 
Batch[13615] - loss: 0.000264 
Batch[13616] - loss: 0.000314 
Batch[13617] - loss: 0.000246 
Batch[13618] - loss: 0.000344 
Batch[13619] - loss: 0.000203 
Batch[13620] - loss: 0.000419 
Batch[13621] - loss: 0.000350 
Batch[13622] - loss: 0.000098 
Batch[13623] - loss: 0.000236 
Batch[13624] - loss: 0.000402 
Batch[13625] - loss: 0.000266 
Batch[13626] - loss: 0.000321 
Batch[13627] - loss: 0.000256 
Batch[13628] - loss: 0.000476 
Batch[13629] - loss: 0.000207 
Batch[13630] - loss: 0.000255 
Batch[13631] - loss: 0.000377 
Batch[13632] - loss: 0.000478 
Batch[13633] - loss: 0.000270 
Batch[13634] - loss: 0.000212 
Batch[13635] - loss: 0.000216 
Batch[13636] - loss: 0.000361 
Batch[13637] - loss: 0.000265 
Batch[13638] - loss: 0.000293 
Batch[13639] - loss: 0.000166 
Batch[13640] - loss: 0.000453 
Batch[13641] - loss: 0.000507 
Batch[13642] - loss: 0.000356 
Batch[13643] - loss: 0.000339 
Batch[13644] - loss: 0.000232 
Batch[13645] - loss: 0.000164 
Batch[13646] - loss: 0.000265 
Batch[13647] - loss: 0.000291 
Batch[13648] - loss: 0.000268 
Batch[13649] - loss: 0.000284 
Batch[13650] - loss: 0.000389 
Batch[13651] - loss: 0.000495 
Batch[13652] - loss: 0.000376 
Batch[13653] - loss: 0.000331 
Batch[13654] - loss: 0.000317 
Batch[13655] - loss: 0.000475 
Batch[13656] - loss: 0.000337 
Batch[13657] - loss: 0.000294 
Batch[13658] - loss: 0.000183 
Batch[13659] - loss: 0.000254 
Batch[13660] - loss: 0.000275 
Batch[13661] - loss: 0.000270 
Batch[13662] - loss: 0.000140 
Batch[13663] - loss: 0.000306 
Batch[13664] - loss: 0.000159 
Batch[13665] - loss: 0.000160 
Batch[13666] - loss: 0.000224 
Batch[13667] - loss: 0.000191 
Batch[13668] - loss: 0.000213 
Batch[13669] - loss: 0.000213 
Batch[13670] - loss: 0.000631 
Batch[13671] - loss: 0.000210 
Batch[13672] - loss: 0.000352 
Batch[13673] - loss: 0.000380 
Batch[13674] - loss: 0.000321 
Batch[13675] - loss: 0.000281 
Batch[13676] - loss: 0.000294 
Batch[13677] - loss: 0.000388 
Batch[13678] - loss: 0.000519 
Batch[13679] - loss: 0.000419 
Batch[13680] - loss: 0.000458 
Batch[13681] - loss: 0.000415 
Batch[13682] - loss: 0.000255 
Batch[13683] - loss: 0.000326 
Batch[13684] - loss: 0.000297 
Batch[13685] - loss: 0.000282 
Batch[13686] - loss: 0.000225 
Batch[13687] - loss: 0.000427 
Batch[13688] - loss: 0.000255 
Batch[13689] - loss: 0.000421 
Batch[13690] - loss: 0.000294 
Batch[13691] - loss: 0.000316 
Batch[13692] - loss: 0.000307 
Batch[13693] - loss: 0.000581 
Batch[13694] - loss: 0.000258 
Batch[13695] - loss: 0.000120 
Batch[13696] - loss: 0.000411 
Batch[13697] - loss: 0.000177 
Batch[13698] - loss: 0.000282 
Batch[13699] - loss: 0.000309 
Batch[13700] - loss: 0.000257 

Evaluation - loss: 0.000072 pearson: 0.5511 

early stop by 1500 steps.
Batch[13701] - loss: 0.000295 
Batch[13702] - loss: 0.000268 
Batch[13703] - loss: 0.000289 
Batch[13704] - loss: 0.000224 
Batch[13705] - loss: 0.000255 
Batch[13706] - loss: 0.000274 
Batch[13707] - loss: 0.000354 
Batch[13708] - loss: 0.000412 
Batch[13709] - loss: 0.000223 
Batch[13710] - loss: 0.000108 
Batch[13711] - loss: 0.000244 
Batch[13712] - loss: 0.000317 
Batch[13713] - loss: 0.000233 
Batch[13714] - loss: 0.000443 
Batch[13715] - loss: 0.000249 
Batch[13716] - loss: 0.000329 
Batch[13717] - loss: 0.000198 
Batch[13718] - loss: 0.000332 
Batch[13719] - loss: 0.000464 
Batch[13720] - loss: 0.000357 
Batch[13721] - loss: 0.000328 
Batch[13722] - loss: 0.000187 
Batch[13723] - loss: 0.000257 
Batch[13724] - loss: 0.000310 
Batch[13725] - loss: 0.000280 
Batch[13726] - loss: 0.000364 
Batch[13727] - loss: 0.000320 
Batch[13728] - loss: 0.000430 
Batch[13729] - loss: 0.000249 
Batch[13730] - loss: 0.000150 
Batch[13731] - loss: 0.000111 
Batch[13732] - loss: 0.000285 
Batch[13733] - loss: 0.000177 
Batch[13734] - loss: 0.000308 
Batch[13735] - loss: 0.000344 
Batch[13736] - loss: 0.000220 
Batch[13737] - loss: 0.000320 
Batch[13738] - loss: 0.000239 
Batch[13739] - loss: 0.000273 
Batch[13740] - loss: 0.000280 
Batch[13741] - loss: 0.000536 
Batch[13742] - loss: 0.000306 
Batch[13743] - loss: 0.000271 
Batch[13744] - loss: 0.000476 
Batch[13745] - loss: 0.000271 
Batch[13746] - loss: 0.000291 
Batch[13747] - loss: 0.000405 
Batch[13748] - loss: 0.000296 
Batch[13749] - loss: 0.000281 
Batch[13750] - loss: 0.000545 
Batch[13751] - loss: 0.000356 
Batch[13752] - loss: 0.000312 
Batch[13753] - loss: 0.000134 
Batch[13754] - loss: 0.000380 
Batch[13755] - loss: 0.000238 
Batch[13756] - loss: 0.000157 
Batch[13757] - loss: 0.000280 
Batch[13758] - loss: 0.000333 
Batch[13759] - loss: 0.000200 
Batch[13760] - loss: 0.000255 
Batch[13761] - loss: 0.000367 
Batch[13762] - loss: 0.000284 
Batch[13763] - loss: 0.000275 
Batch[13764] - loss: 0.000316 
Batch[13765] - loss: 0.000265 
Batch[13766] - loss: 0.000346 
Batch[13767] - loss: 0.000214 
Batch[13768] - loss: 0.000182 
Batch[13769] - loss: 0.000224 
Batch[13770] - loss: 0.000248 
Batch[13771] - loss: 0.000321 
Batch[13772] - loss: 0.000228 
Batch[13773] - loss: 0.000359 
Batch[13774] - loss: 0.000201 
Batch[13775] - loss: 0.000236 
Batch[13776] - loss: 0.000215 
Batch[13777] - loss: 0.000318 
Batch[13778] - loss: 0.000214 
Batch[13779] - loss: 0.000284 
Batch[13780] - loss: 0.000181 
Batch[13781] - loss: 0.000359 
Batch[13782] - loss: 0.000332 
Batch[13783] - loss: 0.000441 
Batch[13784] - loss: 0.000299 
Batch[13785] - loss: 0.000323 
Batch[13786] - loss: 0.000177 
Batch[13787] - loss: 0.000190 
Batch[13788] - loss: 0.000351 
Batch[13789] - loss: 0.000342 
Batch[13790] - loss: 0.000208 
Batch[13791] - loss: 0.000300 
Batch[13792] - loss: 0.000251 
Batch[13793] - loss: 0.000209 
Batch[13794] - loss: 0.000379 
Batch[13795] - loss: 0.000362 
Batch[13796] - loss: 0.000313 
Batch[13797] - loss: 0.000239 
Batch[13798] - loss: 0.000295 
Batch[13799] - loss: 0.000243 
Batch[13800] - loss: 0.000375 

Evaluation - loss: 0.000072 pearson: 0.5511 

early stop by 1500 steps.
Batch[13801] - loss: 0.000270 
Batch[13802] - loss: 0.000222 
Batch[13803] - loss: 0.000149 
Batch[13804] - loss: 0.000163 
Batch[13805] - loss: 0.000310 
Batch[13806] - loss: 0.000234 
Batch[13807] - loss: 0.000146 
Batch[13808] - loss: 0.000214 
Batch[13809] - loss: 0.000258 
Batch[13810] - loss: 0.000442 
Batch[13811] - loss: 0.000279 
Batch[13812] - loss: 0.000240 
Batch[13813] - loss: 0.000195 
Batch[13814] - loss: 0.000248 
Batch[13815] - loss: 0.000317 
Batch[13816] - loss: 0.000279 
Batch[13817] - loss: 0.000172 
Batch[13818] - loss: 0.000261 
Batch[13819] - loss: 0.000262 
Batch[13820] - loss: 0.000399 
Batch[13821] - loss: 0.000620 
Batch[13822] - loss: 0.000633 
Batch[13823] - loss: 0.000227 
Batch[13824] - loss: 0.000244 
Batch[13825] - loss: 0.000251 
Batch[13826] - loss: 0.000188 
Batch[13827] - loss: 0.000291 
Batch[13828] - loss: 0.000293 
Batch[13829] - loss: 0.000296 
Batch[13830] - loss: 0.000243 
Batch[13831] - loss: 0.000180 
Batch[13832] - loss: 0.000320 
Batch[13833] - loss: 0.000589 
Batch[13834] - loss: 0.000214 
Batch[13835] - loss: 0.000168 
Batch[13836] - loss: 0.000226 
Batch[13837] - loss: 0.000342 
Batch[13838] - loss: 0.000183 
Batch[13839] - loss: 0.000262 
Batch[13840] - loss: 0.000450 
Batch[13841] - loss: 0.000270 
Batch[13842] - loss: 0.000161 
Batch[13843] - loss: 0.000428 
Batch[13844] - loss: 0.000286 
Batch[13845] - loss: 0.000274 
Batch[13846] - loss: 0.000191 
Batch[13847] - loss: 0.000276 
Batch[13848] - loss: 0.000328 
Batch[13849] - loss: 0.000196 
Batch[13850] - loss: 0.000242 
Batch[13851] - loss: 0.000369 
Batch[13852] - loss: 0.000311 
Batch[13853] - loss: 0.000323 
Batch[13854] - loss: 0.000352 
Batch[13855] - loss: 0.000136 
Batch[13856] - loss: 0.000173 
Batch[13857] - loss: 0.000317 
Batch[13858] - loss: 0.000311 
Batch[13859] - loss: 0.000221 
Batch[13860] - loss: 0.000256 
Batch[13861] - loss: 0.000274 
Batch[13862] - loss: 0.000638 
Batch[13863] - loss: 0.000203 
Batch[13864] - loss: 0.000220 
Batch[13865] - loss: 0.000414 
Batch[13866] - loss: 0.000235 
Batch[13867] - loss: 0.000374 
Batch[13868] - loss: 0.000162 
Batch[13869] - loss: 0.000401 
Batch[13870] - loss: 0.000230 
Batch[13871] - loss: 0.000261 
Batch[13872] - loss: 0.000169 
Batch[13873] - loss: 0.000265 
Batch[13874] - loss: 0.000168 
Batch[13875] - loss: 0.000133 
Batch[13876] - loss: 0.000189 
Batch[13877] - loss: 0.000284 
Batch[13878] - loss: 0.000200 
Batch[13879] - loss: 0.000345 
Batch[13880] - loss: 0.000257 
Batch[13881] - loss: 0.000222 
Batch[13882] - loss: 0.000245 
Batch[13883] - loss: 0.000191 
Batch[13884] - loss: 0.000408 
Batch[13885] - loss: 0.000159 
Batch[13886] - loss: 0.000168 
Batch[13887] - loss: 0.000372 
Batch[13888] - loss: 0.000316 
Batch[13889] - loss: 0.000190 
Batch[13890] - loss: 0.000322 
Batch[13891] - loss: 0.000357 
Batch[13892] - loss: 0.000346 
Batch[13893] - loss: 0.000225 
Batch[13894] - loss: 0.000301 
Batch[13895] - loss: 0.000338 
Batch[13896] - loss: 0.000472 
Batch[13897] - loss: 0.000381 
Batch[13898] - loss: 0.000183 
Batch[13899] - loss: 0.000279 
Batch[13900] - loss: 0.000310 

Evaluation - loss: 0.000072 pearson: 0.5501 

early stop by 1500 steps.
Batch[13901] - loss: 0.000456 
Batch[13902] - loss: 0.000357 
Batch[13903] - loss: 0.000298 
Batch[13904] - loss: 0.000193 
Batch[13905] - loss: 0.000395 
Batch[13906] - loss: 0.000287 
Batch[13907] - loss: 0.000286 
Batch[13908] - loss: 0.000306 
Batch[13909] - loss: 0.000549 
Batch[13910] - loss: 0.000167 
Batch[13911] - loss: 0.000175 
Batch[13912] - loss: 0.000277 
Batch[13913] - loss: 0.000261 
Batch[13914] - loss: 0.000146 
Batch[13915] - loss: 0.000190 
Batch[13916] - loss: 0.000112 
Batch[13917] - loss: 0.000187 
Batch[13918] - loss: 0.000366 
Batch[13919] - loss: 0.000382 
Batch[13920] - loss: 0.000334 
Batch[13921] - loss: 0.000320 
Batch[13922] - loss: 0.000189 
Batch[13923] - loss: 0.000151 
Batch[13924] - loss: 0.000155 
Batch[13925] - loss: 0.000246 
Batch[13926] - loss: 0.000128 
Batch[13927] - loss: 0.000266 
Batch[13928] - loss: 0.000349 
Batch[13929] - loss: 0.000205 
Batch[13930] - loss: 0.000203 
Batch[13931] - loss: 0.000246 
Batch[13932] - loss: 0.000267 
Batch[13933] - loss: 0.000281 
Batch[13934] - loss: 0.000179 
Batch[13935] - loss: 0.000104 
Batch[13936] - loss: 0.000193 
Batch[13937] - loss: 0.000223 
Batch[13938] - loss: 0.000260 
Batch[13939] - loss: 0.000266 
Batch[13940] - loss: 0.000190 
Batch[13941] - loss: 0.000352 
Batch[13942] - loss: 0.000257 
Batch[13943] - loss: 0.000178 
Batch[13944] - loss: 0.000200 
Batch[13945] - loss: 0.000131 
Batch[13946] - loss: 0.000341 
Batch[13947] - loss: 0.000213 
Batch[13948] - loss: 0.000296 
Batch[13949] - loss: 0.000200 
Batch[13950] - loss: 0.000219 
Batch[13951] - loss: 0.000357 
Batch[13952] - loss: 0.000279 
Batch[13953] - loss: 0.000273 
Batch[13954] - loss: 0.000223 
Batch[13955] - loss: 0.000215 
Batch[13956] - loss: 0.000312 
Batch[13957] - loss: 0.000268 
Batch[13958] - loss: 0.000200 
Batch[13959] - loss: 0.000316 
Batch[13960] - loss: 0.000228 
Batch[13961] - loss: 0.000463 
Batch[13962] - loss: 0.000141 
Batch[13963] - loss: 0.000429 
Batch[13964] - loss: 0.000472 
Batch[13965] - loss: 0.000213 
Batch[13966] - loss: 0.000283 
Batch[13967] - loss: 0.000371 
Batch[13968] - loss: 0.000194 
Batch[13969] - loss: 0.000264 
Batch[13970] - loss: 0.000621 
Batch[13971] - loss: 0.000262 
Batch[13972] - loss: 0.000221 
Batch[13973] - loss: 0.000205 
Batch[13974] - loss: 0.000122 
Batch[13975] - loss: 0.000300 
Batch[13976] - loss: 0.000355 
Batch[13977] - loss: 0.000280 
Batch[13978] - loss: 0.000205 
Batch[13979] - loss: 0.000248 
Batch[13980] - loss: 0.000446 
Batch[13981] - loss: 0.000269 
Batch[13982] - loss: 0.000227 
Batch[13983] - loss: 0.000436 
Batch[13984] - loss: 0.000193 
Batch[13985] - loss: 0.000297 
Batch[13986] - loss: 0.000224 
Batch[13987] - loss: 0.000367 
Batch[13988] - loss: 0.000337 
Batch[13989] - loss: 0.000245 
Batch[13990] - loss: 0.000292 
Batch[13991] - loss: 0.000253 
Batch[13992] - loss: 0.000252 
Batch[13993] - loss: 0.000228 
Batch[13994] - loss: 0.000415 
Batch[13995] - loss: 0.000131 
Batch[13996] - loss: 0.000272 
Batch[13997] - loss: 0.000199 
Batch[13998] - loss: 0.000285 
Batch[13999] - loss: 0.000131 
Batch[14000] - loss: 0.000289 

Evaluation - loss: 0.000072 pearson: 0.5496 

early stop by 1500 steps.
Batch[14001] - loss: 0.000432 
Batch[14002] - loss: 0.000197 
Batch[14003] - loss: 0.000340 
Batch[14004] - loss: 0.000310 
Batch[14005] - loss: 0.000190 
Batch[14006] - loss: 0.000150 
Batch[14007] - loss: 0.000180 
Batch[14008] - loss: 0.000144 
Batch[14009] - loss: 0.000146 
Batch[14010] - loss: 0.000241 
Batch[14011] - loss: 0.000356 
Batch[14012] - loss: 0.000574 
Batch[14013] - loss: 0.000230 
Batch[14014] - loss: 0.000330 
Batch[14015] - loss: 0.000272 
Batch[14016] - loss: 0.000320 
Batch[14017] - loss: 0.000197 
Batch[14018] - loss: 0.000242 
Batch[14019] - loss: 0.000273 
Batch[14020] - loss: 0.000273 
Batch[14021] - loss: 0.000223 
Batch[14022] - loss: 0.000205 
Batch[14023] - loss: 0.000230 
Batch[14024] - loss: 0.000290 
Batch[14025] - loss: 0.000355 
Batch[14026] - loss: 0.000231 
Batch[14027] - loss: 0.000444 
Batch[14028] - loss: 0.000319 
Batch[14029] - loss: 0.000229 
Batch[14030] - loss: 0.000246 
Batch[14031] - loss: 0.000257 
Batch[14032] - loss: 0.000309 
Batch[14033] - loss: 0.000247 
Batch[14034] - loss: 0.000192 
Batch[14035] - loss: 0.000464 
Batch[14036] - loss: 0.000230 
Batch[14037] - loss: 0.000287 
Batch[14038] - loss: 0.000177 
Batch[14039] - loss: 0.000406 
Batch[14040] - loss: 0.000287 
Batch[14041] - loss: 0.000371 
Batch[14042] - loss: 0.000262 
Batch[14043] - loss: 0.000207 
Batch[14044] - loss: 0.000264 
Batch[14045] - loss: 0.000214 
Batch[14046] - loss: 0.000171 
Batch[14047] - loss: 0.000214 
Batch[14048] - loss: 0.000335 
Batch[14049] - loss: 0.000254 
Batch[14050] - loss: 0.000104 
Batch[14051] - loss: 0.000219 
Batch[14052] - loss: 0.000351 
Batch[14053] - loss: 0.000186 
Batch[14054] - loss: 0.000225 
Batch[14055] - loss: 0.000264 
Batch[14056] - loss: 0.000295 
Batch[14057] - loss: 0.000215 
Batch[14058] - loss: 0.000249 
Batch[14059] - loss: 0.000290 
Batch[14060] - loss: 0.000166 
Batch[14061] - loss: 0.000490 
Batch[14062] - loss: 0.000141 
Batch[14063] - loss: 0.000195 
Batch[14064] - loss: 0.000240 
Batch[14065] - loss: 0.000447 
Batch[14066] - loss: 0.000218 
Batch[14067] - loss: 0.000160 
Batch[14068] - loss: 0.000358 
Batch[14069] - loss: 0.000225 
Batch[14070] - loss: 0.000366 
Batch[14071] - loss: 0.000292 
Batch[14072] - loss: 0.000123 
Batch[14073] - loss: 0.000257 
Batch[14074] - loss: 0.000335 
Batch[14075] - loss: 0.000217 
Batch[14076] - loss: 0.000205 
Batch[14077] - loss: 0.000221 
Batch[14078] - loss: 0.000229 
Batch[14079] - loss: 0.000190 
Batch[14080] - loss: 0.000353 
Batch[14081] - loss: 0.000267 
Batch[14082] - loss: 0.000224 
Batch[14083] - loss: 0.000161 
Batch[14084] - loss: 0.000181 
Batch[14085] - loss: 0.000305 
Batch[14086] - loss: 0.000198 
Batch[14087] - loss: 0.000143 
Batch[14088] - loss: 0.000273 
Batch[14089] - loss: 0.000283 
Batch[14090] - loss: 0.000172 
Batch[14091] - loss: 0.000149 
Batch[14092] - loss: 0.000244 
Batch[14093] - loss: 0.000342 
Batch[14094] - loss: 0.000245 
Batch[14095] - loss: 0.000308 
Batch[14096] - loss: 0.000252 
Batch[14097] - loss: 0.000194 
Batch[14098] - loss: 0.000250 
Batch[14099] - loss: 0.000320 
Batch[14100] - loss: 0.000207 

Evaluation - loss: 0.000072 pearson: 0.5522 

early stop by 1500 steps.
Batch[14101] - loss: 0.000150 
Batch[14102] - loss: 0.000285 
Batch[14103] - loss: 0.000200 
Batch[14104] - loss: 0.000427 
Batch[14105] - loss: 0.000214 
Batch[14106] - loss: 0.000150 
Batch[14107] - loss: 0.000522 
Batch[14108] - loss: 0.000216 
Batch[14109] - loss: 0.000385 
Batch[14110] - loss: 0.000280 
Batch[14111] - loss: 0.000298 
Batch[14112] - loss: 0.000242 
Batch[14113] - loss: 0.000367 
Batch[14114] - loss: 0.000288 
Batch[14115] - loss: 0.000239 
Batch[14116] - loss: 0.000150 
Batch[14117] - loss: 0.000191 
Batch[14118] - loss: 0.000185 
Batch[14119] - loss: 0.000503 
Batch[14120] - loss: 0.000187 
Batch[14121] - loss: 0.000258 
Batch[14122] - loss: 0.000221 
Batch[14123] - loss: 0.000466 
Batch[14124] - loss: 0.000260 
Batch[14125] - loss: 0.000396 
Batch[14126] - loss: 0.000375 
Batch[14127] - loss: 0.000164 
Batch[14128] - loss: 0.000251 
Batch[14129] - loss: 0.000141 
Batch[14130] - loss: 0.000219 
Batch[14131] - loss: 0.000320 
Batch[14132] - loss: 0.000209 
Batch[14133] - loss: 0.000276 
Batch[14134] - loss: 0.000418 
Batch[14135] - loss: 0.000144 
Batch[14136] - loss: 0.000262 
Batch[14137] - loss: 0.000269 
Batch[14138] - loss: 0.000373 
Batch[14139] - loss: 0.000198 
Batch[14140] - loss: 0.000276 
Batch[14141] - loss: 0.000230 
Batch[14142] - loss: 0.000187 
Batch[14143] - loss: 0.000154 
Batch[14144] - loss: 0.000241 
Batch[14145] - loss: 0.000356 
Batch[14146] - loss: 0.000216 
Batch[14147] - loss: 0.000247 
Batch[14148] - loss: 0.000152 
Batch[14149] - loss: 0.000175 
Batch[14150] - loss: 0.000381 
Batch[14151] - loss: 0.000403 
Batch[14152] - loss: 0.000339 
Batch[14153] - loss: 0.000309 
Batch[14154] - loss: 0.000163 
Batch[14155] - loss: 0.000491 
Batch[14156] - loss: 0.000243 
Batch[14157] - loss: 0.000290 
Batch[14158] - loss: 0.000286 
Batch[14159] - loss: 0.000357 
Batch[14160] - loss: 0.000307 
Batch[14161] - loss: 0.000236 
Batch[14162] - loss: 0.000216 
Batch[14163] - loss: 0.000226 
Batch[14164] - loss: 0.000259 
Batch[14165] - loss: 0.000346 
Batch[14166] - loss: 0.000273 
Batch[14167] - loss: 0.000208 
Batch[14168] - loss: 0.000200 
Batch[14169] - loss: 0.000389 
Batch[14170] - loss: 0.000173 
Batch[14171] - loss: 0.000194 
Batch[14172] - loss: 0.000279 
Batch[14173] - loss: 0.000276 
Batch[14174] - loss: 0.000288 
Batch[14175] - loss: 0.000216 
Batch[14176] - loss: 0.000246 
Batch[14177] - loss: 0.000180 
Batch[14178] - loss: 0.000159 
Batch[14179] - loss: 0.000218 
Batch[14180] - loss: 0.000079 
Batch[14181] - loss: 0.000200 
Batch[14182] - loss: 0.000303 
Batch[14183] - loss: 0.000225 
Batch[14184] - loss: 0.000310 
Batch[14185] - loss: 0.000529 
Batch[14186] - loss: 0.000320 
Batch[14187] - loss: 0.000213 
Batch[14188] - loss: 0.000211 
Batch[14189] - loss: 0.000209 
Batch[14190] - loss: 0.000287 
Batch[14191] - loss: 0.000263 
Batch[14192] - loss: 0.000185 
Batch[14193] - loss: 0.000222 
Batch[14194] - loss: 0.000198 
Batch[14195] - loss: 0.000337 
Batch[14196] - loss: 0.000258 
Batch[14197] - loss: 0.000160 
Batch[14198] - loss: 0.000185 
Batch[14199] - loss: 0.000374 
Batch[14200] - loss: 0.000312 

Evaluation - loss: 0.000072 pearson: 0.5504 

early stop by 1500 steps.
Batch[14201] - loss: 0.000384 
Batch[14202] - loss: 0.000113 
Batch[14203] - loss: 0.000238 
Batch[14204] - loss: 0.000210 
Batch[14205] - loss: 0.000269 
Batch[14206] - loss: 0.000174 
Batch[14207] - loss: 0.000202 
Batch[14208] - loss: 0.000310 
Batch[14209] - loss: 0.000163 
Batch[14210] - loss: 0.000315 
Batch[14211] - loss: 0.000321 
Batch[14212] - loss: 0.000310 
Batch[14213] - loss: 0.000185 
Batch[14214] - loss: 0.000101 
Batch[14215] - loss: 0.000185 
Batch[14216] - loss: 0.000197 
Batch[14217] - loss: 0.000203 
Batch[14218] - loss: 0.000168 
Batch[14219] - loss: 0.000131 
Batch[14220] - loss: 0.000149 
Batch[14221] - loss: 0.000224 
Batch[14222] - loss: 0.000298 
Batch[14223] - loss: 0.000216 
Batch[14224] - loss: 0.000142 
Batch[14225] - loss: 0.000396 
Batch[14226] - loss: 0.000146 
Batch[14227] - loss: 0.000325 
Batch[14228] - loss: 0.000181 
Batch[14229] - loss: 0.000231 
Batch[14230] - loss: 0.000214 
Batch[14231] - loss: 0.000236 
Batch[14232] - loss: 0.000273 
Batch[14233] - loss: 0.000255 
Batch[14234] - loss: 0.000236 
Batch[14235] - loss: 0.000303 
Batch[14236] - loss: 0.000304 
Batch[14237] - loss: 0.000377 
Batch[14238] - loss: 0.000308 
Batch[14239] - loss: 0.000254 
Batch[14240] - loss: 0.000304 
Batch[14241] - loss: 0.000218 
Batch[14242] - loss: 0.000578 
Batch[14243] - loss: 0.000142 
Batch[14244] - loss: 0.000195 
Batch[14245] - loss: 0.000281 
Batch[14246] - loss: 0.000325 
Batch[14247] - loss: 0.000274 
Batch[14248] - loss: 0.000197 
Batch[14249] - loss: 0.000273 
Batch[14250] - loss: 0.000240 
Batch[14251] - loss: 0.000170 
Batch[14252] - loss: 0.000264 
Batch[14253] - loss: 0.000226 
Batch[14254] - loss: 0.000409 
Batch[14255] - loss: 0.000225 
Batch[14256] - loss: 0.000309 
Batch[14257] - loss: 0.000365 
Batch[14258] - loss: 0.000325 
Batch[14259] - loss: 0.000341 
Batch[14260] - loss: 0.000270 
Batch[14261] - loss: 0.000400 
Batch[14262] - loss: 0.000340 
Batch[14263] - loss: 0.000208 
Batch[14264] - loss: 0.000220 
Batch[14265] - loss: 0.000227 
Batch[14266] - loss: 0.000287 
Batch[14267] - loss: 0.000316 
Batch[14268] - loss: 0.000200 
Batch[14269] - loss: 0.000147 
Batch[14270] - loss: 0.000313 
Batch[14271] - loss: 0.000152 
Batch[14272] - loss: 0.000167 
Batch[14273] - loss: 0.000154 
Batch[14274] - loss: 0.000218 
Batch[14275] - loss: 0.000285 
Batch[14276] - loss: 0.000206 
Batch[14277] - loss: 0.000175 
Batch[14278] - loss: 0.000266 
Batch[14279] - loss: 0.000220 
Batch[14280] - loss: 0.000296 
Batch[14281] - loss: 0.000279 
Batch[14282] - loss: 0.000335 
Batch[14283] - loss: 0.000251 
Batch[14284] - loss: 0.000115 
Batch[14285] - loss: 0.000153 
Batch[14286] - loss: 0.000318 
Batch[14287] - loss: 0.000137 
Batch[14288] - loss: 0.000350 
Batch[14289] - loss: 0.000139 
Batch[14290] - loss: 0.000329 
Batch[14291] - loss: 0.000155 
Batch[14292] - loss: 0.000304 
Batch[14293] - loss: 0.000169 
Batch[14294] - loss: 0.000148 
Batch[14295] - loss: 0.000306 
Batch[14296] - loss: 0.000587 
Batch[14297] - loss: 0.000243 
Batch[14298] - loss: 0.000272 
Batch[14299] - loss: 0.000279 
Batch[14300] - loss: 0.000335 

Evaluation - loss: 0.000072 pearson: 0.5515 

early stop by 1500 steps.
Batch[14301] - loss: 0.000257 
Batch[14302] - loss: 0.000239 
Batch[14303] - loss: 0.000295 
Batch[14304] - loss: 0.000304 
Batch[14305] - loss: 0.000185 
Batch[14306] - loss: 0.000233 
Batch[14307] - loss: 0.000294 
Batch[14308] - loss: 0.000399 
Batch[14309] - loss: 0.000302 
Batch[14310] - loss: 0.000371 
Batch[14311] - loss: 0.000245 
Batch[14312] - loss: 0.000354 
Batch[14313] - loss: 0.000393 
Batch[14314] - loss: 0.000273 
Batch[14315] - loss: 0.000231 
Batch[14316] - loss: 0.000537 
Batch[14317] - loss: 0.000160 
Batch[14318] - loss: 0.000188 
Batch[14319] - loss: 0.000354 
Batch[14320] - loss: 0.000242 
Batch[14321] - loss: 0.000227 
Batch[14322] - loss: 0.000181 
Batch[14323] - loss: 0.000160 
Batch[14324] - loss: 0.000245 
Batch[14325] - loss: 0.000168 
Batch[14326] - loss: 0.000288 
Batch[14327] - loss: 0.000198 
Batch[14328] - loss: 0.000458 
Batch[14329] - loss: 0.000295 
Batch[14330] - loss: 0.000163 
Batch[14331] - loss: 0.000237 
Batch[14332] - loss: 0.000445 
Batch[14333] - loss: 0.000496 
Batch[14334] - loss: 0.000274 
Batch[14335] - loss: 0.000285 
Batch[14336] - loss: 0.000220 
Batch[14337] - loss: 0.000191 
Batch[14338] - loss: 0.000327 
Batch[14339] - loss: 0.000200 
Batch[14340] - loss: 0.000210 
Batch[14341] - loss: 0.000253 
Batch[14342] - loss: 0.000214 
Batch[14343] - loss: 0.000319 
Batch[14344] - loss: 0.000252 
Batch[14345] - loss: 0.000206 
Batch[14346] - loss: 0.000179 
Batch[14347] - loss: 0.000232 
Batch[14348] - loss: 0.000317 
Batch[14349] - loss: 0.000197 
Batch[14350] - loss: 0.000250 
Batch[14351] - loss: 0.000139 
Batch[14352] - loss: 0.000210 
Batch[14353] - loss: 0.000282 
Batch[14354] - loss: 0.000368 
Batch[14355] - loss: 0.000170 
Batch[14356] - loss: 0.000183 
Batch[14357] - loss: 0.000438 
Batch[14358] - loss: 0.000231 
Batch[14359] - loss: 0.000255 
Batch[14360] - loss: 0.000380 
Batch[14361] - loss: 0.000224 
Batch[14362] - loss: 0.000260 
Batch[14363] - loss: 0.000217 
Batch[14364] - loss: 0.000236 
Batch[14365] - loss: 0.000194 
Batch[14366] - loss: 0.000378 
Batch[14367] - loss: 0.000285 
Batch[14368] - loss: 0.000166 
Batch[14369] - loss: 0.000239 
Batch[14370] - loss: 0.000471 
Batch[14371] - loss: 0.000144 
Batch[14372] - loss: 0.000320 
Batch[14373] - loss: 0.000245 
Batch[14374] - loss: 0.000268 
Batch[14375] - loss: 0.000326 
Batch[14376] - loss: 0.000219 
Batch[14377] - loss: 0.000335 
Batch[14378] - loss: 0.000108 
Batch[14379] - loss: 0.000312 
Batch[14380] - loss: 0.000280 
Batch[14381] - loss: 0.000146 
Batch[14382] - loss: 0.000341 
Batch[14383] - loss: 0.000308 
Batch[14384] - loss: 0.000229 
Batch[14385] - loss: 0.000407 
Batch[14386] - loss: 0.000262 
Batch[14387] - loss: 0.000280 
Batch[14388] - loss: 0.000162 
Batch[14389] - loss: 0.000244 
Batch[14390] - loss: 0.000223 
Batch[14391] - loss: 0.000328 
Batch[14392] - loss: 0.000188 
Batch[14393] - loss: 0.000280 
Batch[14394] - loss: 0.000317 
Batch[14395] - loss: 0.000284 
Batch[14396] - loss: 0.000207 
Batch[14397] - loss: 0.000178 
Batch[14398] - loss: 0.000283 
Batch[14399] - loss: 0.000303 
Batch[14400] - loss: 0.000295 

Evaluation - loss: 0.000071 pearson: 0.5546 

early stop by 1500 steps.
Batch[14401] - loss: 0.000381 
Batch[14402] - loss: 0.000297 
Batch[14403] - loss: 0.000291 
Batch[14404] - loss: 0.000320 
Batch[14405] - loss: 0.000247 
Batch[14406] - loss: 0.000367 
Batch[14407] - loss: 0.000150 
Batch[14408] - loss: 0.000255 
Batch[14409] - loss: 0.000389 
Batch[14410] - loss: 0.000306 
Batch[14411] - loss: 0.000226 
Batch[14412] - loss: 0.000160 
Batch[14413] - loss: 0.000339 
Batch[14414] - loss: 0.000379 
Batch[14415] - loss: 0.000299 
Batch[14416] - loss: 0.000128 
Batch[14417] - loss: 0.000132 
Batch[14418] - loss: 0.000294 
Batch[14419] - loss: 0.000163 
Batch[14420] - loss: 0.000272 
Batch[14421] - loss: 0.000202 
Batch[14422] - loss: 0.000211 
Batch[14423] - loss: 0.000158 
Batch[14424] - loss: 0.000295 
Batch[14425] - loss: 0.000329 
Batch[14426] - loss: 0.000362 
Batch[14427] - loss: 0.000155 
Batch[14428] - loss: 0.000337 
Batch[14429] - loss: 0.000235 
Batch[14430] - loss: 0.000311 
Batch[14431] - loss: 0.000309 
Batch[14432] - loss: 0.000217 
Batch[14433] - loss: 0.000276 
Batch[14434] - loss: 0.000205 
Batch[14435] - loss: 0.000262 
Batch[14436] - loss: 0.000292 
Batch[14437] - loss: 0.000594 
Batch[14438] - loss: 0.000168 
Batch[14439] - loss: 0.000348 
Batch[14440] - loss: 0.000176 
Batch[14441] - loss: 0.000135 
Batch[14442] - loss: 0.000228 
Batch[14443] - loss: 0.000467 
Batch[14444] - loss: 0.000221 
Batch[14445] - loss: 0.000422 
Batch[14446] - loss: 0.000257 
Batch[14447] - loss: 0.000409 
Batch[14448] - loss: 0.000313 
Batch[14449] - loss: 0.000467 
Batch[14450] - loss: 0.000245 
Batch[14451] - loss: 0.000228 
Batch[14452] - loss: 0.000490 
Batch[14453] - loss: 0.000275 
Batch[14454] - loss: 0.000197 
Batch[14455] - loss: 0.000333 
Batch[14456] - loss: 0.000244 
Batch[14457] - loss: 0.000443 
Batch[14458] - loss: 0.000604 
Batch[14459] - loss: 0.000166 
Batch[14460] - loss: 0.000195 
Batch[14461] - loss: 0.000183 
Batch[14462] - loss: 0.000235 
Batch[14463] - loss: 0.000297 
Batch[14464] - loss: 0.000331 
Batch[14465] - loss: 0.000282 
Batch[14466] - loss: 0.000255 
Batch[14467] - loss: 0.000165 
Batch[14468] - loss: 0.000265 
Batch[14469] - loss: 0.000232 
Batch[14470] - loss: 0.000184 
Batch[14471] - loss: 0.000223 
Batch[14472] - loss: 0.000360 
Batch[14473] - loss: 0.000377 
Batch[14474] - loss: 0.000261 
Batch[14475] - loss: 0.000189 
Batch[14476] - loss: 0.000351 
Batch[14477] - loss: 0.000181 
Batch[14478] - loss: 0.000431 
Batch[14479] - loss: 0.000115 
Batch[14480] - loss: 0.000297 
Batch[14481] - loss: 0.000280 
Batch[14482] - loss: 0.000256 
Batch[14483] - loss: 0.000359 
Batch[14484] - loss: 0.000156 
Batch[14485] - loss: 0.000110 
Batch[14486] - loss: 0.000248 
Batch[14487] - loss: 0.000332 
Batch[14488] - loss: 0.000175 
Batch[14489] - loss: 0.000378 
Batch[14490] - loss: 0.000193 
Batch[14491] - loss: 0.000291 
Batch[14492] - loss: 0.000130 
Batch[14493] - loss: 0.000175 
Batch[14494] - loss: 0.000258 
Batch[14495] - loss: 0.000206 
Batch[14496] - loss: 0.000141 
Batch[14497] - loss: 0.000254 
Batch[14498] - loss: 0.000190 
Batch[14499] - loss: 0.000269 
Batch[14500] - loss: 0.000325 

Evaluation - loss: 0.000072 pearson: 0.5515 

early stop by 1500 steps.
Batch[14501] - loss: 0.000484 
Batch[14502] - loss: 0.000385 
Batch[14503] - loss: 0.000219 
Batch[14504] - loss: 0.000207 
Batch[14505] - loss: 0.000273 
Batch[14506] - loss: 0.000232 
Batch[14507] - loss: 0.000195 
Batch[14508] - loss: 0.000296 
Batch[14509] - loss: 0.000292 
Batch[14510] - loss: 0.000208 
Batch[14511] - loss: 0.000135 
Batch[14512] - loss: 0.000302 
Batch[14513] - loss: 0.000300 
Batch[14514] - loss: 0.000235 
Batch[14515] - loss: 0.000489 
Batch[14516] - loss: 0.000303 
Batch[14517] - loss: 0.000230 
Batch[14518] - loss: 0.000355 
Batch[14519] - loss: 0.000167 
Batch[14520] - loss: 0.000229 
Batch[14521] - loss: 0.000384 
Batch[14522] - loss: 0.000196 
Batch[14523] - loss: 0.000522 
Batch[14524] - loss: 0.000120 
Batch[14525] - loss: 0.000157 
Batch[14526] - loss: 0.000455 
Batch[14527] - loss: 0.000126 
Batch[14528] - loss: 0.000304 
Batch[14529] - loss: 0.000217 
Batch[14530] - loss: 0.000292 
Batch[14531] - loss: 0.000301 
Batch[14532] - loss: 0.000170 
Batch[14533] - loss: 0.000370 
Batch[14534] - loss: 0.000166 
Batch[14535] - loss: 0.000306 
Batch[14536] - loss: 0.000378 
Batch[14537] - loss: 0.000240 
Batch[14538] - loss: 0.000335 
Batch[14539] - loss: 0.000331 
Batch[14540] - loss: 0.000130 
Batch[14541] - loss: 0.000618 
Batch[14542] - loss: 0.000307 
Batch[14543] - loss: 0.000366 
Batch[14544] - loss: 0.000352 
Batch[14545] - loss: 0.000356 
Batch[14546] - loss: 0.000179 
Batch[14547] - loss: 0.000201 
Batch[14548] - loss: 0.000229 
Batch[14549] - loss: 0.000292 
Batch[14550] - loss: 0.000182 
Batch[14551] - loss: 0.000425 
Batch[14552] - loss: 0.000314 
Batch[14553] - loss: 0.000115 
Batch[14554] - loss: 0.000140 
Batch[14555] - loss: 0.000381 
Batch[14556] - loss: 0.000211 
Batch[14557] - loss: 0.000195 
Batch[14558] - loss: 0.000203 
Batch[14559] - loss: 0.000536 
Batch[14560] - loss: 0.000200 
Batch[14561] - loss: 0.000299 
Batch[14562] - loss: 0.000291 
Batch[14563] - loss: 0.000277 
Batch[14564] - loss: 0.000211 
Batch[14565] - loss: 0.000279 
Batch[14566] - loss: 0.000276 
Batch[14567] - loss: 0.000309 
Batch[14568] - loss: 0.000291 
Batch[14569] - loss: 0.000223 
Batch[14570] - loss: 0.000327 
Batch[14571] - loss: 0.000247 
Batch[14572] - loss: 0.000311 
Batch[14573] - loss: 0.000186 
Batch[14574] - loss: 0.000402 
Batch[14575] - loss: 0.000307 
Batch[14576] - loss: 0.000181 
Batch[14577] - loss: 0.000265 
Batch[14578] - loss: 0.000247 
Batch[14579] - loss: 0.000282 
Batch[14580] - loss: 0.000341 
Batch[14581] - loss: 0.000288 
Batch[14582] - loss: 0.000336 
Batch[14583] - loss: 0.000293 
Batch[14584] - loss: 0.000206 
Batch[14585] - loss: 0.000273 
Batch[14586] - loss: 0.000265 
Batch[14587] - loss: 0.000389 
Batch[14588] - loss: 0.000250 
Batch[14589] - loss: 0.000266 
Batch[14590] - loss: 0.000277 
Batch[14591] - loss: 0.000227 
Batch[14592] - loss: 0.000220 
Batch[14593] - loss: 0.000440 
Batch[14594] - loss: 0.000220 
Batch[14595] - loss: 0.000404 
Batch[14596] - loss: 0.000320 
Batch[14597] - loss: 0.000379 
Batch[14598] - loss: 0.000164 
Batch[14599] - loss: 0.000542 
Batch[14600] - loss: 0.000433 

Evaluation - loss: 0.000072 pearson: 0.5497 

early stop by 1500 steps.
Batch[14601] - loss: 0.000259 
Batch[14602] - loss: 0.000209 
Batch[14603] - loss: 0.000285 
Batch[14604] - loss: 0.000456 
Batch[14605] - loss: 0.000221 
Batch[14606] - loss: 0.000421 
Batch[14607] - loss: 0.000261 
Batch[14608] - loss: 0.000297 
Batch[14609] - loss: 0.000239 
Batch[14610] - loss: 0.000156 
Batch[14611] - loss: 0.000177 
Batch[14612] - loss: 0.000192 
Batch[14613] - loss: 0.000375 
Batch[14614] - loss: 0.000349 
Batch[14615] - loss: 0.000234 
Batch[14616] - loss: 0.000229 
Batch[14617] - loss: 0.000235 
Batch[14618] - loss: 0.000199 
Batch[14619] - loss: 0.000222 
Batch[14620] - loss: 0.000140 
Batch[14621] - loss: 0.000300 
Batch[14622] - loss: 0.000180 
Batch[14623] - loss: 0.000335 
Batch[14624] - loss: 0.000327 
Batch[14625] - loss: 0.000200 
Batch[14626] - loss: 0.000206 
Batch[14627] - loss: 0.000255 
Batch[14628] - loss: 0.000833 
Batch[14629] - loss: 0.000460 
Batch[14630] - loss: 0.000309 
Batch[14631] - loss: 0.000196 
Batch[14632] - loss: 0.000292 
Batch[14633] - loss: 0.000202 
Batch[14634] - loss: 0.000348 
Batch[14635] - loss: 0.000270 
Batch[14636] - loss: 0.000301 
Batch[14637] - loss: 0.000273 
Batch[14638] - loss: 0.000303 
Batch[14639] - loss: 0.000434 
Batch[14640] - loss: 0.000249 
Batch[14641] - loss: 0.000215 
Batch[14642] - loss: 0.000233 
Batch[14643] - loss: 0.000231 
Batch[14644] - loss: 0.000542 
Batch[14645] - loss: 0.000317 
Batch[14646] - loss: 0.000168 
Batch[14647] - loss: 0.000495 
Batch[14648] - loss: 0.000449 
Batch[14649] - loss: 0.000291 
Batch[14650] - loss: 0.000202 
Batch[14651] - loss: 0.000410 
Batch[14652] - loss: 0.000426 
Batch[14653] - loss: 0.000177 
Batch[14654] - loss: 0.000505 
Batch[14655] - loss: 0.000447 
Batch[14656] - loss: 0.000231 
Batch[14657] - loss: 0.000158 
Batch[14658] - loss: 0.000242 
Batch[14659] - loss: 0.000200 
Batch[14660] - loss: 0.000330 
Batch[14661] - loss: 0.000172 
Batch[14662] - loss: 0.000332 
Batch[14663] - loss: 0.000125 
Batch[14664] - loss: 0.000169 
Batch[14665] - loss: 0.000386 
Batch[14666] - loss: 0.000473 
Batch[14667] - loss: 0.000204 
Batch[14668] - loss: 0.000342 
Batch[14669] - loss: 0.000347 
Batch[14670] - loss: 0.000206 
Batch[14671] - loss: 0.000301 
Batch[14672] - loss: 0.000525 
Batch[14673] - loss: 0.000180 
Batch[14674] - loss: 0.000258 
Batch[14675] - loss: 0.000345 
Batch[14676] - loss: 0.000305 
Batch[14677] - loss: 0.000393 
Batch[14678] - loss: 0.000141 
Batch[14679] - loss: 0.000166 
Batch[14680] - loss: 0.000295 
Batch[14681] - loss: 0.000196 
Batch[14682] - loss: 0.000190 
Batch[14683] - loss: 0.000178 
Batch[14684] - loss: 0.000418 
Batch[14685] - loss: 0.000335 
Batch[14686] - loss: 0.000343 
Batch[14687] - loss: 0.000232 
Batch[14688] - loss: 0.000398 
Batch[14689] - loss: 0.000298 
Batch[14690] - loss: 0.000410 
Batch[14691] - loss: 0.000198 
Batch[14692] - loss: 0.000146 
Batch[14693] - loss: 0.000180 
Batch[14694] - loss: 0.000198 
Batch[14695] - loss: 0.000220 
Batch[14696] - loss: 0.000287 
Batch[14697] - loss: 0.000375 
Batch[14698] - loss: 0.000166 
Batch[14699] - loss: 0.000249 
Batch[14700] - loss: 0.000175 

Evaluation - loss: 0.000072 pearson: 0.5521 

early stop by 1500 steps.
Batch[14701] - loss: 0.000257 
Batch[14702] - loss: 0.000250 
Batch[14703] - loss: 0.000304 
Batch[14704] - loss: 0.000440 
Batch[14705] - loss: 0.000213 
Batch[14706] - loss: 0.000372 
Batch[14707] - loss: 0.000267 
Batch[14708] - loss: 0.000187 
Batch[14709] - loss: 0.000236 
Batch[14710] - loss: 0.000367 
Batch[14711] - loss: 0.000330 
Batch[14712] - loss: 0.000236 
Batch[14713] - loss: 0.000240 
Batch[14714] - loss: 0.000347 
Batch[14715] - loss: 0.000366 
Batch[14716] - loss: 0.000253 
Batch[14717] - loss: 0.000134 
Batch[14718] - loss: 0.000224 
Batch[14719] - loss: 0.000266 
Batch[14720] - loss: 0.000309 
Batch[14721] - loss: 0.000256 
Batch[14722] - loss: 0.000239 
Batch[14723] - loss: 0.000198 
Batch[14724] - loss: 0.000144 
Batch[14725] - loss: 0.000522 
Batch[14726] - loss: 0.000363 
Batch[14727] - loss: 0.000275 
Batch[14728] - loss: 0.000261 
Batch[14729] - loss: 0.000286 
Batch[14730] - loss: 0.000233 
Batch[14731] - loss: 0.000260 
Batch[14732] - loss: 0.000407 
Batch[14733] - loss: 0.000211 
Batch[14734] - loss: 0.000182 
Batch[14735] - loss: 0.000325 
Batch[14736] - loss: 0.000521 
Batch[14737] - loss: 0.000332 
Batch[14738] - loss: 0.000133 
Batch[14739] - loss: 0.000275 
Batch[14740] - loss: 0.000212 
Batch[14741] - loss: 0.000132 
Batch[14742] - loss: 0.000221 
Batch[14743] - loss: 0.000296 
Batch[14744] - loss: 0.000147 
Batch[14745] - loss: 0.000452 
Batch[14746] - loss: 0.000325 
Batch[14747] - loss: 0.000248 
Batch[14748] - loss: 0.000524 
Batch[14749] - loss: 0.000279 
Batch[14750] - loss: 0.000230 
Batch[14751] - loss: 0.000275 
Batch[14752] - loss: 0.000345 
Batch[14753] - loss: 0.000350 
Batch[14754] - loss: 0.000201 
Batch[14755] - loss: 0.000346 
Batch[14756] - loss: 0.000232 
Batch[14757] - loss: 0.000414 
Batch[14758] - loss: 0.000316 
Batch[14759] - loss: 0.000193 
Batch[14760] - loss: 0.000297 
Batch[14761] - loss: 0.000206 
Batch[14762] - loss: 0.000180 
Batch[14763] - loss: 0.000517 
Batch[14764] - loss: 0.000224 
Batch[14765] - loss: 0.000335 
Batch[14766] - loss: 0.000409 
Batch[14767] - loss: 0.000275 
Batch[14768] - loss: 0.000168 
Batch[14769] - loss: 0.000201 
Batch[14770] - loss: 0.000172 
Batch[14771] - loss: 0.000241 
Batch[14772] - loss: 0.000250 
Batch[14773] - loss: 0.000219 
Batch[14774] - loss: 0.000245 
Batch[14775] - loss: 0.000338 
Batch[14776] - loss: 0.000204 
Batch[14777] - loss: 0.000393 
Batch[14778] - loss: 0.000265 
Batch[14779] - loss: 0.000282 
Batch[14780] - loss: 0.000246 
Batch[14781] - loss: 0.000201 
Batch[14782] - loss: 0.000236 
Batch[14783] - loss: 0.000361 
Batch[14784] - loss: 0.000353 
Batch[14785] - loss: 0.000185 
Batch[14786] - loss: 0.000356 
Batch[14787] - loss: 0.000256 
Batch[14788] - loss: 0.000274 
Batch[14789] - loss: 0.000386 
Batch[14790] - loss: 0.000286 
Batch[14791] - loss: 0.000229 
Batch[14792] - loss: 0.000172 
Batch[14793] - loss: 0.000289 
Batch[14794] - loss: 0.000242 
Batch[14795] - loss: 0.000430 
Batch[14796] - loss: 0.000346 
Batch[14797] - loss: 0.000299 
Batch[14798] - loss: 0.000305 
Batch[14799] - loss: 0.000156 
Batch[14800] - loss: 0.000189 

Evaluation - loss: 0.000072 pearson: 0.5499 

early stop by 1500 steps.
Batch[14801] - loss: 0.000366 
Batch[14802] - loss: 0.000309 
Batch[14803] - loss: 0.000185 
Batch[14804] - loss: 0.000209 
Batch[14805] - loss: 0.000245 
Batch[14806] - loss: 0.000240 
Batch[14807] - loss: 0.000294 
Batch[14808] - loss: 0.000220 
Batch[14809] - loss: 0.000318 
Batch[14810] - loss: 0.000210 
Batch[14811] - loss: 0.000215 
Batch[14812] - loss: 0.000230 
Batch[14813] - loss: 0.000357 
Batch[14814] - loss: 0.000302 
Batch[14815] - loss: 0.000266 
Batch[14816] - loss: 0.000181 
Batch[14817] - loss: 0.000222 
Batch[14818] - loss: 0.000174 
Batch[14819] - loss: 0.000376 
Batch[14820] - loss: 0.000319 
Batch[14821] - loss: 0.000468 
Batch[14822] - loss: 0.000144 
Batch[14823] - loss: 0.000286 
Batch[14824] - loss: 0.000152 
Batch[14825] - loss: 0.000179 
Batch[14826] - loss: 0.000333 
Batch[14827] - loss: 0.000249 
Batch[14828] - loss: 0.000252 
Batch[14829] - loss: 0.000262 
Batch[14830] - loss: 0.000237 
Batch[14831] - loss: 0.000272 
Batch[14832] - loss: 0.000184 
Batch[14833] - loss: 0.000185 
Batch[14834] - loss: 0.000386 
Batch[14835] - loss: 0.000119 
Batch[14836] - loss: 0.000222 
Batch[14837] - loss: 0.000124 
Batch[14838] - loss: 0.000186 
Batch[14839] - loss: 0.000257 
Batch[14840] - loss: 0.000134 
Batch[14841] - loss: 0.000138 
Batch[14842] - loss: 0.000268 
Batch[14843] - loss: 0.000192 
Batch[14844] - loss: 0.000185 
Batch[14845] - loss: 0.000143 
Batch[14846] - loss: 0.000183 
Batch[14847] - loss: 0.000328 
Batch[14848] - loss: 0.000213 
Batch[14849] - loss: 0.000254 
Batch[14850] - loss: 0.000185 
Batch[14851] - loss: 0.000263 
Batch[14852] - loss: 0.000219 
Batch[14853] - loss: 0.000220 
Batch[14854] - loss: 0.000337 
Batch[14855] - loss: 0.000262 
Batch[14856] - loss: 0.000243 
Batch[14857] - loss: 0.000228 
Batch[14858] - loss: 0.000591 
Batch[14859] - loss: 0.000311 
Batch[14860] - loss: 0.000355 
Batch[14861] - loss: 0.000417 
Batch[14862] - loss: 0.000226 
Batch[14863] - loss: 0.000173 
Batch[14864] - loss: 0.000298 
Batch[14865] - loss: 0.000164 
Batch[14866] - loss: 0.000255 
Batch[14867] - loss: 0.000274 
Batch[14868] - loss: 0.000257 
Batch[14869] - loss: 0.000309 
Batch[14870] - loss: 0.000221 
Batch[14871] - loss: 0.000419 
Batch[14872] - loss: 0.000235 
Batch[14873] - loss: 0.000203 
Batch[14874] - loss: 0.000245 
Batch[14875] - loss: 0.000220 
Batch[14876] - loss: 0.000288 
Batch[14877] - loss: 0.000264 
Batch[14878] - loss: 0.000254 
Batch[14879] - loss: 0.000250 
Batch[14880] - loss: 0.000259 
Batch[14881] - loss: 0.000250 
Batch[14882] - loss: 0.000129 
Batch[14883] - loss: 0.000225 
Batch[14884] - loss: 0.000249 
Batch[14885] - loss: 0.000249 
Batch[14886] - loss: 0.000445 
Batch[14887] - loss: 0.000162 
Batch[14888] - loss: 0.000301 
Batch[14889] - loss: 0.000408 
Batch[14890] - loss: 0.000224 
Batch[14891] - loss: 0.000291 
Batch[14892] - loss: 0.000184 
Batch[14893] - loss: 0.000137 
Batch[14894] - loss: 0.000360 
Batch[14895] - loss: 0.000219 
Batch[14896] - loss: 0.000393 
Batch[14897] - loss: 0.000380 
Batch[14898] - loss: 0.000105 
Batch[14899] - loss: 0.000293 
Batch[14900] - loss: 0.000215 

Evaluation - loss: 0.000072 pearson: 0.5493 

early stop by 1500 steps.
Batch[14901] - loss: 0.000158 
Batch[14902] - loss: 0.000264 
Batch[14903] - loss: 0.000208 
Batch[14904] - loss: 0.000307 
Batch[14905] - loss: 0.000182 
Batch[14906] - loss: 0.000160 
Batch[14907] - loss: 0.000256 
Batch[14908] - loss: 0.000401 
Batch[14909] - loss: 0.000306 
Batch[14910] - loss: 0.000182 
Batch[14911] - loss: 0.000204 
Batch[14912] - loss: 0.000394 
Batch[14913] - loss: 0.000275 
Batch[14914] - loss: 0.000244 
Batch[14915] - loss: 0.000294 
Batch[14916] - loss: 0.000215 
Batch[14917] - loss: 0.000288 
Batch[14918] - loss: 0.000519 
Batch[14919] - loss: 0.000272 
Batch[14920] - loss: 0.000431 
Batch[14921] - loss: 0.000361 
Batch[14922] - loss: 0.000323 
Batch[14923] - loss: 0.000150 
Batch[14924] - loss: 0.000202 
Batch[14925] - loss: 0.000237 
Batch[14926] - loss: 0.000188 
Batch[14927] - loss: 0.000287 
Batch[14928] - loss: 0.000217 
Batch[14929] - loss: 0.000193 
Batch[14930] - loss: 0.000143 
Batch[14931] - loss: 0.000213 
Batch[14932] - loss: 0.000294 
Batch[14933] - loss: 0.000320 
Batch[14934] - loss: 0.000401 
Batch[14935] - loss: 0.000223 
Batch[14936] - loss: 0.000161 
Batch[14937] - loss: 0.000178 
Batch[14938] - loss: 0.000359 
Batch[14939] - loss: 0.000180 
Batch[14940] - loss: 0.000204 
Batch[14941] - loss: 0.000230 
Batch[14942] - loss: 0.000336 
Batch[14943] - loss: 0.000244 
Batch[14944] - loss: 0.000181 
Batch[14945] - loss: 0.000347 
Batch[14946] - loss: 0.000625 
Batch[14947] - loss: 0.000337 
Batch[14948] - loss: 0.000244 
Batch[14949] - loss: 0.000250 
Batch[14950] - loss: 0.000191 
Batch[14951] - loss: 0.000224 
Batch[14952] - loss: 0.000452 
Batch[14953] - loss: 0.000178 
Batch[14954] - loss: 0.000197 
Batch[14955] - loss: 0.000187 
Batch[14956] - loss: 0.000133 
Batch[14957] - loss: 0.000201 
Batch[14958] - loss: 0.000202 
Batch[14959] - loss: 0.000260 
Batch[14960] - loss: 0.000257 
Batch[14961] - loss: 0.000217 
Batch[14962] - loss: 0.000204 
Batch[14963] - loss: 0.000176 
Batch[14964] - loss: 0.000166 
Batch[14965] - loss: 0.000183 
Batch[14966] - loss: 0.000434 
Batch[14967] - loss: 0.000135 
Batch[14968] - loss: 0.000129 
Batch[14969] - loss: 0.000268 
Batch[14970] - loss: 0.000411 
Batch[14971] - loss: 0.000263 
Batch[14972] - loss: 0.000150 
Batch[14973] - loss: 0.000290 
Batch[14974] - loss: 0.000330 
Batch[14975] - loss: 0.000191 
Batch[14976] - loss: 0.000238 
Batch[14977] - loss: 0.000198 
Batch[14978] - loss: 0.000210 
Batch[14979] - loss: 0.000353 
Batch[14980] - loss: 0.000171 
Batch[14981] - loss: 0.000298 
Batch[14982] - loss: 0.000206 
Batch[14983] - loss: 0.000380 
Batch[14984] - loss: 0.000327 
Batch[14985] - loss: 0.000252 
Batch[14986] - loss: 0.000200 
Batch[14987] - loss: 0.000339 
Batch[14988] - loss: 0.000433 
Batch[14989] - loss: 0.000164 
Batch[14990] - loss: 0.000269 
Batch[14991] - loss: 0.000300 
Batch[14992] - loss: 0.000339 
Batch[14993] - loss: 0.000235 
Batch[14994] - loss: 0.000230 
Batch[14995] - loss: 0.000169 
Batch[14996] - loss: 0.000230 
Batch[14997] - loss: 0.000205 
Batch[14998] - loss: 0.000229 
Batch[14999] - loss: 0.000289 
Batch[15000] - loss: 0.000263 

Evaluation - loss: 0.000072 pearson: 0.5506 

early stop by 1500 steps.
Batch[15001] - loss: 0.000186 
Batch[15002] - loss: 0.000143 
Batch[15003] - loss: 0.000415 
Batch[15004] - loss: 0.000339 
Batch[15005] - loss: 0.000302 
Batch[15006] - loss: 0.000262 
Batch[15007] - loss: 0.000172 
Batch[15008] - loss: 0.000225 
Batch[15009] - loss: 0.000452 
Batch[15010] - loss: 0.000570 
Batch[15011] - loss: 0.000232 
Batch[15012] - loss: 0.000317 
Batch[15013] - loss: 0.000262 
Batch[15014] - loss: 0.000322 
Batch[15015] - loss: 0.000234 
Batch[15016] - loss: 0.000231 
Batch[15017] - loss: 0.000292 
Batch[15018] - loss: 0.000287 
Batch[15019] - loss: 0.000352 
Batch[15020] - loss: 0.000155 
Batch[15021] - loss: 0.000208 
Batch[15022] - loss: 0.000309 
Batch[15023] - loss: 0.000271 
Batch[15024] - loss: 0.000209 
Batch[15025] - loss: 0.000295 
Batch[15026] - loss: 0.000388 
Batch[15027] - loss: 0.000262 
Batch[15028] - loss: 0.000246 
Batch[15029] - loss: 0.000365 
Batch[15030] - loss: 0.000165 
Batch[15031] - loss: 0.000304 
Batch[15032] - loss: 0.000114 
Batch[15033] - loss: 0.000260 
Batch[15034] - loss: 0.000226 
Batch[15035] - loss: 0.000189 
Batch[15036] - loss: 0.000218 
Batch[15037] - loss: 0.000317 
Batch[15038] - loss: 0.000169 
Batch[15039] - loss: 0.000125 
Batch[15040] - loss: 0.000120 
Batch[15041] - loss: 0.000224 
Batch[15042] - loss: 0.000273 
Batch[15043] - loss: 0.000348 
Batch[15044] - loss: 0.000194 
Batch[15045] - loss: 0.000254 
Batch[15046] - loss: 0.000130 
Batch[15047] - loss: 0.000293 
Batch[15048] - loss: 0.000172 
Batch[15049] - loss: 0.000265 
Batch[15050] - loss: 0.000394 
Batch[15051] - loss: 0.000188 
Batch[15052] - loss: 0.000284 
Batch[15053] - loss: 0.000394 
Batch[15054] - loss: 0.000179 
Batch[15055] - loss: 0.000267 
Batch[15056] - loss: 0.000241 
Batch[15057] - loss: 0.000299 
Batch[15058] - loss: 0.000312 
Batch[15059] - loss: 0.000200 
Batch[15060] - loss: 0.000197 
Batch[15061] - loss: 0.000330 
Batch[15062] - loss: 0.000226 
Batch[15063] - loss: 0.000154 
Batch[15064] - loss: 0.000258 
Batch[15065] - loss: 0.000322 
Batch[15066] - loss: 0.000217 
Batch[15067] - loss: 0.000195 
Batch[15068] - loss: 0.000226 
Batch[15069] - loss: 0.000239 
Batch[15070] - loss: 0.000192 
Batch[15071] - loss: 0.000252 
Batch[15072] - loss: 0.000340 
Batch[15073] - loss: 0.000301 
Batch[15074] - loss: 0.000181 
Batch[15075] - loss: 0.000373 
Batch[15076] - loss: 0.000162 
Batch[15077] - loss: 0.000239 
Batch[15078] - loss: 0.000327 
Batch[15079] - loss: 0.000194 
Batch[15080] - loss: 0.000327 
Batch[15081] - loss: 0.000243 
Batch[15082] - loss: 0.000255 
Batch[15083] - loss: 0.000312 
Batch[15084] - loss: 0.000276 
Batch[15085] - loss: 0.000311 
Batch[15086] - loss: 0.000398 
Batch[15087] - loss: 0.000211 
Batch[15088] - loss: 0.000215 
Batch[15089] - loss: 0.000166 
Batch[15090] - loss: 0.000221 
Batch[15091] - loss: 0.000177 
Batch[15092] - loss: 0.000205 
Batch[15093] - loss: 0.000244 
Batch[15094] - loss: 0.000262 
Batch[15095] - loss: 0.000195 
Batch[15096] - loss: 0.000268 
Batch[15097] - loss: 0.000145 
Batch[15098] - loss: 0.000305 
Batch[15099] - loss: 0.000292 
Batch[15100] - loss: 0.000167 

Evaluation - loss: 0.000072 pearson: 0.5512 

early stop by 1500 steps.
Batch[15101] - loss: 0.000225 
Batch[15102] - loss: 0.000164 
Batch[15103] - loss: 0.000278 
Batch[15104] - loss: 0.000367 
Batch[15105] - loss: 0.000135 
Batch[15106] - loss: 0.000792 
Batch[15107] - loss: 0.000343 
Batch[15108] - loss: 0.000337 
Batch[15109] - loss: 0.000138 
Batch[15110] - loss: 0.000206 
Batch[15111] - loss: 0.000127 
Batch[15112] - loss: 0.000160 
Batch[15113] - loss: 0.000288 
Batch[15114] - loss: 0.000277 
Batch[15115] - loss: 0.000149 
Batch[15116] - loss: 0.000214 
Batch[15117] - loss: 0.000484 
Batch[15118] - loss: 0.000467 
Batch[15119] - loss: 0.000237 
Batch[15120] - loss: 0.000220 
Batch[15121] - loss: 0.000220 
Batch[15122] - loss: 0.000236 
Batch[15123] - loss: 0.000138 
Batch[15124] - loss: 0.000231 
Batch[15125] - loss: 0.000140 
Batch[15126] - loss: 0.000486 
Batch[15127] - loss: 0.000189 
Batch[15128] - loss: 0.000268 
Batch[15129] - loss: 0.000179 
Batch[15130] - loss: 0.000255 
Batch[15131] - loss: 0.000204 
Batch[15132] - loss: 0.000310 
Batch[15133] - loss: 0.000241 
Batch[15134] - loss: 0.000491 
Batch[15135] - loss: 0.000176 
Batch[15136] - loss: 0.000226 
Batch[15137] - loss: 0.000277 
Batch[15138] - loss: 0.000361 
Batch[15139] - loss: 0.000154 
Batch[15140] - loss: 0.000404 
Batch[15141] - loss: 0.000219 
Batch[15142] - loss: 0.000266 
Batch[15143] - loss: 0.000179 
Batch[15144] - loss: 0.000198 
Batch[15145] - loss: 0.000308 
Batch[15146] - loss: 0.000250 
Batch[15147] - loss: 0.000157 
Batch[15148] - loss: 0.000431 
Batch[15149] - loss: 0.000351 
Batch[15150] - loss: 0.000152 
Batch[15151] - loss: 0.000197 
Batch[15152] - loss: 0.000083 
Batch[15153] - loss: 0.000241 
Batch[15154] - loss: 0.000116 
Batch[15155] - loss: 0.000173 
Batch[15156] - loss: 0.000142 
Batch[15157] - loss: 0.000198 
Batch[15158] - loss: 0.000254 
Batch[15159] - loss: 0.000187 
Batch[15160] - loss: 0.000236 
Batch[15161] - loss: 0.000244 
Batch[15162] - loss: 0.000281 
Batch[15163] - loss: 0.000546 
Batch[15164] - loss: 0.000235 
Batch[15165] - loss: 0.000234 
Batch[15166] - loss: 0.000315 
Batch[15167] - loss: 0.000137 
Batch[15168] - loss: 0.000176 
Batch[15169] - loss: 0.000404 
Batch[15170] - loss: 0.000336 
Batch[15171] - loss: 0.000204 
Batch[15172] - loss: 0.000114 
Batch[15173] - loss: 0.000202 
Batch[15174] - loss: 0.000244 
Batch[15175] - loss: 0.000481 
Batch[15176] - loss: 0.000269 
Batch[15177] - loss: 0.000301 
Batch[15178] - loss: 0.000292 
Batch[15179] - loss: 0.000298 
Batch[15180] - loss: 0.000125 
Batch[15181] - loss: 0.000342 
Batch[15182] - loss: 0.000233 
Batch[15183] - loss: 0.000136 
Batch[15184] - loss: 0.000186 
Batch[15185] - loss: 0.000315 
Batch[15186] - loss: 0.000205 
Batch[15187] - loss: 0.000212 
Batch[15188] - loss: 0.000254 
Batch[15189] - loss: 0.000250 
Batch[15190] - loss: 0.000270 
Batch[15191] - loss: 0.000150 
Batch[15192] - loss: 0.000194 
Batch[15193] - loss: 0.000251 
Batch[15194] - loss: 0.000256 
Batch[15195] - loss: 0.000235 
Batch[15196] - loss: 0.000193 
Batch[15197] - loss: 0.000454 
Batch[15198] - loss: 0.000220 
Batch[15199] - loss: 0.000284 
Batch[15200] - loss: 0.000285 

Evaluation - loss: 0.000072 pearson: 0.5514 

early stop by 1500 steps.
Batch[15201] - loss: 0.000436 
Batch[15202] - loss: 0.000181 
Batch[15203] - loss: 0.000181 
Batch[15204] - loss: 0.000259 
Batch[15205] - loss: 0.000273 
Batch[15206] - loss: 0.000209 
Batch[15207] - loss: 0.000362 
Batch[15208] - loss: 0.000278 
Batch[15209] - loss: 0.000229 
Batch[15210] - loss: 0.000250 
Batch[15211] - loss: 0.000100 
Batch[15212] - loss: 0.000442 
Batch[15213] - loss: 0.000212 
Batch[15214] - loss: 0.000378 
Batch[15215] - loss: 0.000350 
Batch[15216] - loss: 0.000202 
Batch[15217] - loss: 0.000212 
Batch[15218] - loss: 0.000245 
Batch[15219] - loss: 0.000307 
Batch[15220] - loss: 0.000360 
Batch[15221] - loss: 0.000348 
Batch[15222] - loss: 0.000448 
Batch[15223] - loss: 0.000293 
Batch[15224] - loss: 0.000373 
Batch[15225] - loss: 0.000208 
Batch[15226] - loss: 0.000376 
Batch[15227] - loss: 0.000388 
Batch[15228] - loss: 0.000229 
Batch[15229] - loss: 0.000284 
Batch[15230] - loss: 0.000170 
Batch[15231] - loss: 0.000196 
Batch[15232] - loss: 0.000259 
Batch[15233] - loss: 0.000203 
Batch[15234] - loss: 0.000274 
Batch[15235] - loss: 0.000164 
Batch[15236] - loss: 0.000233 
Batch[15237] - loss: 0.000210 
Batch[15238] - loss: 0.000164 
Batch[15239] - loss: 0.000209 
Batch[15240] - loss: 0.000246 
Batch[15241] - loss: 0.000245 
Batch[15242] - loss: 0.000303 
Batch[15243] - loss: 0.000196 
Batch[15244] - loss: 0.000134 
Batch[15245] - loss: 0.000149 
Batch[15246] - loss: 0.000179 
Batch[15247] - loss: 0.000241 
Batch[15248] - loss: 0.000152 
Batch[15249] - loss: 0.000152 
Batch[15250] - loss: 0.000258 
Batch[15251] - loss: 0.000128 
Batch[15252] - loss: 0.000240 
Batch[15253] - loss: 0.000203 
Batch[15254] - loss: 0.000224 
Batch[15255] - loss: 0.000300 
Batch[15256] - loss: 0.000230 
Batch[15257] - loss: 0.000366 
Batch[15258] - loss: 0.000099 
Batch[15259] - loss: 0.000300 
Batch[15260] - loss: 0.000251 
Batch[15261] - loss: 0.000466 
Batch[15262] - loss: 0.000193 
Batch[15263] - loss: 0.000240 
Batch[15264] - loss: 0.000168 
Batch[15265] - loss: 0.000239 
Batch[15266] - loss: 0.000179 
Batch[15267] - loss: 0.000429 
Batch[15268] - loss: 0.000199 
Batch[15269] - loss: 0.000199 
Batch[15270] - loss: 0.000306 
Batch[15271] - loss: 0.000622 
Batch[15272] - loss: 0.000131 
Batch[15273] - loss: 0.000178 
Batch[15274] - loss: 0.000256 
Batch[15275] - loss: 0.000227 
Batch[15276] - loss: 0.000348 
Batch[15277] - loss: 0.000247 
Batch[15278] - loss: 0.000363 
Batch[15279] - loss: 0.000256 
Batch[15280] - loss: 0.000289 
Batch[15281] - loss: 0.000233 
Batch[15282] - loss: 0.000167 
Batch[15283] - loss: 0.000223 
Batch[15284] - loss: 0.000426 
Batch[15285] - loss: 0.000272 
Batch[15286] - loss: 0.000240 
Batch[15287] - loss: 0.000297 
Batch[15288] - loss: 0.000373 
Batch[15289] - loss: 0.000256 
Batch[15290] - loss: 0.000195 
Batch[15291] - loss: 0.000166 
Batch[15292] - loss: 0.000266 
Batch[15293] - loss: 0.000435 
Batch[15294] - loss: 0.000162 
Batch[15295] - loss: 0.000289 
Batch[15296] - loss: 0.000275 
Batch[15297] - loss: 0.000243 
Batch[15298] - loss: 0.000141 
Batch[15299] - loss: 0.000186 
Batch[15300] - loss: 0.000338 

Evaluation - loss: 0.000072 pearson: 0.5525 

early stop by 1500 steps.
Batch[15301] - loss: 0.000231 
Batch[15302] - loss: 0.000231 
Batch[15303] - loss: 0.000269 
Batch[15304] - loss: 0.000269 
Batch[15305] - loss: 0.000222 
Batch[15306] - loss: 0.000107 
Batch[15307] - loss: 0.000148 
Batch[15308] - loss: 0.000266 
Batch[15309] - loss: 0.000195 
Batch[15310] - loss: 0.000289 
Batch[15311] - loss: 0.000281 
Batch[15312] - loss: 0.000275 
Batch[15313] - loss: 0.000305 
Batch[15314] - loss: 0.000224 
Batch[15315] - loss: 0.000177 
Batch[15316] - loss: 0.000198 
Batch[15317] - loss: 0.000145 
Batch[15318] - loss: 0.000193 
Batch[15319] - loss: 0.000132 
Batch[15320] - loss: 0.000189 
Batch[15321] - loss: 0.000274 
Batch[15322] - loss: 0.000212 
Batch[15323] - loss: 0.000219 
Batch[15324] - loss: 0.000155 
Batch[15325] - loss: 0.000239 
Batch[15326] - loss: 0.000247 
Batch[15327] - loss: 0.000237 
Batch[15328] - loss: 0.000183 
Batch[15329] - loss: 0.000254 
Batch[15330] - loss: 0.000174 
Batch[15331] - loss: 0.000194 
Batch[15332] - loss: 0.000311 
Batch[15333] - loss: 0.000305 
Batch[15334] - loss: 0.000206 
Batch[15335] - loss: 0.000344 
Batch[15336] - loss: 0.000179 
Batch[15337] - loss: 0.000253 
Batch[15338] - loss: 0.000269 
Batch[15339] - loss: 0.000236 
Batch[15340] - loss: 0.000206 
Batch[15341] - loss: 0.000170 
Batch[15342] - loss: 0.000300 
Batch[15343] - loss: 0.000309 
Batch[15344] - loss: 0.000335 
Batch[15345] - loss: 0.000221 
Batch[15346] - loss: 0.000231 
Batch[15347] - loss: 0.000216 
Batch[15348] - loss: 0.000225 
Batch[15349] - loss: 0.000159 
Batch[15350] - loss: 0.000227 
Batch[15351] - loss: 0.000297 
Batch[15352] - loss: 0.000269 
Batch[15353] - loss: 0.000138 
Batch[15354] - loss: 0.000304 
Batch[15355] - loss: 0.000088 
Batch[15356] - loss: 0.000175 
Batch[15357] - loss: 0.000285 
Batch[15358] - loss: 0.000232 
Batch[15359] - loss: 0.000251 
Batch[15360] - loss: 0.000194 
Batch[15361] - loss: 0.000278 
Batch[15362] - loss: 0.000113 
Batch[15363] - loss: 0.000257 
Batch[15364] - loss: 0.000145 
Batch[15365] - loss: 0.000201 
Batch[15366] - loss: 0.000266 
Batch[15367] - loss: 0.000267 
Batch[15368] - loss: 0.000437 
Batch[15369] - loss: 0.000281 
Batch[15370] - loss: 0.000254 
Batch[15371] - loss: 0.000197 
Batch[15372] - loss: 0.000250 
Batch[15373] - loss: 0.000334 
Batch[15374] - loss: 0.000388 
Batch[15375] - loss: 0.000490 
Batch[15376] - loss: 0.000402 
Batch[15377] - loss: 0.000328 
Batch[15378] - loss: 0.000382 
Batch[15379] - loss: 0.000121 
Batch[15380] - loss: 0.000189 
Batch[15381] - loss: 0.000189 
Batch[15382] - loss: 0.000176 
Batch[15383] - loss: 0.000232 
Batch[15384] - loss: 0.000182 
Batch[15385] - loss: 0.000162 
Batch[15386] - loss: 0.000219 
Batch[15387] - loss: 0.000392 
Batch[15388] - loss: 0.000225 
Batch[15389] - loss: 0.000273 
Batch[15390] - loss: 0.000166 
Batch[15391] - loss: 0.000218 
Batch[15392] - loss: 0.000197 
Batch[15393] - loss: 0.000209 
Batch[15394] - loss: 0.000178 
Batch[15395] - loss: 0.000169 
Batch[15396] - loss: 0.000286 
Batch[15397] - loss: 0.000237 
Batch[15398] - loss: 0.000176 
Batch[15399] - loss: 0.000191 
Batch[15400] - loss: 0.000304 

Evaluation - loss: 0.000072 pearson: 0.5491 

early stop by 1500 steps.
Batch[15401] - loss: 0.000256 
Batch[15402] - loss: 0.000292 
Batch[15403] - loss: 0.000484 
Batch[15404] - loss: 0.000303 
Batch[15405] - loss: 0.000306 
Batch[15406] - loss: 0.000173 
Batch[15407] - loss: 0.000303 
Batch[15408] - loss: 0.000193 
Batch[15409] - loss: 0.000279 
Batch[15410] - loss: 0.000179 
Batch[15411] - loss: 0.000206 
Batch[15412] - loss: 0.000210 
Batch[15413] - loss: 0.000177 
Batch[15414] - loss: 0.000182 
Batch[15415] - loss: 0.000261 
Batch[15416] - loss: 0.000221 
Batch[15417] - loss: 0.000318 
Batch[15418] - loss: 0.000250 
Batch[15419] - loss: 0.000095 
Batch[15420] - loss: 0.000190 
Batch[15421] - loss: 0.000296 
Batch[15422] - loss: 0.000367 
Batch[15423] - loss: 0.000201 
Batch[15424] - loss: 0.000227 
Batch[15425] - loss: 0.000514 
Batch[15426] - loss: 0.000123 
Batch[15427] - loss: 0.000202 
Batch[15428] - loss: 0.000134 
Batch[15429] - loss: 0.000094 
Batch[15430] - loss: 0.000699 
Batch[15431] - loss: 0.000351 
Batch[15432] - loss: 0.000279 
Batch[15433] - loss: 0.000270 
Batch[15434] - loss: 0.000314 
Batch[15435] - loss: 0.000165 
Batch[15436] - loss: 0.000255 
Batch[15437] - loss: 0.000228 
Batch[15438] - loss: 0.000247 
Batch[15439] - loss: 0.000180 
Batch[15440] - loss: 0.000135 
Batch[15441] - loss: 0.000234 
Batch[15442] - loss: 0.000216 
Batch[15443] - loss: 0.000124 
Batch[15444] - loss: 0.000164 
Batch[15445] - loss: 0.000320 
Batch[15446] - loss: 0.000300 
Batch[15447] - loss: 0.000137 
Batch[15448] - loss: 0.000362 
Batch[15449] - loss: 0.000297 
Batch[15450] - loss: 0.000356 
Batch[15451] - loss: 0.000268 
Batch[15452] - loss: 0.000247 
Batch[15453] - loss: 0.000198 
Batch[15454] - loss: 0.000250 
Batch[15455] - loss: 0.000162 
Batch[15456] - loss: 0.000266 
Batch[15457] - loss: 0.000244 
Batch[15458] - loss: 0.000161 
Batch[15459] - loss: 0.000224 
Batch[15460] - loss: 0.000162 
Batch[15461] - loss: 0.000233 
Batch[15462] - loss: 0.000219 
Batch[15463] - loss: 0.000331 
Batch[15464] - loss: 0.000348 
Batch[15465] - loss: 0.000250 
Batch[15466] - loss: 0.000455 
Batch[15467] - loss: 0.000396 
Batch[15468] - loss: 0.000295 
Batch[15469] - loss: 0.000283 
Batch[15470] - loss: 0.000119 
Batch[15471] - loss: 0.000271 
Batch[15472] - loss: 0.000428 
Batch[15473] - loss: 0.000309 
Batch[15474] - loss: 0.000297 
Batch[15475] - loss: 0.000312 
Batch[15476] - loss: 0.000168 
Batch[15477] - loss: 0.000285 
Batch[15478] - loss: 0.000252 
Batch[15479] - loss: 0.000205 
Batch[15480] - loss: 0.000239 
Batch[15481] - loss: 0.000210 
Batch[15482] - loss: 0.000182 
Batch[15483] - loss: 0.000264 
Batch[15484] - loss: 0.000306 
Batch[15485] - loss: 0.000271 
Batch[15486] - loss: 0.000170 
Batch[15487] - loss: 0.000181 
Batch[15488] - loss: 0.000168 
Batch[15489] - loss: 0.000233 
Batch[15490] - loss: 0.000188 
Batch[15491] - loss: 0.000184 
Batch[15492] - loss: 0.000542 
Batch[15493] - loss: 0.000270 
Batch[15494] - loss: 0.000258 
Batch[15495] - loss: 0.000129 
Batch[15496] - loss: 0.000250 
Batch[15497] - loss: 0.000178 
Batch[15498] - loss: 0.000156 
Batch[15499] - loss: 0.000338 
Batch[15500] - loss: 0.000279 

Evaluation - loss: 0.000072 pearson: 0.5523 

early stop by 1500 steps.
Batch[15501] - loss: 0.000210 
Batch[15502] - loss: 0.000174 
Batch[15503] - loss: 0.000191 
Batch[15504] - loss: 0.000219 
Batch[15505] - loss: 0.000200 
Batch[15506] - loss: 0.000150 
Batch[15507] - loss: 0.000263 
Batch[15508] - loss: 0.000151 
Batch[15509] - loss: 0.000154 
Batch[15510] - loss: 0.000242 
Batch[15511] - loss: 0.000202 
Batch[15512] - loss: 0.000233 
Batch[15513] - loss: 0.000270 
Batch[15514] - loss: 0.000128 
Batch[15515] - loss: 0.000206 
Batch[15516] - loss: 0.000208 
Batch[15517] - loss: 0.000415 
Batch[15518] - loss: 0.000503 
Batch[15519] - loss: 0.000459 
Batch[15520] - loss: 0.000234 
Batch[15521] - loss: 0.000241 
Batch[15522] - loss: 0.000613 
Batch[15523] - loss: 0.000181 
Batch[15524] - loss: 0.000290 
Batch[15525] - loss: 0.000152 
Batch[15526] - loss: 0.000154 
Batch[15527] - loss: 0.000230 
Batch[15528] - loss: 0.000246 
Batch[15529] - loss: 0.000172 
Batch[15530] - loss: 0.000258 
Batch[15531] - loss: 0.000387 
Batch[15532] - loss: 0.000203 
Batch[15533] - loss: 0.000225 
Batch[15534] - loss: 0.000189 
Batch[15535] - loss: 0.000216 
Batch[15536] - loss: 0.000168 
Batch[15537] - loss: 0.000226 
Batch[15538] - loss: 0.000144 
Batch[15539] - loss: 0.000248 
Batch[15540] - loss: 0.000233 
Batch[15541] - loss: 0.000173 
Batch[15542] - loss: 0.000231 
Batch[15543] - loss: 0.000204 
Batch[15544] - loss: 0.000273 
Batch[15545] - loss: 0.000216 
Batch[15546] - loss: 0.000258 
Batch[15547] - loss: 0.000198 
Batch[15548] - loss: 0.000098 
Batch[15549] - loss: 0.000124 
Batch[15550] - loss: 0.000168 
Batch[15551] - loss: 0.000287 
Batch[15552] - loss: 0.000257 
Batch[15553] - loss: 0.000260 
Batch[15554] - loss: 0.000215 
Batch[15555] - loss: 0.000327 
Batch[15556] - loss: 0.000332 
Batch[15557] - loss: 0.000225 
Batch[15558] - loss: 0.000259 
Batch[15559] - loss: 0.000098 
Batch[15560] - loss: 0.000247 
Batch[15561] - loss: 0.000257 
Batch[15562] - loss: 0.000206 
Batch[15563] - loss: 0.000330 
Batch[15564] - loss: 0.000221 
Batch[15565] - loss: 0.000189 
Batch[15566] - loss: 0.000342 
Batch[15567] - loss: 0.000187 
Batch[15568] - loss: 0.000453 
Batch[15569] - loss: 0.000191 
Batch[15570] - loss: 0.000307 
Batch[15571] - loss: 0.000398 
Batch[15572] - loss: 0.000294 
Batch[15573] - loss: 0.000158 
Batch[15574] - loss: 0.000313 
Batch[15575] - loss: 0.000178 
Batch[15576] - loss: 0.000201 
Batch[15577] - loss: 0.000210 
Batch[15578] - loss: 0.000209 
Batch[15579] - loss: 0.000194 
Batch[15580] - loss: 0.000286 
Batch[15581] - loss: 0.000288 
Batch[15582] - loss: 0.000183 
Batch[15583] - loss: 0.000232 
Batch[15584] - loss: 0.000237 
Batch[15585] - loss: 0.000690 
Batch[15586] - loss: 0.000174 
Batch[15587] - loss: 0.000235 
Batch[15588] - loss: 0.000172 
Batch[15589] - loss: 0.000218 
Batch[15590] - loss: 0.000218 
Batch[15591] - loss: 0.000191 
Batch[15592] - loss: 0.000243 
Batch[15593] - loss: 0.000513 
Batch[15594] - loss: 0.000121 
Batch[15595] - loss: 0.000180 
Batch[15596] - loss: 0.000246 
Batch[15597] - loss: 0.000129 
Batch[15598] - loss: 0.000220 
Batch[15599] - loss: 0.000376 
Batch[15600] - loss: 0.000228 

Evaluation - loss: 0.000072 pearson: 0.5503 

early stop by 1500 steps.
Batch[15601] - loss: 0.000134 
Batch[15602] - loss: 0.000231 
Batch[15603] - loss: 0.000140 
Batch[15604] - loss: 0.000374 
Batch[15605] - loss: 0.000158 
Batch[15606] - loss: 0.000235 
Batch[15607] - loss: 0.000174 
Batch[15608] - loss: 0.000319 
Batch[15609] - loss: 0.000316 
Batch[15610] - loss: 0.000168 
Batch[15611] - loss: 0.000337 
Batch[15612] - loss: 0.000214 
Batch[15613] - loss: 0.000246 
Batch[15614] - loss: 0.000260 
Batch[15615] - loss: 0.000297 
Batch[15616] - loss: 0.000263 
Batch[15617] - loss: 0.000152 
Batch[15618] - loss: 0.000159 
Batch[15619] - loss: 0.000167 
Batch[15620] - loss: 0.000357 
Batch[15621] - loss: 0.000209 
Batch[15622] - loss: 0.000191 
Batch[15623] - loss: 0.000292 
Batch[15624] - loss: 0.000142 
Batch[15625] - loss: 0.000509 
Batch[15626] - loss: 0.000150 
Batch[15627] - loss: 0.000181 
Batch[15628] - loss: 0.000230 
Batch[15629] - loss: 0.000184 
Batch[15630] - loss: 0.000213 
Batch[15631] - loss: 0.000392 
Batch[15632] - loss: 0.000232 
Batch[15633] - loss: 0.000170 
Batch[15634] - loss: 0.000304 
Batch[15635] - loss: 0.000318 
Batch[15636] - loss: 0.000231 
Batch[15637] - loss: 0.000293 
Batch[15638] - loss: 0.000337 
Batch[15639] - loss: 0.000166 
Batch[15640] - loss: 0.000207 
Batch[15641] - loss: 0.000405 
Batch[15642] - loss: 0.000224 
Batch[15643] - loss: 0.000197 
Batch[15644] - loss: 0.000427 
Batch[15645] - loss: 0.000145 
Batch[15646] - loss: 0.000193 
Batch[15647] - loss: 0.000220 
Batch[15648] - loss: 0.000099 
Batch[15649] - loss: 0.000253 
Batch[15650] - loss: 0.000277 
Batch[15651] - loss: 0.000378 
Batch[15652] - loss: 0.000156 
Batch[15653] - loss: 0.000225 
Batch[15654] - loss: 0.000284 
Batch[15655] - loss: 0.000188 
Batch[15656] - loss: 0.000201 
Batch[15657] - loss: 0.000201 
Batch[15658] - loss: 0.000145 
Batch[15659] - loss: 0.000225 
Batch[15660] - loss: 0.000098 
Batch[15661] - loss: 0.000174 
Batch[15662] - loss: 0.000389 
Batch[15663] - loss: 0.000304 
Batch[15664] - loss: 0.000230 
Batch[15665] - loss: 0.000159 
Batch[15666] - loss: 0.000291 
Batch[15667] - loss: 0.000138 
Batch[15668] - loss: 0.000166 
Batch[15669] - loss: 0.000254 
Batch[15670] - loss: 0.000181 
Batch[15671] - loss: 0.000257 
Batch[15672] - loss: 0.000269 
Batch[15673] - loss: 0.000147 
Batch[15674] - loss: 0.000156 
Batch[15675] - loss: 0.000393 
Batch[15676] - loss: 0.000386 
Batch[15677] - loss: 0.000206 
Batch[15678] - loss: 0.000159 
Batch[15679] - loss: 0.000132 
Batch[15680] - loss: 0.000223 
Batch[15681] - loss: 0.000384 
Batch[15682] - loss: 0.000352 
Batch[15683] - loss: 0.000257 
Batch[15684] - loss: 0.000289 
Batch[15685] - loss: 0.000278 
Batch[15686] - loss: 0.000194 
Batch[15687] - loss: 0.000212 
Batch[15688] - loss: 0.000352 
Batch[15689] - loss: 0.000279 
Batch[15690] - loss: 0.000209 
Batch[15691] - loss: 0.000174 
Batch[15692] - loss: 0.000339 
Batch[15693] - loss: 0.000463 
Batch[15694] - loss: 0.000259 
Batch[15695] - loss: 0.000177 
Batch[15696] - loss: 0.000126 
Batch[15697] - loss: 0.000213 
Batch[15698] - loss: 0.000247 
Batch[15699] - loss: 0.000297 
Batch[15700] - loss: 0.000248 

Evaluation - loss: 0.000072 pearson: 0.5494 

early stop by 1500 steps.
Batch[15701] - loss: 0.000372 
Batch[15702] - loss: 0.000231 
Batch[15703] - loss: 0.000237 
Batch[15704] - loss: 0.000481 
Batch[15705] - loss: 0.000183 
Batch[15706] - loss: 0.000246 
Batch[15707] - loss: 0.000223 
Batch[15708] - loss: 0.000203 
Batch[15709] - loss: 0.000142 
Batch[15710] - loss: 0.000300 
Batch[15711] - loss: 0.000488 
Batch[15712] - loss: 0.000201 
Batch[15713] - loss: 0.000149 
Batch[15714] - loss: 0.000415 
Batch[15715] - loss: 0.000145 
Batch[15716] - loss: 0.000275 
Batch[15717] - loss: 0.000182 
Batch[15718] - loss: 0.000195 
Batch[15719] - loss: 0.000236 
Batch[15720] - loss: 0.000190 
Batch[15721] - loss: 0.000278 
Batch[15722] - loss: 0.000164 
Batch[15723] - loss: 0.000316 
Batch[15724] - loss: 0.000207 
Batch[15725] - loss: 0.000172 
Batch[15726] - loss: 0.000220 
Batch[15727] - loss: 0.000137 
Batch[15728] - loss: 0.000171 
Batch[15729] - loss: 0.000184 
Batch[15730] - loss: 0.000298 
Batch[15731] - loss: 0.000294 
Batch[15732] - loss: 0.000362 
Batch[15733] - loss: 0.000187 
Batch[15734] - loss: 0.000175 
Batch[15735] - loss: 0.000196 
Batch[15736] - loss: 0.000216 
Batch[15737] - loss: 0.000108 
Batch[15738] - loss: 0.000273 
Batch[15739] - loss: 0.000328 
Batch[15740] - loss: 0.000134 
Batch[15741] - loss: 0.000108 
Batch[15742] - loss: 0.000196 
Batch[15743] - loss: 0.000257 
Batch[15744] - loss: 0.000325 
Batch[15745] - loss: 0.000489 
Batch[15746] - loss: 0.000127 
Batch[15747] - loss: 0.000244 
Batch[15748] - loss: 0.000462 
Batch[15749] - loss: 0.000193 
Batch[15750] - loss: 0.000213 
Batch[15751] - loss: 0.000357 
Batch[15752] - loss: 0.000156 
Batch[15753] - loss: 0.000206 
Batch[15754] - loss: 0.000159 
Batch[15755] - loss: 0.000267 
Batch[15756] - loss: 0.000303 
Batch[15757] - loss: 0.000307 
Batch[15758] - loss: 0.000226 
Batch[15759] - loss: 0.000283 
Batch[15760] - loss: 0.000282 
Batch[15761] - loss: 0.000207 
Batch[15762] - loss: 0.000145 
Batch[15763] - loss: 0.000378 
Batch[15764] - loss: 0.000165 
Batch[15765] - loss: 0.000317 
Batch[15766] - loss: 0.000309 
Batch[15767] - loss: 0.000480 
Batch[15768] - loss: 0.000423 
Batch[15769] - loss: 0.000216 
Batch[15770] - loss: 0.000166 
Batch[15771] - loss: 0.000368 
Batch[15772] - loss: 0.000248 
Batch[15773] - loss: 0.000310 
Batch[15774] - loss: 0.000304 
Batch[15775] - loss: 0.000214 
Batch[15776] - loss: 0.000222 
Batch[15777] - loss: 0.000214 
Batch[15778] - loss: 0.000200 
Batch[15779] - loss: 0.000160 
Batch[15780] - loss: 0.000223 
Batch[15781] - loss: 0.000278 
Batch[15782] - loss: 0.000239 
Batch[15783] - loss: 0.000253 
Batch[15784] - loss: 0.000112 
Batch[15785] - loss: 0.000191 
Batch[15786] - loss: 0.000359 
Batch[15787] - loss: 0.000205 
Batch[15788] - loss: 0.000232 
Batch[15789] - loss: 0.000131 
Batch[15790] - loss: 0.000276 
Batch[15791] - loss: 0.000185 
Batch[15792] - loss: 0.000348 
Batch[15793] - loss: 0.000116 
Batch[15794] - loss: 0.000175 
Batch[15795] - loss: 0.000356 
Batch[15796] - loss: 0.000156 
Batch[15797] - loss: 0.000170 
Batch[15798] - loss: 0.000346 
Batch[15799] - loss: 0.000222 
Batch[15800] - loss: 0.000270 

Evaluation - loss: 0.000072 pearson: 0.5494 

early stop by 1500 steps.
Batch[15801] - loss: 0.000241 
Batch[15802] - loss: 0.000251 
Batch[15803] - loss: 0.000352 
Batch[15804] - loss: 0.000294 
Batch[15805] - loss: 0.000184 
Batch[15806] - loss: 0.000194 
Batch[15807] - loss: 0.000353 
Batch[15808] - loss: 0.000443 
Batch[15809] - loss: 0.000225 
Batch[15810] - loss: 0.000251 
Batch[15811] - loss: 0.000231 
Batch[15812] - loss: 0.000247 
Batch[15813] - loss: 0.000185 
Batch[15814] - loss: 0.000357 
Batch[15815] - loss: 0.000236 
Batch[15816] - loss: 0.000434 
Batch[15817] - loss: 0.000301 
Batch[15818] - loss: 0.000194 
Batch[15819] - loss: 0.000153 
Batch[15820] - loss: 0.000231 
Batch[15821] - loss: 0.000276 
Batch[15822] - loss: 0.000145 
Batch[15823] - loss: 0.000176 
Batch[15824] - loss: 0.000497 
Batch[15825] - loss: 0.000221 
Batch[15826] - loss: 0.000250 
Batch[15827] - loss: 0.000199 
Batch[15828] - loss: 0.000328 
Batch[15829] - loss: 0.000175 
Batch[15830] - loss: 0.000215 
Batch[15831] - loss: 0.000294 
Batch[15832] - loss: 0.000225 
Batch[15833] - loss: 0.000191 
Batch[15834] - loss: 0.000161 
Batch[15835] - loss: 0.000219 
Batch[15836] - loss: 0.000300 
Batch[15837] - loss: 0.000166 
Batch[15838] - loss: 0.000196 
Batch[15839] - loss: 0.000236 
Batch[15840] - loss: 0.000172 
Batch[15841] - loss: 0.000173 
Batch[15842] - loss: 0.000340 
Batch[15843] - loss: 0.000182 
Batch[15844] - loss: 0.000147 
Batch[15845] - loss: 0.000383 
Batch[15846] - loss: 0.000236 
Batch[15847] - loss: 0.000315 
Batch[15848] - loss: 0.000451 
Batch[15849] - loss: 0.000135 
Batch[15850] - loss: 0.000207 
Batch[15851] - loss: 0.000445 
Batch[15852] - loss: 0.000261 
Batch[15853] - loss: 0.000164 
Batch[15854] - loss: 0.000198 
Batch[15855] - loss: 0.000168 
Batch[15856] - loss: 0.000188 
Batch[15857] - loss: 0.000131 
Batch[15858] - loss: 0.000233 
Batch[15859] - loss: 0.000163 
Batch[15860] - loss: 0.000160 
Batch[15861] - loss: 0.000302 
Batch[15862] - loss: 0.000236 
Batch[15863] - loss: 0.000201 
Batch[15864] - loss: 0.000309 
Batch[15865] - loss: 0.000125 
Batch[15866] - loss: 0.000382 
Batch[15867] - loss: 0.000109 
Batch[15868] - loss: 0.000187 
Batch[15869] - loss: 0.000173 
Batch[15870] - loss: 0.000260 
Batch[15871] - loss: 0.000308 
Batch[15872] - loss: 0.000399 
Batch[15873] - loss: 0.000111 
Batch[15874] - loss: 0.000118 
Batch[15875] - loss: 0.000176 
Batch[15876] - loss: 0.000392 
Batch[15877] - loss: 0.000148 
Batch[15878] - loss: 0.000167 
Batch[15879] - loss: 0.000195 
Batch[15880] - loss: 0.000143 
Batch[15881] - loss: 0.000136 
Batch[15882] - loss: 0.000176 
Batch[15883] - loss: 0.000183 
Batch[15884] - loss: 0.000173 
Batch[15885] - loss: 0.000218 
Batch[15886] - loss: 0.000201 
Batch[15887] - loss: 0.000256 
Batch[15888] - loss: 0.000217 
Batch[15889] - loss: 0.000167 
Batch[15890] - loss: 0.000170 
Batch[15891] - loss: 0.000364 
Batch[15892] - loss: 0.000169 
Batch[15893] - loss: 0.000266 
Batch[15894] - loss: 0.000320 
Batch[15895] - loss: 0.000319 
Batch[15896] - loss: 0.000234 
Batch[15897] - loss: 0.000189 
Batch[15898] - loss: 0.000215 
Batch[15899] - loss: 0.000272 
Batch[15900] - loss: 0.000305 

Evaluation - loss: 0.000072 pearson: 0.5489 

early stop by 1500 steps.
Batch[15901] - loss: 0.000235 
Batch[15902] - loss: 0.000136 
Batch[15903] - loss: 0.000212 
Batch[15904] - loss: 0.000153 
Batch[15905] - loss: 0.000347 
Batch[15906] - loss: 0.000227 
Batch[15907] - loss: 0.000170 
Batch[15908] - loss: 0.000223 
Batch[15909] - loss: 0.000222 
Batch[15910] - loss: 0.000214 
Batch[15911] - loss: 0.000198 
Batch[15912] - loss: 0.000185 
Batch[15913] - loss: 0.000249 
Batch[15914] - loss: 0.000408 
Batch[15915] - loss: 0.000212 
Batch[15916] - loss: 0.000176 
Batch[15917] - loss: 0.000170 
Batch[15918] - loss: 0.000356 
Batch[15919] - loss: 0.000282 
Batch[15920] - loss: 0.000220 
Batch[15921] - loss: 0.000135 
Batch[15922] - loss: 0.000271 
Batch[15923] - loss: 0.000435 
Batch[15924] - loss: 0.000334 
Batch[15925] - loss: 0.000413 
Batch[15926] - loss: 0.000298 
Batch[15927] - loss: 0.000464 
Batch[15928] - loss: 0.000173 
Batch[15929] - loss: 0.000184 
Batch[15930] - loss: 0.000331 
Batch[15931] - loss: 0.000149 
Batch[15932] - loss: 0.000217 
Batch[15933] - loss: 0.000312 
Batch[15934] - loss: 0.000137 
Batch[15935] - loss: 0.000167 
Batch[15936] - loss: 0.000099 
Batch[15937] - loss: 0.000212 
Batch[15938] - loss: 0.000215 
Batch[15939] - loss: 0.000148 
Batch[15940] - loss: 0.000150 
Batch[15941] - loss: 0.000214 
Batch[15942] - loss: 0.000200 
Batch[15943] - loss: 0.000138 
Batch[15944] - loss: 0.000363 
Batch[15945] - loss: 0.000284 
Batch[15946] - loss: 0.000227 
Batch[15947] - loss: 0.000269 
Batch[15948] - loss: 0.000221 
Batch[15949] - loss: 0.000369 
Batch[15950] - loss: 0.000229 
Batch[15951] - loss: 0.000245 
Batch[15952] - loss: 0.000281 
Batch[15953] - loss: 0.000182 
Batch[15954] - loss: 0.000182 
Batch[15955] - loss: 0.000296 
Batch[15956] - loss: 0.000296 
Batch[15957] - loss: 0.000202 
Batch[15958] - loss: 0.000239 
Batch[15959] - loss: 0.000199 
Batch[15960] - loss: 0.000274 
Batch[15961] - loss: 0.000298 
Batch[15962] - loss: 0.000256 
Batch[15963] - loss: 0.000128 
Batch[15964] - loss: 0.000176 
Batch[15965] - loss: 0.000349 
Batch[15966] - loss: 0.000197 
Batch[15967] - loss: 0.000166 
Batch[15968] - loss: 0.000446 
Batch[15969] - loss: 0.000218 
Batch[15970] - loss: 0.000195 
Batch[15971] - loss: 0.000360 
Batch[15972] - loss: 0.000263 
Batch[15973] - loss: 0.000249 
Batch[15974] - loss: 0.000091 
Batch[15975] - loss: 0.000204 
Batch[15976] - loss: 0.000159 
Batch[15977] - loss: 0.000194 
Batch[15978] - loss: 0.000272 
Batch[15979] - loss: 0.000260 
Batch[15980] - loss: 0.000166 
Batch[15981] - loss: 0.000288 
Batch[15982] - loss: 0.000145 
Batch[15983] - loss: 0.000190 
Batch[15984] - loss: 0.000235 
Batch[15985] - loss: 0.000178 
Batch[15986] - loss: 0.000221 
Batch[15987] - loss: 0.000170 
Batch[15988] - loss: 0.000142 
Batch[15989] - loss: 0.000196 
Batch[15990] - loss: 0.000232 
Batch[15991] - loss: 0.000194 
Batch[15992] - loss: 0.000304 
Batch[15993] - loss: 0.000168 
Batch[15994] - loss: 0.000372 
Batch[15995] - loss: 0.000396 
Batch[15996] - loss: 0.000172 
Batch[15997] - loss: 0.000227 
Batch[15998] - loss: 0.000177 
Batch[15999] - loss: 0.000174 
Batch[16000] - loss: 0.000312 

Evaluation - loss: 0.000072 pearson: 0.5514 

early stop by 1500 steps.
Batch[16001] - loss: 0.000250 
Batch[16002] - loss: 0.000204 
Batch[16003] - loss: 0.000436 
Batch[16004] - loss: 0.000364 
Batch[16005] - loss: 0.000237 
Batch[16006] - loss: 0.000186 
Batch[16007] - loss: 0.000308 
Batch[16008] - loss: 0.000260 
Batch[16009] - loss: 0.000236 
Batch[16010] - loss: 0.000168 
Batch[16011] - loss: 0.000212 
Batch[16012] - loss: 0.000191 
Batch[16013] - loss: 0.000232 
Batch[16014] - loss: 0.000076 
Batch[16015] - loss: 0.000259 
Batch[16016] - loss: 0.000126 
Batch[16017] - loss: 0.000372 
Batch[16018] - loss: 0.000242 
Batch[16019] - loss: 0.000212 
Batch[16020] - loss: 0.000198 
Batch[16021] - loss: 0.000469 
Batch[16022] - loss: 0.000286 
Batch[16023] - loss: 0.000225 
Batch[16024] - loss: 0.000207 
Batch[16025] - loss: 0.000269 
Batch[16026] - loss: 0.000225 
Batch[16027] - loss: 0.000155 
Batch[16028] - loss: 0.000132 
Batch[16029] - loss: 0.000275 
Batch[16030] - loss: 0.000134 
Batch[16031] - loss: 0.000233 
Batch[16032] - loss: 0.000373 
Batch[16033] - loss: 0.000140 
Batch[16034] - loss: 0.000159 
Batch[16035] - loss: 0.000236 
Batch[16036] - loss: 0.000174 
Batch[16037] - loss: 0.000239 
Batch[16038] - loss: 0.000195 
Batch[16039] - loss: 0.000202 
Batch[16040] - loss: 0.000421 
Batch[16041] - loss: 0.000458 
Batch[16042] - loss: 0.000164 
Batch[16043] - loss: 0.000262 
Batch[16044] - loss: 0.000230 
Batch[16045] - loss: 0.000160 
Batch[16046] - loss: 0.000287 
Batch[16047] - loss: 0.000307 
Batch[16048] - loss: 0.000157 
Batch[16049] - loss: 0.000255 
Batch[16050] - loss: 0.000249 
Batch[16051] - loss: 0.000196 
Batch[16052] - loss: 0.000405 
Batch[16053] - loss: 0.000247 
Batch[16054] - loss: 0.000201 
Batch[16055] - loss: 0.000163 
Batch[16056] - loss: 0.000217 
Batch[16057] - loss: 0.000212 
Batch[16058] - loss: 0.000222 
Batch[16059] - loss: 0.000213 
Batch[16060] - loss: 0.000178 
Batch[16061] - loss: 0.000203 
Batch[16062] - loss: 0.000167 
Batch[16063] - loss: 0.000149 
Batch[16064] - loss: 0.000160 
Batch[16065] - loss: 0.000193 
Batch[16066] - loss: 0.000158 
Batch[16067] - loss: 0.000230 
Batch[16068] - loss: 0.000307 
Batch[16069] - loss: 0.000134 
Batch[16070] - loss: 0.000149 
Batch[16071] - loss: 0.000255 
Batch[16072] - loss: 0.000289 
Batch[16073] - loss: 0.000450 
Batch[16074] - loss: 0.000130 
Batch[16075] - loss: 0.000216 
Batch[16076] - loss: 0.000231 
Batch[16077] - loss: 0.000186 
Batch[16078] - loss: 0.000256 
Batch[16079] - loss: 0.000089 
Batch[16080] - loss: 0.000180 
Batch[16081] - loss: 0.000136 
Batch[16082] - loss: 0.000201 
Batch[16083] - loss: 0.000161 
Batch[16084] - loss: 0.000236 
Batch[16085] - loss: 0.000433 
Batch[16086] - loss: 0.000203 
Batch[16087] - loss: 0.000167 
Batch[16088] - loss: 0.000161 
Batch[16089] - loss: 0.000361 
Batch[16090] - loss: 0.000172 
Batch[16091] - loss: 0.000254 
Batch[16092] - loss: 0.000216 
Batch[16093] - loss: 0.000280 
Batch[16094] - loss: 0.000198 
Batch[16095] - loss: 0.000281 
Batch[16096] - loss: 0.000318 
Batch[16097] - loss: 0.000270 
Batch[16098] - loss: 0.000172 
Batch[16099] - loss: 0.000285 
Batch[16100] - loss: 0.000349 

Evaluation - loss: 0.000072 pearson: 0.5511 

early stop by 1500 steps.
Batch[16101] - loss: 0.000227 
Batch[16102] - loss: 0.000211 
Batch[16103] - loss: 0.000206 
Batch[16104] - loss: 0.000199 
Batch[16105] - loss: 0.000320 
Batch[16106] - loss: 0.000185 
Batch[16107] - loss: 0.000194 
Batch[16108] - loss: 0.000124 
Batch[16109] - loss: 0.000167 
Batch[16110] - loss: 0.000342 
Batch[16111] - loss: 0.000308 
Batch[16112] - loss: 0.000220 
Batch[16113] - loss: 0.000266 
Batch[16114] - loss: 0.000305 
Batch[16115] - loss: 0.000267 
Batch[16116] - loss: 0.000236 
Batch[16117] - loss: 0.000211 
Batch[16118] - loss: 0.000333 
Batch[16119] - loss: 0.000303 
Batch[16120] - loss: 0.000120 
Batch[16121] - loss: 0.000169 
Batch[16122] - loss: 0.000194 
Batch[16123] - loss: 0.000177 
Batch[16124] - loss: 0.000254 
Batch[16125] - loss: 0.000220 
Batch[16126] - loss: 0.000178 
Batch[16127] - loss: 0.000229 
Batch[16128] - loss: 0.000178 
Batch[16129] - loss: 0.000203 
Batch[16130] - loss: 0.000151 
Batch[16131] - loss: 0.000248 
Batch[16132] - loss: 0.000267 
Batch[16133] - loss: 0.000151 
Batch[16134] - loss: 0.000276 
Batch[16135] - loss: 0.000309 
Batch[16136] - loss: 0.000187 
Batch[16137] - loss: 0.000177 
Batch[16138] - loss: 0.000160 
Batch[16139] - loss: 0.000197 
Batch[16140] - loss: 0.000173 
Batch[16141] - loss: 0.000308 
Batch[16142] - loss: 0.000260 
Batch[16143] - loss: 0.000201 
Batch[16144] - loss: 0.000181 
Batch[16145] - loss: 0.000227 
Batch[16146] - loss: 0.000077 
Batch[16147] - loss: 0.000258 
Batch[16148] - loss: 0.000137 
Batch[16149] - loss: 0.000224 
Batch[16150] - loss: 0.000195 
Batch[16151] - loss: 0.000190 
Batch[16152] - loss: 0.000205 
Batch[16153] - loss: 0.000279 
Batch[16154] - loss: 0.000155 
Batch[16155] - loss: 0.000372 
Batch[16156] - loss: 0.000196 
Batch[16157] - loss: 0.000107 
Batch[16158] - loss: 0.000282 
Batch[16159] - loss: 0.000209 
Batch[16160] - loss: 0.000200 
Batch[16161] - loss: 0.000348 
Batch[16162] - loss: 0.000190 
Batch[16163] - loss: 0.000139 
Batch[16164] - loss: 0.000142 
Batch[16165] - loss: 0.000332 
Batch[16166] - loss: 0.000339 
Batch[16167] - loss: 0.000209 
Batch[16168] - loss: 0.000193 
Batch[16169] - loss: 0.000227 
Batch[16170] - loss: 0.000123 
Batch[16171] - loss: 0.000153 
Batch[16172] - loss: 0.000241 
Batch[16173] - loss: 0.000147 
Batch[16174] - loss: 0.000172 
Batch[16175] - loss: 0.000233 
Batch[16176] - loss: 0.000205 
Batch[16177] - loss: 0.000201 
Batch[16178] - loss: 0.000296 
Batch[16179] - loss: 0.000261 
Batch[16180] - loss: 0.000300 
Batch[16181] - loss: 0.000281 
Batch[16182] - loss: 0.000141 
Batch[16183] - loss: 0.000256 
Batch[16184] - loss: 0.000238 
Batch[16185] - loss: 0.000323 
Batch[16186] - loss: 0.000180 
Batch[16187] - loss: 0.000451 
Batch[16188] - loss: 0.000260 
Batch[16189] - loss: 0.000199 
Batch[16190] - loss: 0.000196 
Batch[16191] - loss: 0.000166 
Batch[16192] - loss: 0.000335 
Batch[16193] - loss: 0.000266 
Batch[16194] - loss: 0.000160 
Batch[16195] - loss: 0.000178 
Batch[16196] - loss: 0.000391 
Batch[16197] - loss: 0.000289 
Batch[16198] - loss: 0.000150 
Batch[16199] - loss: 0.000157 
Batch[16200] - loss: 0.000153 

Evaluation - loss: 0.000072 pearson: 0.5498 

early stop by 1500 steps.
Batch[16201] - loss: 0.000116 
Batch[16202] - loss: 0.000176 
Batch[16203] - loss: 0.000223 
Batch[16204] - loss: 0.000132 
Batch[16205] - loss: 0.000307 
Batch[16206] - loss: 0.000153 
Batch[16207] - loss: 0.000141 
Batch[16208] - loss: 0.000280 
Batch[16209] - loss: 0.000306 
Batch[16210] - loss: 0.000250 
Batch[16211] - loss: 0.000351 
Batch[16212] - loss: 0.000187 
Batch[16213] - loss: 0.000136 
Batch[16214] - loss: 0.000344 
Batch[16215] - loss: 0.000090 
Batch[16216] - loss: 0.000176 
Batch[16217] - loss: 0.000272 
Batch[16218] - loss: 0.000167 
Batch[16219] - loss: 0.000192 
Batch[16220] - loss: 0.000132 
Batch[16221] - loss: 0.000106 
Batch[16222] - loss: 0.000310 
Batch[16223] - loss: 0.000178 
Batch[16224] - loss: 0.000179 
Batch[16225] - loss: 0.000291 
Batch[16226] - loss: 0.000255 
Batch[16227] - loss: 0.000265 
Batch[16228] - loss: 0.000263 
Batch[16229] - loss: 0.000198 
Batch[16230] - loss: 0.000260 
Batch[16231] - loss: 0.000544 
Batch[16232] - loss: 0.000225 
Batch[16233] - loss: 0.000244 
Batch[16234] - loss: 0.000169 
Batch[16235] - loss: 0.000196 
Batch[16236] - loss: 0.000245 
Batch[16237] - loss: 0.000117 
Batch[16238] - loss: 0.000195 
Batch[16239] - loss: 0.000219 
Batch[16240] - loss: 0.000175 
Batch[16241] - loss: 0.000270 
Batch[16242] - loss: 0.000122 
Batch[16243] - loss: 0.000246 
Batch[16244] - loss: 0.000329 
Batch[16245] - loss: 0.000233 
Batch[16246] - loss: 0.000190 
Batch[16247] - loss: 0.000169 
Batch[16248] - loss: 0.000307 
Batch[16249] - loss: 0.000217 
Batch[16250] - loss: 0.000334 
Batch[16251] - loss: 0.000212 
Batch[16252] - loss: 0.000304 
Batch[16253] - loss: 0.000218 
Batch[16254] - loss: 0.000132 
Batch[16255] - loss: 0.000396 
Batch[16256] - loss: 0.000144 
Batch[16257] - loss: 0.000191 
Batch[16258] - loss: 0.000101 
Batch[16259] - loss: 0.000141 
Batch[16260] - loss: 0.000156 
Batch[16261] - loss: 0.000240 
Batch[16262] - loss: 0.000173 
Batch[16263] - loss: 0.000277 
Batch[16264] - loss: 0.000148 
Batch[16265] - loss: 0.000285 
Batch[16266] - loss: 0.000267 
Batch[16267] - loss: 0.000203 
Batch[16268] - loss: 0.000241 
Batch[16269] - loss: 0.000133 
Batch[16270] - loss: 0.000242 
Batch[16271] - loss: 0.000195 
Batch[16272] - loss: 0.000165 
Batch[16273] - loss: 0.000189 
Batch[16274] - loss: 0.000278 
Batch[16275] - loss: 0.000340 
Batch[16276] - loss: 0.000217 
Batch[16277] - loss: 0.000099 
Batch[16278] - loss: 0.000220 
Batch[16279] - loss: 0.000159 
Batch[16280] - loss: 0.000346 
Batch[16281] - loss: 0.000261 
Batch[16282] - loss: 0.000249 
Batch[16283] - loss: 0.000255 
Batch[16284] - loss: 0.000163 
Batch[16285] - loss: 0.000245 
Batch[16286] - loss: 0.000130 
Batch[16287] - loss: 0.000326 
Batch[16288] - loss: 0.000209 
Batch[16289] - loss: 0.000165 
Batch[16290] - loss: 0.000161 
Batch[16291] - loss: 0.000151 
Batch[16292] - loss: 0.000271 
Batch[16293] - loss: 0.000165 
Batch[16294] - loss: 0.000145 
Batch[16295] - loss: 0.000113 
Batch[16296] - loss: 0.000261 
Batch[16297] - loss: 0.000365 
Batch[16298] - loss: 0.000105 
Batch[16299] - loss: 0.000277 
Batch[16300] - loss: 0.000232 

Evaluation - loss: 0.000072 pearson: 0.5507 

early stop by 1500 steps.
Batch[16301] - loss: 0.000229 
Batch[16302] - loss: 0.000160 
Batch[16303] - loss: 0.000147 
Batch[16304] - loss: 0.000199 
Batch[16305] - loss: 0.000293 
Batch[16306] - loss: 0.000261 
Batch[16307] - loss: 0.000413 
Batch[16308] - loss: 0.000301 
Batch[16309] - loss: 0.000251 
Batch[16310] - loss: 0.000199 
Batch[16311] - loss: 0.000223 
Batch[16312] - loss: 0.000678 
Batch[16313] - loss: 0.000218 
Batch[16314] - loss: 0.000222 
Batch[16315] - loss: 0.000186 
Batch[16316] - loss: 0.000247 
Batch[16317] - loss: 0.000162 
Batch[16318] - loss: 0.000197 
Batch[16319] - loss: 0.000161 
Batch[16320] - loss: 0.000210 
Batch[16321] - loss: 0.000171 
Batch[16322] - loss: 0.000336 
Batch[16323] - loss: 0.000088 
Batch[16324] - loss: 0.000189 
Batch[16325] - loss: 0.000177 
Batch[16326] - loss: 0.000274 
Batch[16327] - loss: 0.000176 
Batch[16328] - loss: 0.000156 
Batch[16329] - loss: 0.000260 
Batch[16330] - loss: 0.000212 
Batch[16331] - loss: 0.000166 
Batch[16332] - loss: 0.000439 
Batch[16333] - loss: 0.000226 
Batch[16334] - loss: 0.000250 
Batch[16335] - loss: 0.000194 
Batch[16336] - loss: 0.000234 
Batch[16337] - loss: 0.000193 
Batch[16338] - loss: 0.000168 
Batch[16339] - loss: 0.000295 
Batch[16340] - loss: 0.000114 
Batch[16341] - loss: 0.000282 
Batch[16342] - loss: 0.000427 
Batch[16343] - loss: 0.000269 
Batch[16344] - loss: 0.000193 
Batch[16345] - loss: 0.000240 
Batch[16346] - loss: 0.000115 
Batch[16347] - loss: 0.000142 
Batch[16348] - loss: 0.000225 
Batch[16349] - loss: 0.000127 
Batch[16350] - loss: 0.000195 
Batch[16351] - loss: 0.000265 
Batch[16352] - loss: 0.000176 
Batch[16353] - loss: 0.000177 
Batch[16354] - loss: 0.000192 
Batch[16355] - loss: 0.000177 
Batch[16356] - loss: 0.000135 
Batch[16357] - loss: 0.000151 
Batch[16358] - loss: 0.000241 
Batch[16359] - loss: 0.000198 
Batch[16360] - loss: 0.000182 
Batch[16361] - loss: 0.000163 
Batch[16362] - loss: 0.000220 
Batch[16363] - loss: 0.000245 
Batch[16364] - loss: 0.000147 
Batch[16365] - loss: 0.000199 
Batch[16366] - loss: 0.000247 
Batch[16367] - loss: 0.000215 
Batch[16368] - loss: 0.000465 
Batch[16369] - loss: 0.000272 
Batch[16370] - loss: 0.000180 
Batch[16371] - loss: 0.000251 
Batch[16372] - loss: 0.000162 
Batch[16373] - loss: 0.000256 
Batch[16374] - loss: 0.000240 
Batch[16375] - loss: 0.000200 
Batch[16376] - loss: 0.000171 
Batch[16377] - loss: 0.000313 
Batch[16378] - loss: 0.000210 
Batch[16379] - loss: 0.000215 
Batch[16380] - loss: 0.000352 
Batch[16381] - loss: 0.000278 
Batch[16382] - loss: 0.000091 
Batch[16383] - loss: 0.000261 
Batch[16384] - loss: 0.000135 
Batch[16385] - loss: 0.000192 
Batch[16386] - loss: 0.000281 
Batch[16387] - loss: 0.000192 
Batch[16388] - loss: 0.000252 
Batch[16389] - loss: 0.000258 
Batch[16390] - loss: 0.000237 
Batch[16391] - loss: 0.000286 
Batch[16392] - loss: 0.000357 
Batch[16393] - loss: 0.000223 
Batch[16394] - loss: 0.000267 
Batch[16395] - loss: 0.000240 
Batch[16396] - loss: 0.000233 
Batch[16397] - loss: 0.000194 
Batch[16398] - loss: 0.000258 
Batch[16399] - loss: 0.000250 
Batch[16400] - loss: 0.000212 

Evaluation - loss: 0.000072 pearson: 0.5498 

early stop by 1500 steps.
Batch[16401] - loss: 0.000311 
Batch[16402] - loss: 0.000160 
Batch[16403] - loss: 0.000123 
Batch[16404] - loss: 0.000234 
Batch[16405] - loss: 0.000273 
Batch[16406] - loss: 0.000163 
Batch[16407] - loss: 0.000258 
Batch[16408] - loss: 0.000217 
Batch[16409] - loss: 0.000384 
Batch[16410] - loss: 0.000427 
Batch[16411] - loss: 0.000225 
Batch[16412] - loss: 0.000276 
Batch[16413] - loss: 0.000179 
Batch[16414] - loss: 0.000279 
Batch[16415] - loss: 0.000189 
Batch[16416] - loss: 0.000156 
Batch[16417] - loss: 0.000151 
Batch[16418] - loss: 0.000172 
Batch[16419] - loss: 0.000508 
Batch[16420] - loss: 0.000276 
Batch[16421] - loss: 0.000329 
Batch[16422] - loss: 0.000294 
Batch[16423] - loss: 0.000164 
Batch[16424] - loss: 0.000344 
Batch[16425] - loss: 0.000282 
Batch[16426] - loss: 0.000144 
Batch[16427] - loss: 0.000187 
Batch[16428] - loss: 0.000262 
Batch[16429] - loss: 0.000157 
Batch[16430] - loss: 0.000266 
Batch[16431] - loss: 0.000214 
Batch[16432] - loss: 0.000198 
Batch[16433] - loss: 0.000288 
Batch[16434] - loss: 0.000400 
Batch[16435] - loss: 0.000232 
Batch[16436] - loss: 0.000299 
Batch[16437] - loss: 0.000384 
Batch[16438] - loss: 0.000161 
Batch[16439] - loss: 0.000260 
Batch[16440] - loss: 0.000302 
Batch[16441] - loss: 0.000205 
Batch[16442] - loss: 0.000373 
Batch[16443] - loss: 0.000266 
Batch[16444] - loss: 0.000221 
Batch[16445] - loss: 0.000179 
Batch[16446] - loss: 0.000195 
Batch[16447] - loss: 0.000115 
Batch[16448] - loss: 0.000260 
Batch[16449] - loss: 0.000311 
Batch[16450] - loss: 0.000269 
Batch[16451] - loss: 0.000215 
Batch[16452] - loss: 0.000317 
Batch[16453] - loss: 0.000328 
Batch[16454] - loss: 0.000283 
Batch[16455] - loss: 0.000141 
Batch[16456] - loss: 0.000471 
Batch[16457] - loss: 0.000178 
Batch[16458] - loss: 0.000234 
Batch[16459] - loss: 0.000206 
Batch[16460] - loss: 0.000295 
Batch[16461] - loss: 0.000236 
Batch[16462] - loss: 0.000301 
Batch[16463] - loss: 0.000132 
Batch[16464] - loss: 0.000234 
Batch[16465] - loss: 0.000261 
Batch[16466] - loss: 0.000421 
Batch[16467] - loss: 0.000152 
Batch[16468] - loss: 0.000154 
Batch[16469] - loss: 0.000314 
Batch[16470] - loss: 0.000182 
Batch[16471] - loss: 0.000361 
Batch[16472] - loss: 0.000372 
Batch[16473] - loss: 0.000174 
Batch[16474] - loss: 0.000135 
Batch[16475] - loss: 0.000188 
Batch[16476] - loss: 0.000492 
Batch[16477] - loss: 0.000288 
Batch[16478] - loss: 0.000109 
Batch[16479] - loss: 0.000295 
Batch[16480] - loss: 0.000177 
Batch[16481] - loss: 0.000313 
Batch[16482] - loss: 0.000187 
Batch[16483] - loss: 0.000407 
Batch[16484] - loss: 0.000133 
Batch[16485] - loss: 0.000206 
Batch[16486] - loss: 0.000208 
Batch[16487] - loss: 0.000166 
Batch[16488] - loss: 0.000254 
Batch[16489] - loss: 0.000529 
Batch[16490] - loss: 0.000299 
Batch[16491] - loss: 0.000195 
Batch[16492] - loss: 0.000233 
Batch[16493] - loss: 0.000208 
Batch[16494] - loss: 0.000140 
Batch[16495] - loss: 0.000142 
Batch[16496] - loss: 0.000317 
Batch[16497] - loss: 0.000200 
Batch[16498] - loss: 0.000255 
Batch[16499] - loss: 0.000245 
Batch[16500] - loss: 0.000460 

Evaluation - loss: 0.000072 pearson: 0.5509 

early stop by 1500 steps.
Batch[16501] - loss: 0.000278 
Batch[16502] - loss: 0.000269 
Batch[16503] - loss: 0.000216 
Batch[16504] - loss: 0.000191 
Batch[16505] - loss: 0.000276 
Batch[16506] - loss: 0.000203 
Batch[16507] - loss: 0.000180 
Batch[16508] - loss: 0.000296 
Batch[16509] - loss: 0.000227 
Batch[16510] - loss: 0.000416 
Batch[16511] - loss: 0.000320 
Batch[16512] - loss: 0.000312 
Batch[16513] - loss: 0.000267 
Batch[16514] - loss: 0.000250 
Batch[16515] - loss: 0.000234 
Batch[16516] - loss: 0.000363 
Batch[16517] - loss: 0.000298 
Batch[16518] - loss: 0.000281 
Batch[16519] - loss: 0.000226 
Batch[16520] - loss: 0.000273 
Batch[16521] - loss: 0.000168 
Batch[16522] - loss: 0.000192 
Batch[16523] - loss: 0.000291 
Batch[16524] - loss: 0.000339 
Batch[16525] - loss: 0.000225 
Batch[16526] - loss: 0.000205 
Batch[16527] - loss: 0.000129 
Batch[16528] - loss: 0.000498 
Batch[16529] - loss: 0.000460 
Batch[16530] - loss: 0.000219 
Batch[16531] - loss: 0.000136 
Batch[16532] - loss: 0.000342 
Batch[16533] - loss: 0.000166 
Batch[16534] - loss: 0.000239 
Batch[16535] - loss: 0.000147 
Batch[16536] - loss: 0.000378 
Batch[16537] - loss: 0.000286 
Batch[16538] - loss: 0.000317 
Batch[16539] - loss: 0.000154 
Batch[16540] - loss: 0.000298 
Batch[16541] - loss: 0.000216 
Batch[16542] - loss: 0.000229 
Batch[16543] - loss: 0.000324 
Batch[16544] - loss: 0.000247 
Batch[16545] - loss: 0.000163 
Batch[16546] - loss: 0.000268 
Batch[16547] - loss: 0.000323 
Batch[16548] - loss: 0.000382 
Batch[16549] - loss: 0.000247 
Batch[16550] - loss: 0.000195 
Batch[16551] - loss: 0.000191 
Batch[16552] - loss: 0.000209 
Batch[16553] - loss: 0.000219 
Batch[16554] - loss: 0.000142 
Batch[16555] - loss: 0.000232 
Batch[16556] - loss: 0.000267 
Batch[16557] - loss: 0.000216 
Batch[16558] - loss: 0.000236 
Batch[16559] - loss: 0.000189 
Batch[16560] - loss: 0.000076 
Batch[16561] - loss: 0.000210 
Batch[16562] - loss: 0.000165 
Batch[16563] - loss: 0.000206 
Batch[16564] - loss: 0.000245 
Batch[16565] - loss: 0.000273 
Batch[16566] - loss: 0.000501 
Batch[16567] - loss: 0.000253 
Batch[16568] - loss: 0.000233 
Batch[16569] - loss: 0.000134 
Batch[16570] - loss: 0.000457 
Batch[16571] - loss: 0.000296 
Batch[16572] - loss: 0.000226 
Batch[16573] - loss: 0.000209 
Batch[16574] - loss: 0.000178 
Batch[16575] - loss: 0.000298 
Batch[16576] - loss: 0.000137 
Batch[16577] - loss: 0.000145 
Batch[16578] - loss: 0.000149 
Batch[16579] - loss: 0.000239 
Batch[16580] - loss: 0.000188 
Batch[16581] - loss: 0.000222 
Batch[16582] - loss: 0.000240 
Batch[16583] - loss: 0.000137 
Batch[16584] - loss: 0.000174 
Batch[16585] - loss: 0.000268 
Batch[16586] - loss: 0.000344 
Batch[16587] - loss: 0.000535 
Batch[16588] - loss: 0.000227 
Batch[16589] - loss: 0.000285 
Batch[16590] - loss: 0.000195 
Batch[16591] - loss: 0.000314 
Batch[16592] - loss: 0.000221 
Batch[16593] - loss: 0.000250 
Batch[16594] - loss: 0.000125 
Batch[16595] - loss: 0.000212 
Batch[16596] - loss: 0.000141 
Batch[16597] - loss: 0.000390 
Batch[16598] - loss: 0.000454 
Batch[16599] - loss: 0.000201 
Batch[16600] - loss: 0.000324 

Evaluation - loss: 0.000072 pearson: 0.5501 

early stop by 1500 steps.
Batch[16601] - loss: 0.000180 
Batch[16602] - loss: 0.000221 
Batch[16603] - loss: 0.000196 
Batch[16604] - loss: 0.000172 
Batch[16605] - loss: 0.000225 
Batch[16606] - loss: 0.000332 
Batch[16607] - loss: 0.000131 
Batch[16608] - loss: 0.000181 
Batch[16609] - loss: 0.000248 
Batch[16610] - loss: 0.000222 
Batch[16611] - loss: 0.000208 
Batch[16612] - loss: 0.000204 
Batch[16613] - loss: 0.000252 
Batch[16614] - loss: 0.000218 
Batch[16615] - loss: 0.000220 
Batch[16616] - loss: 0.000166 
Batch[16617] - loss: 0.000386 
Batch[16618] - loss: 0.000431 
Batch[16619] - loss: 0.000314 
Batch[16620] - loss: 0.000330 
Batch[16621] - loss: 0.000140 
Batch[16622] - loss: 0.000213 
Batch[16623] - loss: 0.000128 
Batch[16624] - loss: 0.000182 
Batch[16625] - loss: 0.000249 
Batch[16626] - loss: 0.000335 
Batch[16627] - loss: 0.000178 
Batch[16628] - loss: 0.000159 
Batch[16629] - loss: 0.000568 
Batch[16630] - loss: 0.000254 
Batch[16631] - loss: 0.000253 
Batch[16632] - loss: 0.000155 
Batch[16633] - loss: 0.000216 
Batch[16634] - loss: 0.000154 
Batch[16635] - loss: 0.000176 
Batch[16636] - loss: 0.000196 
Batch[16637] - loss: 0.000288 
Batch[16638] - loss: 0.000175 
Batch[16639] - loss: 0.000256 
Batch[16640] - loss: 0.000223 
Batch[16641] - loss: 0.000256 
Batch[16642] - loss: 0.000202 
Batch[16643] - loss: 0.000320 
Batch[16644] - loss: 0.000131 
Batch[16645] - loss: 0.000178 
Batch[16646] - loss: 0.000320 
Batch[16647] - loss: 0.000207 
Batch[16648] - loss: 0.000407 
Batch[16649] - loss: 0.000166 
Batch[16650] - loss: 0.000161 
Batch[16651] - loss: 0.000246 
Batch[16652] - loss: 0.000256 
Batch[16653] - loss: 0.000455 
Batch[16654] - loss: 0.000219 
Batch[16655] - loss: 0.000528 
Batch[16656] - loss: 0.000195 
Batch[16657] - loss: 0.000125 
Batch[16658] - loss: 0.000270 
Batch[16659] - loss: 0.000242 
Batch[16660] - loss: 0.000514 
Batch[16661] - loss: 0.000290 
Batch[16662] - loss: 0.000234 
Batch[16663] - loss: 0.000327 
Batch[16664] - loss: 0.000239 
Batch[16665] - loss: 0.000201 
Batch[16666] - loss: 0.000171 
Batch[16667] - loss: 0.000151 
Batch[16668] - loss: 0.000142 
Batch[16669] - loss: 0.000357 
Batch[16670] - loss: 0.000174 
Batch[16671] - loss: 0.000226 
Batch[16672] - loss: 0.000181 
Batch[16673] - loss: 0.000378 
Batch[16674] - loss: 0.000162 
Batch[16675] - loss: 0.000241 
Batch[16676] - loss: 0.000156 
Batch[16677] - loss: 0.000219 
Batch[16678] - loss: 0.000342 
Batch[16679] - loss: 0.000373 
Batch[16680] - loss: 0.000312 
Batch[16681] - loss: 0.000303 
Batch[16682] - loss: 0.000181 
Batch[16683] - loss: 0.000214 
Batch[16684] - loss: 0.000342 
Batch[16685] - loss: 0.000239 
Batch[16686] - loss: 0.000226 
Batch[16687] - loss: 0.000132 
Batch[16688] - loss: 0.000296 
Batch[16689] - loss: 0.000325 
Batch[16690] - loss: 0.000353 
Batch[16691] - loss: 0.000215 
Batch[16692] - loss: 0.000190 
Batch[16693] - loss: 0.000249 
Batch[16694] - loss: 0.000165 
Batch[16695] - loss: 0.000170 
Batch[16696] - loss: 0.000291 
Batch[16697] - loss: 0.000195 
Batch[16698] - loss: 0.000399 
Batch[16699] - loss: 0.000156 
Batch[16700] - loss: 0.000131 

Evaluation - loss: 0.000072 pearson: 0.5502 

early stop by 1500 steps.
Batch[16701] - loss: 0.000228 
Batch[16702] - loss: 0.000251 
Batch[16703] - loss: 0.000094 
Batch[16704] - loss: 0.000236 
Batch[16705] - loss: 0.000318 
Batch[16706] - loss: 0.000208 
Batch[16707] - loss: 0.000120 
Batch[16708] - loss: 0.000225 
Batch[16709] - loss: 0.000198 
Batch[16710] - loss: 0.000184 
Batch[16711] - loss: 0.000207 
Batch[16712] - loss: 0.000172 
Batch[16713] - loss: 0.000190 
Batch[16714] - loss: 0.000222 
Batch[16715] - loss: 0.000272 
Batch[16716] - loss: 0.000307 
Batch[16717] - loss: 0.000365 
Batch[16718] - loss: 0.000214 
Batch[16719] - loss: 0.000238 
Batch[16720] - loss: 0.000192 
Batch[16721] - loss: 0.000283 
Batch[16722] - loss: 0.000383 
Batch[16723] - loss: 0.000274 
Batch[16724] - loss: 0.000230 
Batch[16725] - loss: 0.000390 
Batch[16726] - loss: 0.000215 
Batch[16727] - loss: 0.000262 
Batch[16728] - loss: 0.000267 
Batch[16729] - loss: 0.000186 
Batch[16730] - loss: 0.000494 
Batch[16731] - loss: 0.000261 
Batch[16732] - loss: 0.000225 
Batch[16733] - loss: 0.000262 
Batch[16734] - loss: 0.000263 
Batch[16735] - loss: 0.000300 
Batch[16736] - loss: 0.000186 
Batch[16737] - loss: 0.000188 
Batch[16738] - loss: 0.000289 
Batch[16739] - loss: 0.000149 
Batch[16740] - loss: 0.000253 
Batch[16741] - loss: 0.000356 
Batch[16742] - loss: 0.000126 
Batch[16743] - loss: 0.000238 
Batch[16744] - loss: 0.000127 
Batch[16745] - loss: 0.000238 
Batch[16746] - loss: 0.000175 
Batch[16747] - loss: 0.000413 
Batch[16748] - loss: 0.000234 
Batch[16749] - loss: 0.000237 
Batch[16750] - loss: 0.000205 
Batch[16751] - loss: 0.000222 
Batch[16752] - loss: 0.000294 
Batch[16753] - loss: 0.000157 
Batch[16754] - loss: 0.000488 
Batch[16755] - loss: 0.000146 
Batch[16756] - loss: 0.000139 
Batch[16757] - loss: 0.000136 
Batch[16758] - loss: 0.000179 
Batch[16759] - loss: 0.000176 
Batch[16760] - loss: 0.000164 
Batch[16761] - loss: 0.000190 
Batch[16762] - loss: 0.000231 
Batch[16763] - loss: 0.000082 
Batch[16764] - loss: 0.000183 
Batch[16765] - loss: 0.000233 
Batch[16766] - loss: 0.000093 
Batch[16767] - loss: 0.000121 
Batch[16768] - loss: 0.000165 
Batch[16769] - loss: 0.000164 
Batch[16770] - loss: 0.000222 
Batch[16771] - loss: 0.000300 
Batch[16772] - loss: 0.000142 
Batch[16773] - loss: 0.000217 
Batch[16774] - loss: 0.000168 
Batch[16775] - loss: 0.000153 
Batch[16776] - loss: 0.000236 
Batch[16777] - loss: 0.000294 
Batch[16778] - loss: 0.000128 
Batch[16779] - loss: 0.000300 
Batch[16780] - loss: 0.000262 
Batch[16781] - loss: 0.000291 
Batch[16782] - loss: 0.000245 
Batch[16783] - loss: 0.000195 
Batch[16784] - loss: 0.000149 
Batch[16785] - loss: 0.000163 
Batch[16786] - loss: 0.000203 
Batch[16787] - loss: 0.000255 
Batch[16788] - loss: 0.000309 
Batch[16789] - loss: 0.000197 
Batch[16790] - loss: 0.000185 
Batch[16791] - loss: 0.000169 
Batch[16792] - loss: 0.000320 
Batch[16793] - loss: 0.000228 
Batch[16794] - loss: 0.000253 
Batch[16795] - loss: 0.000217 
Batch[16796] - loss: 0.000217 
Batch[16797] - loss: 0.000285 
Batch[16798] - loss: 0.000116 
Batch[16799] - loss: 0.000269 
Batch[16800] - loss: 0.000172 

Evaluation - loss: 0.000072 pearson: 0.5502 

early stop by 1500 steps.
Batch[16801] - loss: 0.000251 
Batch[16802] - loss: 0.000321 
Batch[16803] - loss: 0.000341 
Batch[16804] - loss: 0.000195 
Batch[16805] - loss: 0.000146 
Batch[16806] - loss: 0.000175 
Batch[16807] - loss: 0.000282 
Batch[16808] - loss: 0.000226 
Batch[16809] - loss: 0.000154 
Batch[16810] - loss: 0.000182 
Batch[16811] - loss: 0.000322 
Batch[16812] - loss: 0.000239 
Batch[16813] - loss: 0.000183 
Batch[16814] - loss: 0.000184 
Batch[16815] - loss: 0.000202 
Batch[16816] - loss: 0.000260 
Batch[16817] - loss: 0.000261 
Batch[16818] - loss: 0.000475 
Batch[16819] - loss: 0.000281 
Batch[16820] - loss: 0.000101 
Batch[16821] - loss: 0.000166 
Batch[16822] - loss: 0.000185 
Batch[16823] - loss: 0.000160 
Batch[16824] - loss: 0.000280 
Batch[16825] - loss: 0.000267 
Batch[16826] - loss: 0.000293 
Batch[16827] - loss: 0.000222 
Batch[16828] - loss: 0.000101 
Batch[16829] - loss: 0.000133 
Batch[16830] - loss: 0.000137 
Batch[16831] - loss: 0.000112 
Batch[16832] - loss: 0.000489 
Batch[16833] - loss: 0.000409 
Batch[16834] - loss: 0.000154 
Batch[16835] - loss: 0.000327 
Batch[16836] - loss: 0.000193 
Batch[16837] - loss: 0.000199 
Batch[16838] - loss: 0.000143 
Batch[16839] - loss: 0.000285 
Batch[16840] - loss: 0.000118 
Batch[16841] - loss: 0.000301 
Batch[16842] - loss: 0.000265 
Batch[16843] - loss: 0.000234 
Batch[16844] - loss: 0.000178 
Batch[16845] - loss: 0.000198 
Batch[16846] - loss: 0.000199 
Batch[16847] - loss: 0.000151 
Batch[16848] - loss: 0.000227 
Batch[16849] - loss: 0.000237 
Batch[16850] - loss: 0.000159 
Batch[16851] - loss: 0.000187 
Batch[16852] - loss: 0.000249 
Batch[16853] - loss: 0.000191 
Batch[16854] - loss: 0.000197 
Batch[16855] - loss: 0.000100 
Batch[16856] - loss: 0.000294 
Batch[16857] - loss: 0.000251 
Batch[16858] - loss: 0.000288 
Batch[16859] - loss: 0.000639 
Batch[16860] - loss: 0.000208 
Batch[16861] - loss: 0.000260 
Batch[16862] - loss: 0.000209 
Batch[16863] - loss: 0.000209 
Batch[16864] - loss: 0.000181 
Batch[16865] - loss: 0.000227 
Batch[16866] - loss: 0.000144 
Batch[16867] - loss: 0.000207 
Batch[16868] - loss: 0.000300 
Batch[16869] - loss: 0.000299 
Batch[16870] - loss: 0.000251 
Batch[16871] - loss: 0.000259 
Batch[16872] - loss: 0.000250 
Batch[16873] - loss: 0.000226 
Batch[16874] - loss: 0.000414 
Batch[16875] - loss: 0.000241 
Batch[16876] - loss: 0.000216 
Batch[16877] - loss: 0.000219 
Batch[16878] - loss: 0.000271 
Batch[16879] - loss: 0.000184 
Batch[16880] - loss: 0.000207 
Batch[16881] - loss: 0.000215 
Batch[16882] - loss: 0.000207 
Batch[16883] - loss: 0.000188 
Batch[16884] - loss: 0.000200 
Batch[16885] - loss: 0.000248 
Batch[16886] - loss: 0.000110 
Batch[16887] - loss: 0.000117 
Batch[16888] - loss: 0.000243 
Batch[16889] - loss: 0.000198 
Batch[16890] - loss: 0.000250 
Batch[16891] - loss: 0.000201 
Batch[16892] - loss: 0.000110 
Batch[16893] - loss: 0.000129 
Batch[16894] - loss: 0.000254 
Batch[16895] - loss: 0.000201 
Batch[16896] - loss: 0.000188 
Batch[16897] - loss: 0.000217 
Batch[16898] - loss: 0.000169 
Batch[16899] - loss: 0.000272 
Batch[16900] - loss: 0.000154 

Evaluation - loss: 0.000073 pearson: 0.5494 

early stop by 1500 steps.
Batch[16901] - loss: 0.000344 
Batch[16902] - loss: 0.000128 
Batch[16903] - loss: 0.000300 
Batch[16904] - loss: 0.000252 
Batch[16905] - loss: 0.000160 
Batch[16906] - loss: 0.000288 
Batch[16907] - loss: 0.000229 
Batch[16908] - loss: 0.000162 
Batch[16909] - loss: 0.000270 
Batch[16910] - loss: 0.000155 
Batch[16911] - loss: 0.000207 
Batch[16912] - loss: 0.000278 
Batch[16913] - loss: 0.000352 
Batch[16914] - loss: 0.000426 
Batch[16915] - loss: 0.000247 
Batch[16916] - loss: 0.000213 
Batch[16917] - loss: 0.000324 
Batch[16918] - loss: 0.000195 
Batch[16919] - loss: 0.000173 
Batch[16920] - loss: 0.000114 
Batch[16921] - loss: 0.000350 
Batch[16922] - loss: 0.000413 
Batch[16923] - loss: 0.000176 
Batch[16924] - loss: 0.000197 
Batch[16925] - loss: 0.000122 
Batch[16926] - loss: 0.000151 
Batch[16927] - loss: 0.000285 
Batch[16928] - loss: 0.000280 
Batch[16929] - loss: 0.000178 
Batch[16930] - loss: 0.000310 
Batch[16931] - loss: 0.000192 
Batch[16932] - loss: 0.000334 
Batch[16933] - loss: 0.000109 
Batch[16934] - loss: 0.000208 
Batch[16935] - loss: 0.000149 
Batch[16936] - loss: 0.000496 
Batch[16937] - loss: 0.000369 
Batch[16938] - loss: 0.000292 
Batch[16939] - loss: 0.000199 
Batch[16940] - loss: 0.000177 
Batch[16941] - loss: 0.000188 
Batch[16942] - loss: 0.000195 
Batch[16943] - loss: 0.000134 
Batch[16944] - loss: 0.000212 
Batch[16945] - loss: 0.000247 
Batch[16946] - loss: 0.000143 
Batch[16947] - loss: 0.000212 
Batch[16948] - loss: 0.000164 
Batch[16949] - loss: 0.000288 
Batch[16950] - loss: 0.000314 
Batch[16951] - loss: 0.000174 
Batch[16952] - loss: 0.000291 
Batch[16953] - loss: 0.000154 
Batch[16954] - loss: 0.000181 
Batch[16955] - loss: 0.000160 
Batch[16956] - loss: 0.000148 
Batch[16957] - loss: 0.000184 
Batch[16958] - loss: 0.000375 
Batch[16959] - loss: 0.000309 
Batch[16960] - loss: 0.000126 
Batch[16961] - loss: 0.000140 
Batch[16962] - loss: 0.000184 
Batch[16963] - loss: 0.000133 
Batch[16964] - loss: 0.000194 
Batch[16965] - loss: 0.000112 
Batch[16966] - loss: 0.000195 
Batch[16967] - loss: 0.000329 
Batch[16968] - loss: 0.000149 
Batch[16969] - loss: 0.000162 
Batch[16970] - loss: 0.000219 
Batch[16971] - loss: 0.000168 
Batch[16972] - loss: 0.000214 
Batch[16973] - loss: 0.000160 
Batch[16974] - loss: 0.000179 
Batch[16975] - loss: 0.000179 
Batch[16976] - loss: 0.000112 
Batch[16977] - loss: 0.000169 
Batch[16978] - loss: 0.000090 
Batch[16979] - loss: 0.000246 
Batch[16980] - loss: 0.000185 
Batch[16981] - loss: 0.000197 
Batch[16982] - loss: 0.000182 
Batch[16983] - loss: 0.000239 
Batch[16984] - loss: 0.000159 
Batch[16985] - loss: 0.000196 
Batch[16986] - loss: 0.000196 
Batch[16987] - loss: 0.000230 
Batch[16988] - loss: 0.000164 
Batch[16989] - loss: 0.000182 
Batch[16990] - loss: 0.000242 
Batch[16991] - loss: 0.000247 
Batch[16992] - loss: 0.000159 
Batch[16993] - loss: 0.000410 
Batch[16994] - loss: 0.000261 
Batch[16995] - loss: 0.000231 
Batch[16996] - loss: 0.000129 
Batch[16997] - loss: 0.000166 
Batch[16998] - loss: 0.000127 
Batch[16999] - loss: 0.000233 
Batch[17000] - loss: 0.000273 

Evaluation - loss: 0.000073 pearson: 0.5492 

early stop by 1500 steps.
Batch[17001] - loss: 0.000335 
Batch[17002] - loss: 0.000176 
Batch[17003] - loss: 0.000143 
Batch[17004] - loss: 0.000261 
Batch[17005] - loss: 0.000281 
Batch[17006] - loss: 0.000164 
Batch[17007] - loss: 0.000323 
Batch[17008] - loss: 0.000270 
Batch[17009] - loss: 0.000208 
Batch[17010] - loss: 0.000178 
Batch[17011] - loss: 0.000265 
Batch[17012] - loss: 0.000220 
Batch[17013] - loss: 0.000189 
Batch[17014] - loss: 0.000308 
Batch[17015] - loss: 0.000164 
Batch[17016] - loss: 0.000204 
Batch[17017] - loss: 0.000115 
Batch[17018] - loss: 0.000263 
Batch[17019] - loss: 0.000143 
Batch[17020] - loss: 0.000125 
Batch[17021] - loss: 0.000157 
Batch[17022] - loss: 0.000210 
Batch[17023] - loss: 0.000161 
Batch[17024] - loss: 0.000272 
Batch[17025] - loss: 0.000437 
Batch[17026] - loss: 0.000306 
Batch[17027] - loss: 0.000168 
Batch[17028] - loss: 0.000207 
Batch[17029] - loss: 0.000468 
Batch[17030] - loss: 0.000172 
Batch[17031] - loss: 0.000147 
Batch[17032] - loss: 0.000292 
Batch[17033] - loss: 0.000206 
Batch[17034] - loss: 0.000160 
Batch[17035] - loss: 0.000164 
Batch[17036] - loss: 0.000357 
Batch[17037] - loss: 0.000228 
Batch[17038] - loss: 0.000135 
Batch[17039] - loss: 0.000117 
Batch[17040] - loss: 0.000254 
Batch[17041] - loss: 0.000117 
Batch[17042] - loss: 0.000220 
Batch[17043] - loss: 0.000142 
Batch[17044] - loss: 0.000221 
Batch[17045] - loss: 0.000278 
Batch[17046] - loss: 0.000182 
Batch[17047] - loss: 0.000246 
Batch[17048] - loss: 0.000194 
Batch[17049] - loss: 0.000293 
Batch[17050] - loss: 0.000210 
Batch[17051] - loss: 0.000165 
Batch[17052] - loss: 0.000153 
Batch[17053] - loss: 0.000162 
Batch[17054] - loss: 0.000284 
Batch[17055] - loss: 0.000309 
Batch[17056] - loss: 0.000126 
Batch[17057] - loss: 0.000226 
Batch[17058] - loss: 0.000214 
Batch[17059] - loss: 0.000255 
Batch[17060] - loss: 0.000119 
Batch[17061] - loss: 0.000222 
Batch[17062] - loss: 0.000470 
Batch[17063] - loss: 0.000360 
Batch[17064] - loss: 0.000219 
Batch[17065] - loss: 0.000239 
Batch[17066] - loss: 0.000147 
Batch[17067] - loss: 0.000234 
Batch[17068] - loss: 0.000322 
Batch[17069] - loss: 0.000178 
Batch[17070] - loss: 0.000131 
Batch[17071] - loss: 0.000141 
Batch[17072] - loss: 0.000195 
Batch[17073] - loss: 0.000424 
Batch[17074] - loss: 0.000186 
Batch[17075] - loss: 0.000203 
Batch[17076] - loss: 0.000162 
Batch[17077] - loss: 0.000127 
Batch[17078] - loss: 0.000326 
Batch[17079] - loss: 0.000228 
Batch[17080] - loss: 0.000194 
Batch[17081] - loss: 0.000137 
Batch[17082] - loss: 0.000150 
Batch[17083] - loss: 0.000166 
Batch[17084] - loss: 0.000186 
Batch[17085] - loss: 0.000210 
Batch[17086] - loss: 0.000313 
Batch[17087] - loss: 0.000198 
Batch[17088] - loss: 0.000149 
Batch[17089] - loss: 0.000255 
Batch[17090] - loss: 0.000296 
Batch[17091] - loss: 0.000315 
Batch[17092] - loss: 0.000222 
Batch[17093] - loss: 0.000147 
Batch[17094] - loss: 0.000137 
Batch[17095] - loss: 0.000222 
Batch[17096] - loss: 0.000233 
Batch[17097] - loss: 0.000355 
Batch[17098] - loss: 0.000091 
Batch[17099] - loss: 0.000212 
Batch[17100] - loss: 0.000226 

Evaluation - loss: 0.000073 pearson: 0.5494 

early stop by 1500 steps.
Batch[17101] - loss: 0.000275 
Batch[17102] - loss: 0.000173 
Batch[17103] - loss: 0.000068 
Batch[17104] - loss: 0.000193 
Batch[17105] - loss: 0.000179 
Batch[17106] - loss: 0.000123 
Batch[17107] - loss: 0.000238 
Batch[17108] - loss: 0.000156 
Batch[17109] - loss: 0.000187 
Batch[17110] - loss: 0.000219 
Batch[17111] - loss: 0.000197 
Batch[17112] - loss: 0.000143 
Batch[17113] - loss: 0.000117 
Batch[17114] - loss: 0.000490 
Batch[17115] - loss: 0.000206 
Batch[17116] - loss: 0.000108 
Batch[17117] - loss: 0.000135 
Batch[17118] - loss: 0.000143 
Batch[17119] - loss: 0.000110 
Batch[17120] - loss: 0.000285 
Batch[17121] - loss: 0.000118 
Batch[17122] - loss: 0.000312 
Batch[17123] - loss: 0.000188 
Batch[17124] - loss: 0.000160 
Batch[17125] - loss: 0.000237 
Batch[17126] - loss: 0.000307 
Batch[17127] - loss: 0.000246 
Batch[17128] - loss: 0.000131 
Batch[17129] - loss: 0.000150 
Batch[17130] - loss: 0.000257 
Batch[17131] - loss: 0.000113 
Batch[17132] - loss: 0.000234 
Batch[17133] - loss: 0.000123 
Batch[17134] - loss: 0.000364 
Batch[17135] - loss: 0.000189 
Batch[17136] - loss: 0.000273 
Batch[17137] - loss: 0.000110 
Batch[17138] - loss: 0.000204 
Batch[17139] - loss: 0.000324 
Batch[17140] - loss: 0.000251 
Batch[17141] - loss: 0.000102 
Batch[17142] - loss: 0.000329 
Batch[17143] - loss: 0.000288 
Batch[17144] - loss: 0.000152 
Batch[17145] - loss: 0.000243 
Batch[17146] - loss: 0.000161 
Batch[17147] - loss: 0.000191 
Batch[17148] - loss: 0.000234 
Batch[17149] - loss: 0.000271 
Batch[17150] - loss: 0.000108 
Batch[17151] - loss: 0.000307 
Batch[17152] - loss: 0.000203 
Batch[17153] - loss: 0.000217 
Batch[17154] - loss: 0.000188 
Batch[17155] - loss: 0.000302 
Batch[17156] - loss: 0.000114 
Batch[17157] - loss: 0.000162 
Batch[17158] - loss: 0.000129 
Batch[17159] - loss: 0.000219 
Batch[17160] - loss: 0.000472 
Batch[17161] - loss: 0.000236 
Batch[17162] - loss: 0.000184 
Batch[17163] - loss: 0.000310 
Batch[17164] - loss: 0.000294 
Batch[17165] - loss: 0.000215 
Batch[17166] - loss: 0.000161 
Batch[17167] - loss: 0.000176 
Batch[17168] - loss: 0.000137 
Batch[17169] - loss: 0.000419 
Batch[17170] - loss: 0.000238 
Batch[17171] - loss: 0.000201 
Batch[17172] - loss: 0.000265 
Batch[17173] - loss: 0.000100 
Batch[17174] - loss: 0.000244 
Batch[17175] - loss: 0.000294 
Batch[17176] - loss: 0.000112 
Batch[17177] - loss: 0.000153 
Batch[17178] - loss: 0.000310 
Batch[17179] - loss: 0.000168 
Batch[17180] - loss: 0.000296 
Batch[17181] - loss: 0.000084 
Batch[17182] - loss: 0.000210 
Batch[17183] - loss: 0.000165 
Batch[17184] - loss: 0.000270 
Batch[17185] - loss: 0.000162 
Batch[17186] - loss: 0.000216 
Batch[17187] - loss: 0.000175 
Batch[17188] - loss: 0.000261 
Batch[17189] - loss: 0.000176 
Batch[17190] - loss: 0.000172 
Batch[17191] - loss: 0.000110 
Batch[17192] - loss: 0.000159 
Batch[17193] - loss: 0.000222 
Batch[17194] - loss: 0.000415 
Batch[17195] - loss: 0.000173 
Batch[17196] - loss: 0.000371 
Batch[17197] - loss: 0.000169 
Batch[17198] - loss: 0.000228 
Batch[17199] - loss: 0.000282 
Batch[17200] - loss: 0.000161 

Evaluation - loss: 0.000073 pearson: 0.5501 

early stop by 1500 steps.
Batch[17201] - loss: 0.000141 
Batch[17202] - loss: 0.000184 
Batch[17203] - loss: 0.000229 
Batch[17204] - loss: 0.000192 
Batch[17205] - loss: 0.000213 
Batch[17206] - loss: 0.000234 
Batch[17207] - loss: 0.000355 
Batch[17208] - loss: 0.000142 
Batch[17209] - loss: 0.000207 
Batch[17210] - loss: 0.000149 
Batch[17211] - loss: 0.000268 
Batch[17212] - loss: 0.000238 
Batch[17213] - loss: 0.000153 
Batch[17214] - loss: 0.000312 
Batch[17215] - loss: 0.000209 
Batch[17216] - loss: 0.000184 
Batch[17217] - loss: 0.000390 
Batch[17218] - loss: 0.000254 
Batch[17219] - loss: 0.000148 
Batch[17220] - loss: 0.000268 
Batch[17221] - loss: 0.000194 
Batch[17222] - loss: 0.000340 
Batch[17223] - loss: 0.000099 
Batch[17224] - loss: 0.000180 
Batch[17225] - loss: 0.000248 
Batch[17226] - loss: 0.000155 
Batch[17227] - loss: 0.000270 
Batch[17228] - loss: 0.000237 
Batch[17229] - loss: 0.000120 
Batch[17230] - loss: 0.000281 
Batch[17231] - loss: 0.000215 
Batch[17232] - loss: 0.000178 
Batch[17233] - loss: 0.000295 
Batch[17234] - loss: 0.000423 
Batch[17235] - loss: 0.000172 
Batch[17236] - loss: 0.000246 
Batch[17237] - loss: 0.000175 
Batch[17238] - loss: 0.000133 
Batch[17239] - loss: 0.000226 
Batch[17240] - loss: 0.000192 
Batch[17241] - loss: 0.000168 
Batch[17242] - loss: 0.000153 
Batch[17243] - loss: 0.000339 
Batch[17244] - loss: 0.000169 
Batch[17245] - loss: 0.000153 
Batch[17246] - loss: 0.000278 
Batch[17247] - loss: 0.000183 
Batch[17248] - loss: 0.000145 
Batch[17249] - loss: 0.000268 
Batch[17250] - loss: 0.000148 
Batch[17251] - loss: 0.000210 
Batch[17252] - loss: 0.000215 
Batch[17253] - loss: 0.000158 
Batch[17254] - loss: 0.000250 
Batch[17255] - loss: 0.000227 
Batch[17256] - loss: 0.000131 
Batch[17257] - loss: 0.000203 
Batch[17258] - loss: 0.000138 
Batch[17259] - loss: 0.000223 
Batch[17260] - loss: 0.000192 
Batch[17261] - loss: 0.000179 
Batch[17262] - loss: 0.000292 
Batch[17263] - loss: 0.000172 
Batch[17264] - loss: 0.000364 
Batch[17265] - loss: 0.000214 
Batch[17266] - loss: 0.000184 
Batch[17267] - loss: 0.000178 
Batch[17268] - loss: 0.000091 
Batch[17269] - loss: 0.000209 
Batch[17270] - loss: 0.000232 
Batch[17271] - loss: 0.000156 
Batch[17272] - loss: 0.000151 
Batch[17273] - loss: 0.000164 
Batch[17274] - loss: 0.000219 
Batch[17275] - loss: 0.000211 
Batch[17276] - loss: 0.000364 
Batch[17277] - loss: 0.000152 
Batch[17278] - loss: 0.000223 
Batch[17279] - loss: 0.000178 
Batch[17280] - loss: 0.000294 
Batch[17281] - loss: 0.000223 
Batch[17282] - loss: 0.000274 
Batch[17283] - loss: 0.000262 
Batch[17284] - loss: 0.000109 
Batch[17285] - loss: 0.000184 
Batch[17286] - loss: 0.000367 
Batch[17287] - loss: 0.000231 
Batch[17288] - loss: 0.000292 
Batch[17289] - loss: 0.000224 
Batch[17290] - loss: 0.000170 
Batch[17291] - loss: 0.000293 
Batch[17292] - loss: 0.000220 
Batch[17293] - loss: 0.000205 
Batch[17294] - loss: 0.000135 
Batch[17295] - loss: 0.000131 
Batch[17296] - loss: 0.000215 
Batch[17297] - loss: 0.000240 
Batch[17298] - loss: 0.000249 
Batch[17299] - loss: 0.000154 
Batch[17300] - loss: 0.000237 

Evaluation - loss: 0.000073 pearson: 0.5496 

early stop by 1500 steps.
Batch[17301] - loss: 0.000334 
Batch[17302] - loss: 0.000214 
Batch[17303] - loss: 0.000157 
Batch[17304] - loss: 0.000090 
Batch[17305] - loss: 0.000244 
Batch[17306] - loss: 0.000208 
Batch[17307] - loss: 0.000114 
Batch[17308] - loss: 0.000206 
Batch[17309] - loss: 0.000112 
Batch[17310] - loss: 0.000142 
Batch[17311] - loss: 0.000227 
Batch[17312] - loss: 0.000224 
Batch[17313] - loss: 0.000281 
Batch[17314] - loss: 0.000214 
Batch[17315] - loss: 0.000257 
Batch[17316] - loss: 0.000212 
Batch[17317] - loss: 0.000354 
Batch[17318] - loss: 0.000234 
Batch[17319] - loss: 0.000109 
Batch[17320] - loss: 0.000221 
Batch[17321] - loss: 0.000196 
Batch[17322] - loss: 0.000234 
Batch[17323] - loss: 0.000203 
Batch[17324] - loss: 0.000236 
Batch[17325] - loss: 0.000165 
Batch[17326] - loss: 0.000154 
Batch[17327] - loss: 0.000132 
Batch[17328] - loss: 0.000351 
Batch[17329] - loss: 0.000156 
Batch[17330] - loss: 0.000223 
Batch[17331] - loss: 0.000141 
Batch[17332] - loss: 0.000243 
Batch[17333] - loss: 0.000166 
Batch[17334] - loss: 0.000190 
Batch[17335] - loss: 0.000217 
Batch[17336] - loss: 0.000342 
Batch[17337] - loss: 0.000159 
Batch[17338] - loss: 0.000167 
Batch[17339] - loss: 0.000200 
Batch[17340] - loss: 0.000173 
Batch[17341] - loss: 0.000222 
Batch[17342] - loss: 0.000157 
Batch[17343] - loss: 0.000277 
Batch[17344] - loss: 0.000132 
Batch[17345] - loss: 0.000212 
Batch[17346] - loss: 0.000150 
Batch[17347] - loss: 0.000179 
Batch[17348] - loss: 0.000154 
Batch[17349] - loss: 0.000223 
Batch[17350] - loss: 0.000213 
Batch[17351] - loss: 0.000123 
Batch[17352] - loss: 0.000205 
Batch[17353] - loss: 0.000153 
Batch[17354] - loss: 0.000287 
Batch[17355] - loss: 0.000175 
Batch[17356] - loss: 0.000161 
Batch[17357] - loss: 0.000315 
Batch[17358] - loss: 0.000193 
Batch[17359] - loss: 0.000197 
Batch[17360] - loss: 0.000257 
Batch[17361] - loss: 0.000137 
Batch[17362] - loss: 0.000151 
Batch[17363] - loss: 0.000189 
Batch[17364] - loss: 0.000160 
Batch[17365] - loss: 0.000114 
Batch[17366] - loss: 0.000187 
Batch[17367] - loss: 0.000144 
Batch[17368] - loss: 0.000401 
Batch[17369] - loss: 0.000151 
Batch[17370] - loss: 0.000254 
Batch[17371] - loss: 0.000153 
Batch[17372] - loss: 0.000137 
Batch[17373] - loss: 0.000364 
Batch[17374] - loss: 0.000206 
Batch[17375] - loss: 0.000395 
Batch[17376] - loss: 0.000152 
Batch[17377] - loss: 0.000112 
Batch[17378] - loss: 0.000198 
Batch[17379] - loss: 0.000220 
Batch[17380] - loss: 0.000408 
Batch[17381] - loss: 0.000231 
Batch[17382] - loss: 0.000221 
Batch[17383] - loss: 0.000365 
Batch[17384] - loss: 0.000232 
Batch[17385] - loss: 0.000171 
Batch[17386] - loss: 0.000208 
Batch[17387] - loss: 0.000179 
Batch[17388] - loss: 0.000166 
Batch[17389] - loss: 0.000107 
Batch[17390] - loss: 0.000186 
Batch[17391] - loss: 0.000212 
Batch[17392] - loss: 0.000237 
Batch[17393] - loss: 0.000103 
Batch[17394] - loss: 0.000069 
Batch[17395] - loss: 0.000137 
Batch[17396] - loss: 0.000194 
Batch[17397] - loss: 0.000187 
Batch[17398] - loss: 0.000253 
Batch[17399] - loss: 0.000253 
Batch[17400] - loss: 0.000252 

Evaluation - loss: 0.000073 pearson: 0.5471 

early stop by 1500 steps.
Batch[17401] - loss: 0.000256 
Batch[17402] - loss: 0.000376 
Batch[17403] - loss: 0.000237 
Batch[17404] - loss: 0.000429 
Batch[17405] - loss: 0.000215 
Batch[17406] - loss: 0.000331 
Batch[17407] - loss: 0.000216 
Batch[17408] - loss: 0.000231 
Batch[17409] - loss: 0.000276 
Batch[17410] - loss: 0.000226 
Batch[17411] - loss: 0.000313 
Batch[17412] - loss: 0.000210 
Batch[17413] - loss: 0.000146 
Batch[17414] - loss: 0.000148 
Batch[17415] - loss: 0.000176 
Batch[17416] - loss: 0.000271 
Batch[17417] - loss: 0.000165 
Batch[17418] - loss: 0.000155 
Batch[17419] - loss: 0.000243 
Batch[17420] - loss: 0.000148 
Batch[17421] - loss: 0.000151 
Batch[17422] - loss: 0.000151 
Batch[17423] - loss: 0.000156 
Batch[17424] - loss: 0.000289 
Batch[17425] - loss: 0.000246 
Batch[17426] - loss: 0.000131 
Batch[17427] - loss: 0.000178 
Batch[17428] - loss: 0.000116 
Batch[17429] - loss: 0.000144 
Batch[17430] - loss: 0.000140 
Batch[17431] - loss: 0.000097 
Batch[17432] - loss: 0.000157 
Batch[17433] - loss: 0.000201 
Batch[17434] - loss: 0.000271 
Batch[17435] - loss: 0.000167 
Batch[17436] - loss: 0.000168 
Batch[17437] - loss: 0.000109 
Batch[17438] - loss: 0.000168 
Batch[17439] - loss: 0.000210 
Batch[17440] - loss: 0.000200 
Batch[17441] - loss: 0.000209 
Batch[17442] - loss: 0.000306 
Batch[17443] - loss: 0.000498 
Batch[17444] - loss: 0.000168 
Batch[17445] - loss: 0.000112 
Batch[17446] - loss: 0.000167 
Batch[17447] - loss: 0.000140 
Batch[17448] - loss: 0.000147 
Batch[17449] - loss: 0.000174 
Batch[17450] - loss: 0.000127 
Batch[17451] - loss: 0.000292 
Batch[17452] - loss: 0.000208 
Batch[17453] - loss: 0.000278 
Batch[17454] - loss: 0.000211 
Batch[17455] - loss: 0.000142 
Batch[17456] - loss: 0.000088 
Batch[17457] - loss: 0.000232 
Batch[17458] - loss: 0.000166 
Batch[17459] - loss: 0.000121 
Batch[17460] - loss: 0.000176 
Batch[17461] - loss: 0.000089 
Batch[17462] - loss: 0.000310 
Batch[17463] - loss: 0.000118 
Batch[17464] - loss: 0.000136 
Batch[17465] - loss: 0.000368 
Batch[17466] - loss: 0.000144 
Batch[17467] - loss: 0.000225 
Batch[17468] - loss: 0.000192 
Batch[17469] - loss: 0.000191 
Batch[17470] - loss: 0.000283 
Batch[17471] - loss: 0.000216 
Batch[17472] - loss: 0.000192 
Batch[17473] - loss: 0.000225 
Batch[17474] - loss: 0.000089 
Batch[17475] - loss: 0.000173 
Batch[17476] - loss: 0.000267 
Batch[17477] - loss: 0.000189 
Batch[17478] - loss: 0.000227 
Batch[17479] - loss: 0.000129 
Batch[17480] - loss: 0.000116 
Batch[17481] - loss: 0.000130 
Batch[17482] - loss: 0.000162 
Batch[17483] - loss: 0.000286 
Batch[17484] - loss: 0.000210 
Batch[17485] - loss: 0.000288 
Batch[17486] - loss: 0.000219 
Batch[17487] - loss: 0.000196 
Batch[17488] - loss: 0.000304 
Batch[17489] - loss: 0.000188 
Batch[17490] - loss: 0.000167 
Batch[17491] - loss: 0.000132 
Batch[17492] - loss: 0.000215 
Batch[17493] - loss: 0.000189 
Batch[17494] - loss: 0.000130 
Batch[17495] - loss: 0.000291 
Batch[17496] - loss: 0.000209 
Batch[17497] - loss: 0.000382 
Batch[17498] - loss: 0.000178 
Batch[17499] - loss: 0.000203 
Batch[17500] - loss: 0.000166 

Evaluation - loss: 0.000073 pearson: 0.5499 

early stop by 1500 steps.
Batch[17501] - loss: 0.000169 
Batch[17502] - loss: 0.000129 
Batch[17503] - loss: 0.000221 
Batch[17504] - loss: 0.000213 
Batch[17505] - loss: 0.000217 
Batch[17506] - loss: 0.000158 
Batch[17507] - loss: 0.000264 
Batch[17508] - loss: 0.000421 
Batch[17509] - loss: 0.000172 
Batch[17510] - loss: 0.000222 
Batch[17511] - loss: 0.000150 
Batch[17512] - loss: 0.000134 
Batch[17513] - loss: 0.000375 
Batch[17514] - loss: 0.000202 
Batch[17515] - loss: 0.000187 
Batch[17516] - loss: 0.000204 
Batch[17517] - loss: 0.000244 
Batch[17518] - loss: 0.000273 
Batch[17519] - loss: 0.000110 
Batch[17520] - loss: 0.000311 
Batch[17521] - loss: 0.000092 
Batch[17522] - loss: 0.000149 
Batch[17523] - loss: 0.000113 
Batch[17524] - loss: 0.000302 
Batch[17525] - loss: 0.000133 
Batch[17526] - loss: 0.000139 
Batch[17527] - loss: 0.000178 
Batch[17528] - loss: 0.000297 
Batch[17529] - loss: 0.000306 
Batch[17530] - loss: 0.000104 
Batch[17531] - loss: 0.000148 
Batch[17532] - loss: 0.000181 
Batch[17533] - loss: 0.000301 
Batch[17534] - loss: 0.000132 
Batch[17535] - loss: 0.000191 
Batch[17536] - loss: 0.000148 
Batch[17537] - loss: 0.000183 
Batch[17538] - loss: 0.000172 
Batch[17539] - loss: 0.000152 
Batch[17540] - loss: 0.000126 
Batch[17541] - loss: 0.000176 
Batch[17542] - loss: 0.000323 
Batch[17543] - loss: 0.000144 
Batch[17544] - loss: 0.000238 
Batch[17545] - loss: 0.000158 
Batch[17546] - loss: 0.000321 
Batch[17547] - loss: 0.000241 
Batch[17548] - loss: 0.000149 
Batch[17549] - loss: 0.000295 
Batch[17550] - loss: 0.000126 
Batch[17551] - loss: 0.000147 
Batch[17552] - loss: 0.000254 
Batch[17553] - loss: 0.000250 
Batch[17554] - loss: 0.000232 
Batch[17555] - loss: 0.000372 
Batch[17556] - loss: 0.000242 
Batch[17557] - loss: 0.000255 
Batch[17558] - loss: 0.000178 
Batch[17559] - loss: 0.000166 
Batch[17560] - loss: 0.000241 
Batch[17561] - loss: 0.000127 
Batch[17562] - loss: 0.000315 
Batch[17563] - loss: 0.000166 
Batch[17564] - loss: 0.000186 
Batch[17565] - loss: 0.000150 
Batch[17566] - loss: 0.000145 
Batch[17567] - loss: 0.000187 
Batch[17568] - loss: 0.000308 
Batch[17569] - loss: 0.000347 
Batch[17570] - loss: 0.000169 
Batch[17571] - loss: 0.000157 
Batch[17572] - loss: 0.000330 
Batch[17573] - loss: 0.000184 
Batch[17574] - loss: 0.000208 
Batch[17575] - loss: 0.000128 
Batch[17576] - loss: 0.000197 
Batch[17577] - loss: 0.000299 
Batch[17578] - loss: 0.000200 
Batch[17579] - loss: 0.000116 
Batch[17580] - loss: 0.000147 
Batch[17581] - loss: 0.000195 
Batch[17582] - loss: 0.000119 
Batch[17583] - loss: 0.000226 
Batch[17584] - loss: 0.000173 
Batch[17585] - loss: 0.000196 
Batch[17586] - loss: 0.000124 
Batch[17587] - loss: 0.000460 
Batch[17588] - loss: 0.000181 
Batch[17589] - loss: 0.000264 
Batch[17590] - loss: 0.000202 
Batch[17591] - loss: 0.000131 
Batch[17592] - loss: 0.000126 
Batch[17593] - loss: 0.000178 
Batch[17594] - loss: 0.000175 
Batch[17595] - loss: 0.000128 
Batch[17596] - loss: 0.000172 
Batch[17597] - loss: 0.000139 
Batch[17598] - loss: 0.000145 
Batch[17599] - loss: 0.000210 
Batch[17600] - loss: 0.000194 

Evaluation - loss: 0.000072 pearson: 0.5512 

early stop by 1500 steps.
Batch[17601] - loss: 0.000143 
Batch[17602] - loss: 0.000128 
Batch[17603] - loss: 0.000173 
Batch[17604] - loss: 0.000250 
Batch[17605] - loss: 0.000150 
Batch[17606] - loss: 0.000222 
Batch[17607] - loss: 0.000170 
Batch[17608] - loss: 0.000165 
Batch[17609] - loss: 0.000255 
Batch[17610] - loss: 0.000189 
Batch[17611] - loss: 0.000329 
Batch[17612] - loss: 0.000164 
Batch[17613] - loss: 0.000255 
Batch[17614] - loss: 0.000233 
Batch[17615] - loss: 0.000185 
Batch[17616] - loss: 0.000151 
Batch[17617] - loss: 0.000181 
Batch[17618] - loss: 0.000156 
Batch[17619] - loss: 0.000161 
Batch[17620] - loss: 0.000149 
Batch[17621] - loss: 0.000227 
Batch[17622] - loss: 0.000201 
Batch[17623] - loss: 0.000227 
Batch[17624] - loss: 0.000203 
Batch[17625] - loss: 0.000192 
Batch[17626] - loss: 0.000505 
Batch[17627] - loss: 0.000149 
Batch[17628] - loss: 0.000130 
Batch[17629] - loss: 0.000250 
Batch[17630] - loss: 0.000233 
Batch[17631] - loss: 0.000247 
Batch[17632] - loss: 0.000141 
Batch[17633] - loss: 0.000485 
Batch[17634] - loss: 0.000162 
Batch[17635] - loss: 0.000187 
Batch[17636] - loss: 0.000209 
Batch[17637] - loss: 0.000334 
Batch[17638] - loss: 0.000181 
Batch[17639] - loss: 0.000158 
Batch[17640] - loss: 0.000105 
Batch[17641] - loss: 0.000151 
Batch[17642] - loss: 0.000208 
Batch[17643] - loss: 0.000292 
Batch[17644] - loss: 0.000105 
Batch[17645] - loss: 0.000232 
Batch[17646] - loss: 0.000112 
Batch[17647] - loss: 0.000148 
Batch[17648] - loss: 0.000300 
Batch[17649] - loss: 0.000343 
Batch[17650] - loss: 0.000274 
Batch[17651] - loss: 0.000253 
Batch[17652] - loss: 0.000138 
Batch[17653] - loss: 0.000125 
Batch[17654] - loss: 0.000239 
Batch[17655] - loss: 0.000179 
Batch[17656] - loss: 0.000164 
Batch[17657] - loss: 0.000274 
Batch[17658] - loss: 0.000253 
Batch[17659] - loss: 0.000319 
Batch[17660] - loss: 0.000182 
Batch[17661] - loss: 0.000183 
Batch[17662] - loss: 0.000096 
Batch[17663] - loss: 0.000136 
Batch[17664] - loss: 0.000109 
